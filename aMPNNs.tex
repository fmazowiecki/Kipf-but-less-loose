%!TEX root =main.tex
\section{The distinguishing power of anonymous MPNNs}
We recall from Section~\ref{sec:MPNNs} that anonymous MPNNs are MPNNs whose message functions
only depend on the previous labels of the vertices involved. The distinguishing power of anonymous MPNNs (or aMPNNs, for short) is well understood.
Indeed, as we will shortly see, it follows from two independent works~\cite{xhlj19,grohewl} that the distinguishing power of aMPNNs can be linked to the distinguishing power of the WL algorithm. Apart from rephrasing known results in terms of aMPNNs, we provide two simplifications of the results of~\cite{grohewl}.

\subsection{General aMPNNs}
Let $( G,\pmb{\nu})$ be a labelled graph.
We denote by $\architectureWL$ the `class' of aMPNNs consisting of the single aMPNN $M_{\textsl{WL}}$ originating from the WL algorithm (see Example~\ref{ex:WL}). We denote the class of anonymous MPNNs by $\architectureano$.

\begin{theorem}[Based on~\cite{xhlj19,grohewl}]\label{thm:eqstrongWL}
The classes $\architectureano$ and  $\architectureWL$ are equally strong.
\end{theorem}


\begin{proof}
We first argue that $\architectureano$ is weaker than $\architectureWL$. The proof is a trivial adaptation of the proofs of Lemma 2 in~\cite{xhlj19} and Theorem 5 in~\cite{grohewl}. We show, by induction on the number of rounds of computation, that  $\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}\sqsubseteq \pmb{\ell}_M^{(t)}$ for all $M \in \architectureano$ and every $t\geq 0$.

Clearly, this holds for $t=0$ since $\pmb{\ell}_{M_{\textsl{WL}}}^{(0)}=\pmb{\ell}_M^{(0)}:=\pmb{\nu}$, by definition.
We assume next that the induction hypothesis holds up to round $t-1$ and consider round $t$.
Let $v$ and $w$ be two vertices such that 
$(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_v=(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_w$ holds.
This implies, by the definition of $M_{\textsl{WL}}$, that $(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_v=(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_w$  and
$$
\ldbl (\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_u\mid u\in N_G(v) \rdbl=
\ldbl (\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_u\mid u\in N_G(w) \rdbl.
$$
By the induction hypothesis, this implies that 
$(\pmb{\ell}_{M}^{(t-1)})_v=(\pmb{\ell}_{M}^{(t-1)})_w$  and
$$
\ldbl (\pmb{\ell}_{M}^{(t-1)})_u\mid u\in N_G(v) \rdbl=
\ldbl (\pmb{\ell}_{M}^{(t-1)})_u\mid u\in N_G(w) \rdbl.
$$
As a consequence, for every vertex $u\in N_G(v)$ there exists a (unique) vertex $u'\in N_G(w)$ such that $(\pmb{\ell}_{M}^{(t-1)})_u=(\pmb{\ell}_{M}^{(t-1)})_{u'}$. Hence,
$$
\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_v,(\pmb{\ell}_{M}^{(t-1)})_u\right)=
\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_w,(\pmb{\ell}_{M}^{(t-1)})_{u'}\right).
$$
Furthermore, this mapping between elements of $N_G(v)$ and $N_G(w)$ is a bijection so that we also have:
$$
\mathbf{m}^{(t)}_v=\sum_{u\in N_G(v)}\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_v,(\pmb{\ell}_{M}^{(t-1)})_u\right)=\sum_{u'\in N_G(w)}\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_w,(\pmb{\ell}_{M}^{(t-1)})_{u'}\right)=\mathbf{m}^{(t)}_w.
$$
We may thus conclude that $$(\pmb{\ell}_{M}^{(t)})_v=\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_v,\mathbf{m}^{(t)}_v\right)=\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_w,\mathbf{m}^{(t)}_w\right)=(\pmb{\ell}_{M}^{(t)})_w,
$$
as desired.
%
% This was shown in \cite{xhlj19} and \cite{grohewl} for  graph neural networks which, in round $t \geq 1$, compute for each vertex $v$ a label $\pmb{\ell}^{(t)}_{v}$, as follows:
% \begin{equation}
% \pmb{\ell}^{(t)}_{v}:=
% f_{\textsl{comb}}^{(t)}\Bigl(
% \pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)
% \Bigr), \label{eq:combaggr}
% \end{equation}
% where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{agg}}^{(t)}$ are general (computable) combination and aggregation functions. Furthermore, $\pmb{\ell}^{(0)}:=\pmb{\nu}$, just as before.
% Clearly, any aMPNN can be written in the form ~(\ref{eq:combaggr}). Conversely, every graph neural network of the form~(\ref{eq:combaggr}) is readily cast as an aMPNN.
%
% Indeed,
% it suffices to observe, just as we did in Example~\ref{ex:WL}, that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{u\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{u})\bigr)$. This was already observed in Lemma 5 in~\cite{xhlj19}, based on Theorem 2 in~\cite{ZaheerKRPSS17}.
%
% Suppose that
% $\pmb{\nu}:V\to\mathbb{A}^s$. It now suffices to define for every $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{A}^s$, every $v\in V$ and $u\in N_G(u)$, and every $t\geq 1$:
% $$
% \textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},v,u):=h^{(t)}(\mathbf{y}) \text{ and } \textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=f_{\textsl{comb}}^{(t)}\left(\mathbf{x},g^{(t)}\left(\mathbf{y}\right)\right).
% $$
% This is clearly an aMPNN which computes the same labelling as~(\ref{eq:combaggr}). We remark that
% Lemma 5 in~\cite{xhlj19} crucially relies on the assumption that labels come from a countable domain
% and that the size of multisets is bounded. These conditions are satisfied because $\mathbb{A}^s$ is a countable set and there are at most $|V|$ neighbours (elements in the multiset) for every vertex in $V$.

It remains to show that $\architectureWL$ is weaker than $\architectureano$. For this, it suffices to recall that $M_{\textsl{WL}}$ is an element of $\architectureano$.
\end{proof}

We remark that the proofs in \cite{xhlj19} and \cite{grohewl} relate to  graph neural networks which, in round $t\geq 1$, compute for each vertex $v$ a label $\pmb{\ell}^{(t)}_{v}$, as follows:
\begin{equation}
\pmb{\ell}^{(t)}_{v}:=
f_{\textsl{comb}}^{(t)}\left(
\pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\left(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\right)
\right), \label{eq:combaggr}
\end{equation}
where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{aggr}}^{(t)}$ are general (computable) combination and aggregation functions which we assume to assign labels in $\mathbb{A}^s$. Furthermore, $\pmb{\ell}^{(0)}:=\pmb{\nu}$, just as before. Every graph neural network of the form~(\ref{eq:combaggr}) is readily cast as an aMPNN. Indeed,
it suffices to observe, just as we did in Example~\ref{ex:WL}, that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{u\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{u})\bigr)$. 
%This was already observed in Lemma 5 in~\cite{xhlj19}, based on Theorem 2 in~\cite{ZaheerKRPSS17}.

Suppose that
$\pmb{\nu}:V\to\mathbb{A}^s$. It now suffices to define for every $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{A}^s$, every $v\in V$ and $u\in N_G(u)$, and every $t\geq 1$:
\begin{equation}
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},v,u):=h^{(t)}(\mathbf{y}) \text{ and } \textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=f_{\textsl{comb}}^{(t)}\left(\mathbf{x},g^{(t)}\left(\mathbf{y}\right)\right).\label{eq:combaggrtoaMPNN}
\end{equation}
This is clearly an aMPNN which computes the same labelling as~(\ref{eq:combaggr}). We remark that
Lemma 5 in~\cite{xhlj19} crucially relies on the assumption that labels come from a countable domain
and that the size of multisets is bounded. These conditions are satisfied because $\mathbb{A}^s$ is a countable set and there are at most $|V|$ neighbours (elements in the multiset) for every vertex in $V$.

The aMPNNs that we consider in this paper are slightly more general than those defined by
(\ref{eq:combaggrtoaMPNN}). Indeed, we consider message functions that can also depend on the previous labelling $\pmb{\ell}_v^{(t-1)}$. In contrast, the message functions in~(\ref{eq:combaggrtoaMPNN}) only depend on $\mathbf{y}$, which corresponds to the previous labelling of neighbours $u\in N_G(v)$. Let $\architecture{}^{-}_{\textsl{anon}}$ denote the class of aMPNNs whose message functions only
depend on the previous labels of neighbours. It now suffices to observe that
 $M_{\textsl{WL}}\in \architecture{}^{-}_{\textsl{anon}}$ to infer,  combined with Theorem~\ref{thm:eqstrongWL}, that:
 \begin{corollary}
	 The classes $\architecture{}^{-}_{\textsl{anon}}$, $\architectureano$ and $\architectureWL$ are all equally strong.
 \end{corollary}
We observe, however, that this does not imply that for every aMPNN $M$ in $\architectureano$ there exists an aMPNN $M'$ in $\architecture{}^{-}_{\textsl{anon}}$
such that $\pmb{\ell}_{M}^{(t)}\equiv \pmb{\ell}_{M'}^{(t)}$ for all $t\geq 0$.
Indeed, the corollary implies that for every $M$ in $\architectureano$ there exists an aMPNN $M'$ in $\architecture{}^{-}_{\textsl{anon}}$ such that $M'\preceq M$, and there exists an $M''$ in $\architectureano$, possibly different from $M$, such that $M''\preceq M'$. In fact, the aMPNN $M''$ in this case is $M_{\textsl{WL}}$.
% What it does imply, however, is that there exists an aMPNN $M$
% in $\architecture{}^{-}_{\textsl{anon}}$ such that $M\equiv M_{\textsl{WL}}$.
\openprob{I actually don't know what is the precise relationship between the aMPNNs coming from combine/aggregate and our aMPNNs.}
\floris{Optional: say something about universal version of equally strong...}
\todo{G: read up until here}
\subsection{GNN-based aMPNNs}
More practical examples of aMPNNs, related to the graph neural networks, were considered in~\cite{grohewl}.
In that paper, the authors consider graph neural networks of the form
$$
\mathbf{L}^{(t)}:=\sigma\left(\mathbf{L}^{(t-1)}\mathbf{W}_1^{(t)}+\mathbf{A}_G\mathbf{L}^{(t-1)}\mathbf{W}_2^{(t)}+\mathbf{B}^{(t)}\right), $$
which we already described in Example~\ref{ex:GNN}. We also know from that example that such graph neural networks correspond to aMPNNs.
Let us denote by $\architecture_{\textsl{GNN}}$ the class of aMPNNs with message and update functions of the form
\begin{equation}\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}_2^{(t)}
\text{ and } 
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\sigma\left(\mathbf{x}\mathbf{W}_1^{(t)}+\mathbf{y} + \mathbf{b}^{(t)}\right) \label{eq:MPNN-GNN}
\end{equation}
for any $\mathbf{W}_1^{(t)}\in\mathbb{A}^{s\times s}$,$\mathbf{W}_2^{(t)}\in\mathbb{A}^{s\times s}$, and $\mathbf{B}^{(t)}\in\mathbb{A}^{n\times s}$ consisting of $n$ copies of a row $\mathbf{b}^{(t)}\in\mathbb{A}^{s}$. 

We start by stating a direct consequence of Theorem~\ref{thm:eqstrongWL}. It follows by observing
that $\architecture_{\textsl{GNN}}$ is sub-class of 
$\architectureano$.
\begin{corollary}
	The class 
$\architecture_{\textsl{GNN}}$ is weaker than $\architectureano$ and is thus also weaker than $\architectureWL$.
\end{corollary}

More challenging is to show that $\architecture_{\textsl{GNN}}$ and $\architectureWL$, and thus also $\architecture_{\textsl{GNN}}$ and $\architectureano$, are equally strong. The following results are known. We denote
by $\architecture_{\textsl{GNN}}^{\textsl{sign}}$
and $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ the classes of  aMPNNs in $\architecture_{\textsl{GNN}}$ whose update functions, as defined in~(\ref{eq:MPNN-GNN}), use the sign and ReLU as activation function $\sigma$, respectively.

\begin{theorem}[\cite{grohewl}] \label{thm:grohe_lower}
(i)~The classes $\architecture_{\textsl{GNN}}^{\textsl{sign}}$ and  $\architectureWL$ are equally strong. (ii)~The class 
$\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ is weaker than $\architectureWL$, and
$\architectureWL$ is weaker than $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$, with a factor of two.
\end{theorem}

% In the proof of this Theorem~\cite{grohewl}, an explicit construction is given of an aMPNN $M_1$ in $\architecture_{\textsl{GNN}}^{\textsl{sgn}}$
% and an aMPNN $M_2$ in $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$
% such that $M_1\preceq M_{\textsl{WL}}$
% and $M_2\preceq_g M_{\textsl{WL}}$ with $g:\mathbb{N}\to \mathbb{N}:t\mapsto 2t$.
The reason for the factor of two in (ii) in this Theorem is due to a simulation of the sign activation function by means of a two-fold application of the ReLu function. We next show that this factor of two can be avoided. As a side effect, we obtain a simpler aMPNN $M$ in $\architecture_{\textsl{GNN}}$, satisfying $M\preceq M_{\textsl{WL}}$, than the one constructed in \cite{grohewl}. The proof strategy is inspired by that of~\cite{grohewl}. Crucial in the proof the notion of \textit{row-independence modulo equality}, which we define next.

\begin{definition}[row-independence modulo equality]\label{def:label2}\normalfont
	A labelling $\pmb{\ell}:V\to\mathbb{A}^s$ is \textit{row-independent modulo equality} if the set of unique labels assigned by $\pmb{\ell}$ are linearly independent. \qed
\end{definition}
In what follows, we always assume that the labelling $\pmb{\nu}$ of $G$ is row-independent modulo equality. One can always ensure this by extending the labels. 
%
% \begin{definition}[a good matrix]\label{def:label3}\normalfont
% A matrix $\mathbf{F}$ is \textit{good relative to an other matrix} $\mathbf{F}'$ if $\mathbf{F}\equiv \mathbf{F}'$ and $\mathbf{F}$ is row-independent modulo equality.\qed
% \end{definition}
\begin{proposition}
The classes $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ and  $\architectureWL$ are equally strong.
\end{proposition}
\begin{proof}
We already know that $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ is weaker than $\architectureWL$ (Theorem~\ref{thm:grohe_lower}). It remains to show that $\architectureWL$ is weaker than $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$. That is, given the aMPNN $M_{\textsl{WL}}$, we need to construct an aMPNN $M$ in $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ such that 
$\pmb{\ell}_{M}^{(t)}\sqsubseteq \pmb{\ell}_{M_{\textsl{WL}}}^{(t)$, for all $t\geq 0$. We observe that
since $\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}\sqsubseteq \pmb{\ell}_{M}^{(t)}$ for any $M$ in $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$, this is equivalent to constructing an $M$ such that $\pmb{\ell}_{M}^{(t)}\equiv \pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$. 

The proof is by induction on the number of computation rounds. The MPNN $M$ in 
$\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ 
that we will construct will use message and update functions of the form:
\begin{equation}\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}^{(t)}
\text{ and } 
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\sigma\left(p\mathbf{x}\mathbf{W}^{(t)}+\mathbf{y} + \mathbf{b}^{(t)}\right) \label{eq:MPNN-GNN-simple}
\end{equation}
for some value $p\in\mathbb{A}$, $0<p<1$, weight matrix $\mathbf{W}^{(t)}\in\mathbb{A}^{s\times s}$, and bias vector $\mathbf{b}^{(t)}\in\mathbb{A}^s$. Note that, in contrast to aMPNNs of the form~(\ref{eq:MPNN-GNN}), we only have one weight matrix per round, instead of two, at the cost of introducing an extra parameter $p\in\mathbb{A}$.  Furthermore, the aMPNN constructed in \cite{grohewl} uses two distinct weight matrices in $\mathbb{A}^{2s\times 2s}$ (we come back to this at the end of this section) whereas our weight matrices are elements of $\mathbb{A}^{s\times s}$.

The induction hypothesis is that $\pmb{\ell}^{(t)}_M\equiv \pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$ and that $\pmb{\ell}^{(t)}_M$ is row-independent modulo equality.

For $t=0$, we have that for any $M\in \architecture_{\textsl{GNN}}^{\textsl{ReLu}}$, $\pmb{\ell}_M^{(0)}=\pmb{\ell}_{M_{\textsl{WL}}}^{(0)}:=\pmb{\nu}$, by definition.
Moreover, $\pmb{\ell}_M^{(0)}$ is row-independent modulo equality because $\pmb{\nu}$ is so, by assumption. 

We next assume that up to round $t-1$, we have found weight matrices and bias vectors for $M$ such that 
$\pmb{\ell}_M^{(t-1)}$ satisfies the induction hypothesis.
We next consider round $t$ and show that we can find a weight matrix $\mathbf{W}^{(t)}\in\mathbb{A}^{s\times s }$ and bias vector $\mathbf{b}^{(t)}\in\mathbb{A}^s$ such that also
$\pmb{\ell}_M^{(t)}$ satisfies the hypothesis.


Let $\mathbf{L}^{(t-1)}\in\mathbb{A}^{n\times s}$ denote the matrix consisting of rows $(\pmb{\ell}_M^{(t-1)})_v$, for $v\in V$.
Moreover, we denote by $\mathsf{uniq}(\mathbf{L}^{(t-1)})$ the $u\times s$-matrix consisting of the $u$ unique rows in $\mathbf{L}^{(t-1)}$. We denote the rows in $\mathsf{uniq}(\mathbf{L}^{(t-1)})$ by $\mathbf{a}_1,\ldots,\mathbf{a}_u\in\mathbb{A}^s$.
By the induction hypothesis, these rows are linearly independent. Following the same argument as in~\cite{grohewl} this implies that there exists an $s\times u$-matrix $\mathbf{U}^{(t)}$ such that $\mathsf{uniq}(\mathbf{L}^{(t-1)})\mathbf{U}^{(t)}=\mathbf{I}_{u\times u}$. 
Let us denote by $\mathbf{e}_1,\ldots,\mathbf{e_u}\in\mathbb{A}^u$ the rows of $\mathbf{I}_{u\times u}$. In other words, in $\mathbf{e}_i$, all entries are zero except for entry $i$ that holds value $1$. 

We consider the following intermediate labelling $\pmb{\mu}^{(t)}:V\to\mathbb{A}^u$ defined by
\begin{equation}
v\mapsto \left((\mathbf{A}+p\mathbf{I})\mathbf{L}^{(t-1)}\mathbf{U}^{(t)}\right)_{v\bullet}.\label{eq:labelmu}
\end{equation}
We know that for every vertex $v$, $(\pmb{\ell}_M^{(t-1)})_v$ corresponds to a unique row $\mathbf{a}_i$ in $\mathsf{uniq}(\mathbf{L}^{(t-1)})$. We denote the index of this row by $\rho(v)$. More specifically, $(\pmb{\ell}_M^{(t-1)})_v=\mathbf{a}_{\rho(v)}$. Let
$N_G(v,i):=\{u \st u\in N_G(v), \rho(v)=i\}$. That is, $N_G(v,i)$ consists of all neighbours $u$ of $v$ which are labelled as $\mathbf{a}_i$ by $\pmb{\ell}_M^{(t-1)}$.
It is now readily verified that the label $\pmb{\mu}^{(t)}_v$ defined in~(\ref{eq:labelmu}) is of the form 
\begin{equation}
\pmb{\mu}^{(t)}_v=\sum_{i=1}^u |N_G(v,i)|\mathbf{e}_i + p\mathbf{e}_{\rho(v)}.  \label{eq:linearcomb}
\end{equation}
We clearly have that $\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}\sqsubseteq\pmb{\mu}^{(t)}$.
\floris{Is this indeed ``clearly''?}
The converse also holds, as is shown in the following lemma.
\begin{lemma}
For any two vertices $v$ and $w$, we have that 
	$\pmb{\mu}^{(t)}_v=\pmb{\mu}^{(t)}_w$ implies 
	$(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_v=(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_w$.
\end{lemma}
\begin{proof}
We argue by contradiction. Suppose, for the sake of contradiction, that there exists two vertices $v,w\in V$ such that 
	\begin{equation}
		\pmb{\mu}^{(t)}_{v}=\pmb{\mu}^{(t)}_{w} \text{ and } (\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_v\neq(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_w \label{eq:contra}
	\end{equation}
	hold.
We show that this is impossible for any value $p$ satisfying $0<p<1$. (Recall from~(\ref{eq:linearcomb}) that $\pmb{\mu}^{(t)}_v$ depends on $p$.)

We distinguish between the following two cases. If $ (\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_v\neq(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_w$ then either
(i)~$(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_v\neq(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_w$; or 
(ii)~$(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_v=(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_w$
but
	$$
	\ldbl (\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_u \st u \in N_G(v) \rdbl\neq
	\ldbl(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_u \st u \in N_G(w) \rdbl.
	$$
% We recall that, by the induction hypothesis, $\pmb{\ell}^{(t-1)}_M$ is row-independent modulo equality. Let us denote by $\mathbf{a}_1,\ldots,\mathbf{a}_d \in \mathbb{A}^s$ the unique labels
% in $\pmb{\ell}^{(t-1)}_M$, which are thus linearly independent.
%
% good for $\pmb{\ell}{}^{(t-1)}$.
% In particular, if we consider the unique row vectors in  $\mathbf{F}^{(t-1)}$, then these are linearly independent. Let us denote the unique row vectors in $\mathbf{F}^{(t-1)}$ by $\mathbf{F}_1,\ldots,\mathbf{F}_s$ for some $s$.

We first consider case (i). In this case, $(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_v\neq(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_w$ implies that
	$(\pmb{\ell}^{(t-1)}_M)_{v}\neq (\pmb{\ell}_M^{(t-1)})_w$. After all,
	$\pmb{\ell}^{(t-1)}_M\equiv \pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)}$ by the induction hypothesis. 
	
It now suffices to observe that $\pmb{\mu}^{(t)}_{v}=\pmb{\mu}^{(t)}_{w}$ implies that
the corresponding linear combinations, as described in~(\ref{eq:linearcomb}), satisfy:
$$
\sum_{i=1}^u |N_G(v,i)|\mathbf{e}_i + p\mathbf{e}_{\rho(v)}=
\sum_{i=1}^u |N_G(w,i)|\mathbf{e}_i + p\mathbf{e}_{\rho(w)}.
$$
We can assume, without loss of generality,  that $(\pmb{\ell}^{(t-1)}_M)_{v}=\mathbf{a}_1$ and
	$(\pmb{\ell}^{(t-1)}_M)_{w}=\mathbf{a}_2$. Recall that $\mathbf{a}_1$ and $\mathbf{a}_2$ are two distinct labels.
Then,
\begin{multline*}
\left(|N_G(v,1)|+p-|N_G(w,1)|\right)\mathbf{e}_1+
\left(|N_G(v,2)|-|N_G(w,2)-p|\right)\mathbf{e}_2{}\\+
\sum_{i=3}^u \left(|N_G(v,i)|-|N_G(w,i)|\right)\mathbf{e}_i+ p\mathbf{e}_{\rho(v)}  - p \mathbf{e}_{\rho(w)}=0.
\end{multline*}
Since $\mathbf{e}_1,\ldots,\mathbf{e}_u$ are linearly independent, this implies that $|N_G(v,i)|-|N_G(w,i)|=0$
for all $i=3,\ldots,u$ and $|N_G(v,1)|+p-|N_G(w,1)|=0$
and $|N_G(v,2)|-|N_G(w,2)|-p|=0$. Since $|N_G(v,1)|-|N_G(w,1)|\in\mathbb{Z}$ and $0<p<1$, this is impossible. We may thus conclude that case (i) cannot occur.
	

Suppose next that we are in case (ii). Recall that for case (ii), we have that
$(\pmb{\ell}{}_{M_{\textsl{WL}}}^{(t-1)})_v=(\pmb{\ell}{}_{M_{\textsl{WL}}}^{(t-1)})_w$ and thus also  $(\pmb{\ell}{}_M^{(t-1)})_v=(\pmb{\ell}{}_M^{(t-1)})_w$.
	Using the same notation as above, we may assume that $(\pmb{\ell}{}_M^{(t-1)})_v=(\pmb{\ell}{}_M^{(t-1)})_w=\mathbf{a}_1$. In case (ii), however, we have that
	$
	\ldbl (\pmb{\ell}{}_{M_{\textsl{WL}}}^{(t-1)})_{u} \st u \in N_G(v) \rdbl\neq
	\ldbl (\pmb{\ell}{}_{M_{\textsl{WL}}}^{(t-1)})_{u} \st u \in N_G(w) \rdbl
	$ and thus also 
	$
	\ldbl (\pmb{\ell}{}_{M}^{(t-1)})_{u} \st u \in N_G(v) \rdbl\neq
	\ldbl (\pmb{\ell}{}_{M}^{(t-1)})_{u} \st u \in N_G(w) \rdbl
	$.
	That is, there must exist a label assigned by $\pmb{\ell}{}_M^{(t-1)}$ that does not occur the same number of times in the neighbourhoods of $v$ and $w$, respectively. Suppose that this label is $\mathbf{a}_2$. The case when this label is $\mathbf{a}_1$ can be treated similarly. 
	It now suffices to  observe that $\pmb{\mu}^{(t)}_{v}=\pmb{\mu}^{(t)}_{w}$ implies that
the corresponding linear combinations, as described in~(\ref{eq:linearcomb}), satisfy:
$$
\left(|N_G(v,1)|+p\right)\mathbf{e}_1 +|N_G(v,2)|\mathbf{e}_2+\sum_{i=3}^u |N_G(v,i)|\mathbf{e}_i=
\left(|N_G(w,1)|+p\right)\mathbf{e}_1 +|N_G(w,2)|\mathbf{e}_2+\sum_{i=3}^u |N_G(w,i)|\mathbf{e}_i.
$$
Using a similar argument as before, based on the linear independence of $\mathbf{e}_1,\ldots,\mathbf{e}_u$,
we can infer that $|N_G(v,2)|=|N_G(w,2)|$. We note,
however, that $\mathbf{a}_2$ appeared a different number
of times among the neighbours of $v$ and $w$. Hence, also case (ii) is ruled out and our assumption~(\ref{eq:contra})
is invalid. This implies $\pmb{\mu}^{(t)}\sqsubseteq\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$, as desired. This concludes the proof of the Lemma. 
\end{proof}


% We show this in two steps.
% \begin{description}
% 	\item [\textit{Step 1.}] First, we show that the intermediate labelling $\pmb{\mu}_M^{(t)}:V\to\mathbb{A}^s$, defined by
% \begin{equation}\pmb{\mu}_v^{(t)}:=p^{(t)}(\pmb{\ell}_M^{(t-1)})_v+\sum_{u\in N_G(v)}(\pmb{\ell}_M^{(t-1)})_u, \label{eq:mu}
% \end{equation}
% for every $v\in V$, satisfies conditions (a), (b) and (c). 
% \item  [\textit{Step 2.}]
% Second, we show the existence of a weight matrix $\mathbf{W}^{(t)}$ and a bias vector
% $\mathbf{b}^{(t)}$ such that the labelling defined by
% $$
% v\mapsto \sigma\left((\pmb{\mu}_M^{(t)})_v\mathbf{W}^{(t)}+ \mathbf{b}^{(t)}\right),
% $$
% which corresponds to $(\pmb{\ell}_M^{(t)})_v$, satisfies the  three desired conditions.
% \end{description}
% It is in the first step that we need that $\pmb{\ell}_M^{(t-1)}$ is row-independent modulo equality. We observe that we only need to show that $\pmb{\mu}_M^{(t)}\sqsubseteq \pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$, to conclude that $\pmb{\mu}_M^{(t)}\equiv \pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$. Of course, we also need to verify conditions (b) and (c).

% \smallskip
% \noindent
% \textbf{Step 1.\,}The labelling \textit{$\pmb{\mu}_M^{(t)}$ satisfies the induction hypotheses.}
% We first verify condition (a), i.e.,
% we show that $\pmb{\mu}_M^{(t)}\sqsubseteq \pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$. 
 
% In fact, we may also conclude that $\pmb{\mu}_M^{(t)}$ is row-independent modulo equality. Indeed, if two distinct labels would be dependent, then one can again infer that $\mathbf{a}_1,\ldots,\mathbf{a}_d$ must be linearly dependent.
 
% \smallskip
% \noindent
% \textbf{Step 2.\, }\textit{Construct weight matrix and bias vector.}
% Let $\mathbf{M}^{(t)}\in\mathbb{A}^{n\times s}$ the matrix consisting of rows $(\pmb{\mu}_M^{(t)})_v$, for $v\in V$.
% Moreover, we denote by $\mathsf{uniq}(\mathbf{M}^{(t)})$ the $u\times s$-matrix consisting of the unique rows in $\mathbf{M}$. We know from step 1 that
% $\mathsf{uniq}(\mathbf{M}^{(t)})$ consist of $u$ linear independent labels in $\mathbb{A}^s$. Following the same argument as in~\cite{grohewl} this implies that there exists a $s\times u$-matrix $\mathbf{U}^{(t)}$ such that $\mathsf{uniq}(\mathbf{M}^{(t)})\mathbf{U}^{(t-1)}=\mathbf{I}_{u\times u}$. 



% By induction $\mathbf{F}^{(t-1)}\equiv\hat{\pmb{\ell}}{}^{(t-1)}$. Let $\Sigma^{(t-1)}$ be the set of  labels assigned by $\hat{\pmb{\ell}}{}^{(t-1)}$ to vertices $v\in V$. 
% For any $v\in V$ and $c\in\Sigma^{(t-1)}$, we denote by $\mathbf{F}^{(t-1)}_{v\bullet}\sim c$ that
% $\hat{\pmb{\ell}}{}^{(t)}_v=c$.
% Then for each $v\in V$ and $c\in \Sigma^{(t-1)}$ we have:
% \begin{align*}
% (\mathbf{A}\mathbf{F}^{(t-1)}\mathbf{M}^{(t-1)})_{vc}&=|\{u\in N_G(v)\mid \mathbf{F}^{(t-1)}_{u\bullet}\sim c\}|.
% \intertext{Furthermore,} 
% p\mathbf{I}(\mathbf{F}^{(t-1)}\mathbf{M}^{(t-1)})_{vc}&=p\delta_{vc},
% \end{align*}
% with $\delta_{vc}=1$ if $\mathbf{F}^{(t-1)}_{v\bullet}\sim c$ and $\delta_{vc}=0$ otherwise.


From here on, we follow again closely the proof strategy
of~\cite{grohewl}. More specifically, we re-establish Lemma 9 from~\cite{grohewl}, which concerned the sign activation, for the ReLU function.
% From the definition of $\pmb{\mu}$ it follows that $\mathbf{M}$ (and thus also $\mathsf{uniq}(\mathbf{M})$) does not contain rows consisting entirely out of zero. Furthermore, 
  
\begin{lemma}\label{lem:ReLUlemma9}
  Let  $\mathbf{C}\in \mathbb{A}^{u\times s}$ be a matrix in which 
  all entries are non-negative, all rows are pairwise disjoint and such that no row consists entirely
  out of zeroes\footnote{Compared to Lemma 9 in from~\cite{grohewl},
 we additionally require non-zero rows.}.
%  \footnote{I believe that this can be
%  guaranteed in 1-WL}).\todo{G: with our extended features we actually guarantee this for free by adding the 1 column; also, t as dimension is a bad choice\ldots}
  Then there exists a matrix $\mathbf{X}\in\mathbb{A}^{s\times s}$ and a constant $m\in\mathbb{A}$
  such that $\text{\normalfont ReLU}(\mathbf{CX}-m\mathbf{J})$ is a
  non-singular matrix in $\mathbb{A}^{u\times u}$.
\end{lemma}
\begin{proof}
Let $C$ be the maximal entry in $\mathbf{C}$ and consider the column vector $\mathbf{z}=(1,C,C^2,\ldots,C^{s-1})^{\textsc{t}}\in\mathbb{A}^{s\times 1}$.
Then each entry in $\mathbf{c}=\mathbf{C}\mathbf{z}\in\mathbb{A}^{u\times 1}$ is positive and they are all pairwise distinct. 
Let $\mathbf{P}$ be a permutation matrix in $\Rb^{u\times u}$ such that $\mathbf{c}'=\mathbf{P}\mathbf{c}$ is such that  $\mathbf{c}'=(c_1',c_2',\ldots,c_u')^{\textsc{	t}}\in\mathbb{A}^{u\times 1}$ with $c_1'> c_2'>\cdots > c_u'>0$. 
Consider $\mathbf{x}=\left(\frac{1}{c_1'},\ldots,\frac{1}{c_u'}\right)\in \mathbb{A}^{1\times u}$. Then, for $\mathbf{D}=\mathbf{c}'\mathbf{x}\in\mathbb{A}^{u\times u}$
$$
\mathbf{D}_{ij}=\frac{c_i'}{c_j'}  \text{ and } \mathbf{D}_{ij}=\begin{cases}  1 & \text{if $i=j$}\\
>1 & \text{if $i<j$}\\
< 1 & \text{if $i>j$}.
\end{cases}
$$
Let $m$ be the greatest value  in $\mathbf{D}$ smaller than $1$.
% G: I think the m instantiated here is not correct
%, i.e., $m=\frac{b_s}{b_1}$.
Consider $\mathbf{E}=\mathbf{D}- m\mathbf{J}$.
Then,
$$
\mathbf{E}_{ij}=\frac{b_i'}{b_j'}- m \text{ and } \mathbf{E}_{ij}=\begin{cases}  1-m & \text{if $i=j$} \\
> 0 & \text{if $i<j$}\\
\leq 0  & \text{if $i>j$}.
\end{cases}
$$
As a consequence,
$$
\text{ReLU}(\mathbf{E})_{ij}=\begin{cases}  1-m & \text{if $i=j$}\\
>0 & \text{if $i<j$}\\
0  & \text{if $i>j$}.
\end{cases}
$$
This is an upper triangular matrix with (nonzero) value $1-m$ on its diagonal. It is therefore non-singular. 
We observe that $\mathbf{Q}\text{ReLU}(\mathbf{E})=\text{ReLU}(\mathbf{Q}\mathbf{E})$ for any row permutation $Q$. Furthermore, non-singularity is preserved under row permutations and $\mathbf{Q}\mathbf{J}=\mathbf{J}$. Hence, if we define $\mathbf{X}=\mathbf{z}\mathbf{x}$ and use the permutation matrix $\mathbf{P}$, then:
\begin{align*}
\mathbf{P}\text{ReLU}(\mathbf{C}\mathbf{X}-m\mathbf{J})&=
\text{ReLU}(\mathbf{P}\mathbf{C}\mathbf{z}\mathbf{x}-m\mathbf{P}\mathbf{J})=\text{ReLU}(\mathbf{E}-m\mathbf{J}),
\end{align*}
and we have that $\text{ReLU}(\mathbf{C}\mathbf{X}-m\mathbf{J})$ is non-singular, as desired. This concludes the proof of the Lemma.
%So, the lemma is satisfied by taking $m$ as above and
%%$m=b_s/b_1$ and % G: this still looks wrong
%$\mathbf{X}=\mathbf{z}\mathbf{x}$.
\end{proof}

We now apply this lemma to the matrix $\mathsf{uniq}(\mathbf{M}^{(t)})$, with $\mathbf{M}^{(t)}\in\mathbb{A}^{n\times s}$ consisting of the rows $\pmb{\mu}^{(t)}_v$, for $v\in V$. Inspecting the expression~(\ref{eq:linearcomb}) for $\pmb{\mu}^{(t)}_v$ we see that each row in $\mathbf{M}^{(t)}$ holds non-negative values and no row consists entirely out of zeroes. Let $\mathbf{X}^{(t)}$ and $m^{(t)}$ be the matrix and constant 
returned by the Lemma such that $\text{ReLU}\left(\mathsf{uniq}(\mathbf{M}^{(t)})\mathbf{X}^{(t)}-m^{(t)}\mathbf{J}_{u\times u}\right)$ is an $u\times u$ non-singular matrix. We now define
$$
\pmb{\ell}_M^{(t)}:=\text{ReLU}\left(\mathbf{M}^{(t)}\mathbf{X}^{(t)}-m^{(t)}\mathbf{J}_{n\times u}\right).$$
\floris{There is minor mismatch here. We assumed so far that labelling take values in $\mathbf{A}^s$. Here, we create a labelling in $\mathbf{A}^u$. We may want to padd with zeroes...}
From the non-singularity of $\text{ReLU}\left(\mathsf{uniq}(\mathbf{M}^{(t)})\mathbf{X}^{(t)}-m^{(t)}\mathbf{J}\right)$ we can immediately infer that $\pmb{\ell}_M^{(t)}$ is row-independent modulo equality. It remains to argue that 
$\pmb{\ell}_M^{(t)}\equiv\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$. This now follows from the fact that $\pmb{\mu}^{(t)}\equiv \pmb{\ell}_{M_{\textsl{WL}}}^{(t)}$
and each of the $u$ unique labels assigned by $\pmb{\mu}^{(t)}$ uniquely corresponds to a row in $\mathsf{uniq}(\mathbf{M}^{(t)})$, which in turn can be mapped bijectively to a row in $\text{ReLU}\left(\mathsf{uniq}(\mathbf{M}^{(t)})\mathbf{X}^{(t)}-m^{(t)}\mathbf{J}_{u\times u
\right)$. We conclude by observing that the desired weight matrices and bias vector are now given by
$\mathbf{W}^{(t)}:=\mathbf{U}^{(t)}\mathbf{X}^{(t)}$ 
and $\mathbf{b}^{(t)}:=-m^{(t)}\mathbf{1}$. This concludes the proof of the theorem.\end{proof}

We remark that the previous proof can be used for $\architecture_{\textsl{GNN}}^{\textsl{sign}}$ as well. One just has to use Lemma 9 in~\cite{grohewl} instead of
Lemma~\ref{lem:ReLUlemma9}. We include the statement of Lemma 9
here for completeness.
\begin{lemma}[Lemma 9 in~\cite{grohewl}]\label{lem:signlemma9}
  Let  $\mathbf{C}\in \mathbb{A}^{u\times s}$ be a matrix in which 
  all entries are non-negative and  all rows are pairwise disjoint.
%  \footnote{I believe that this can be
%  guaranteed in 1-WL}).\todo{G: with our extended features we actually guarantee this for free by adding the 1 column; also, t as dimension is a bad choice\ldots}
  Then there exists a matrix $\mathbf{X}\in\mathbb{A}^{s\times s}$ and a constant $m\in\mathbb{A}$
  such that $\text{\normalfont sign}(\mathbf{CX}-\mathbf{J})$ is a
  non-singular matrix in $\mathbb{A}^{u\times u}$.
\end{lemma}

We observe that the bias vector for the sign activation function is the same for every $t$. A similar statement holds for the ReLU function.
Indeed, we recall that we apply Lemma~\ref{lem:ReLUlemma9} to
$\mathbf{uniq}(\mathbf{M}^{(t)})$. For every $t$,
the entries in this matrix are of the form $i+p<i+1$
or $i$, for $i\in[n]$. Hence, for every $t$, the maximal entry (denoted by $C$ in the proof Lemma~\ref{lem:ReLUlemma9}  is $n+1$. The value $m^{(t)}$ relates to the largest possible ratios, smaller than $1$, of elements in the matrix constructed in  Lemma~\ref{lem:ReLUlemma9}. This ratio is upper bounded by $\frac{(n+1)^s-1}{(n+1)^s}$. Hence, taking
any $m^{(t)}=m$ for $\frac{(n+1)^s-1}{(n+1)^s}<m<1$
suffices. We can take $m$ to be arbitrarily close to $1$, but not $1$ itself.

We can thus strengthen Theorem~\ref{thm:grohe_lower}, as follows. We denote by $\architecture_{\textsl{GNN}^-}$ the 
class of aMPNNs using message and update functions of the form:
\begin{equation}\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}^{(t)}
\text{ and } 
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\sigma\left(p\mathbf{x}\mathbf{W}^{(t)}+\mathbf{y} -q \mathbf{1}^{(t)}\right) \label{eq:MPNN-GNN-simple}
\end{equation}
for some value $p,q\in\mathbb{A}$, $0<p,q<1$ and weight matrix $\mathbf{W}^{(t)}\in\mathbb{A}^{s\times s}$, and where
$\sigma$ can be either the sign or ReLU function.
\begin{corollary}
The class $\architecture_{\textsl{GNN}^-}$ is equally strong as $\architecture_{\textsl{GNN}}$ and is equally strong as $\architectureWL$.\qed
\end{corollary}
We remark that the factor two, needed for the ReLU activation function Theorem~\ref{thm:grohe_lower}, has been eliminated. Phrased in terms of graph neural networks, an aMPNN in $\architecture_{\textsl{GNN}^-}$ is of the form
$$
\mathbf{L}^{(t)}:=\sigma\left((\mathbf{A}+p\mathbf{I})\mathbf{L}^{(t-1)}\mathbf{W}^{(t)}-q\mathbf{J}\right),
$$
and thus these suffices to modelling WL.

In contrast, if one inspects the proof in
~\cite{grohewl} for the sign activation function, the miniaml
$$
[\mathbf{L}^{(0)},\mathbf{L}^{(t)}]:=\sigma\left([\mathbf{L}^{(0)},\mathbf{L}^{(t-1)}]\begin{pmatrix}
\mathbf{I}_{s\times s} & \mathbf{O}_{s\times s}\\
\mathbf{O}_{s\times s} & \mathbf{O}_{s\times s}\end{pmatrix}
+\mathbf{A}[\mathbf{L}^{(0)},\mathbf{L}^{(t-1)}]
\begin{pmatrix}
\mathbf{O}_{s\times s} & \mathbf{O}_{s\times s}\\
\mathbf{O}_{s\times s} & \mathbf{W}_{s\times s}^{(t)}\end{pmatrix}-
\begin{pmatrix}
\mathbf{O}_{s\times s} & \mathbf{J}_{s\times s}\\
\mathbf{O}_{s\times s} & \mathbf{J}_{s\times s}\end{pmatrix}
\right).
$$
