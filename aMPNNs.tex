%!TEX root =main.tex
\section{The distinguishing power of anonymous MPNNs}
We recall from Section~\ref{sec:MPNNs} that anonymous MPNNs are MPNNs whose message functions
only depend on the previous labels of the vertices involved. The distinguishing power of anonymous MPNNs (or aMPNNs for short) is well understood.
Indeed, as we will shortly, it follows from two independent works (\cite{xhlj19} and~\cite{grohewl}), that the distinguishing power of aMPNNs can be linked to the distinguishing power of the WL algorithm. Apart from rephrasing known results in terms of aMPNNs, we provide two simplifications of the results of~\cite{grohewl}.

\subsection{General aMPNNs}
Let $\langle G,\pmb{\nu}\rangle$ be a labelled graph.
We denote by $\architectureWL$ the ``class'' of aMPNNs consisting of the single aMPNN $M_{\textsl{WL}}$ originating from the WL algorithm (see Example~\ref{ex:WL}). We denote the class of anonymous MPNNs by $\architectureano$.

\begin{theorem}[Based on~\cite{xhlj19,grohewl}]\label{thm:eqstrongWL}
The classes $\architectureano$ and  $\architectureWL$ are equally strong.
\end{theorem}


\begin{proof}
We first argue that $\architectureano$ is weaker than $\architectureWL$. The proof is a trivial adaptation of the proofs of Lemma 2 in~\cite{xhlj19} and Theorem 5 in~\cite{grohewl}. We show, by induction on the number of rounds of computation, that  $\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}\sqsubseteq \pmb{\ell}_M^{(t)}$ for every $t\geq 0$.

Clearly, this holds for $t=0$ since $\pmb{\ell}_{M_{\textsl{WL}}}^{(t)}=\pmb{\ell}_M^{(t)}:=\pmb{\nu}$, by definition.
We assume next that the induction hypothesis holds up to round $t-1$ and consider round $t$.
Let $v$ and $w$ be two vertices such that 
$(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_v=(\pmb{\ell}_{M_{\textsl{WL}}}^{(t)})_w$ holds.
This implies, by the definition of $M_{\textsl{WL}}$, that $(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_v=(\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_w$  and
$$
\ldbl (\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_u\mid u\in N_G(v) \rdbl=
\ldbl (\pmb{\ell}_{M_{\textsl{WL}}}^{(t-1)})_u\mid u\in N_G(w) \rdbl.
$$
By the induction hypothesis, this implies that 
$(\pmb{\ell}_{M}^{(t-1)})_v=(\pmb{\ell}_{M}^{(t-1)})_w$  and
$$
\ldbl (\pmb{\ell}_{M}^{(t-1)})_u\mid u\in N_G(v) \rdbl=
\ldbl (\pmb{\ell}_{M}^{(t-1)})_u\mid u\in N_G(w) \rdbl.
$$
As a consequence, for every vertex $u\in N_G(v)$ there exists a (unique) vertex $u'\in N_G(w)$ such that $(\pmb{\ell}_{M}^{(t-1)})_u=(\pmb{\ell}_{M}^{(t-1)})_{u'}$. Hence,
$$
\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_v,(\pmb{\ell}_{M}^{(t-1)})_u\right)=
\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_w,(\pmb{\ell}_{M}^{(t-1)})_{u'}\right).
$$
and also
$$
\mathbf{m}^{(t)}_v=\sum_{u\in N_G(v)}\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_v,(\pmb{\ell}_{M}^{(t-1)})_u\right)=\sum_{u'\in N_G(w)}\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_w,(\pmb{\ell}_{M}^{(t-1)})_{u'}\right)=\mathbf{m}^{(t)}_w.
$$
We may thus conclude that $$(\pmb{\ell}_{M}^{(t)})_v=\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_v,\mathbf{m}^{(t)}_v\right)=\textsc{Upd}^{(t)}\left((\pmb{\ell}_{M}^{(t-1)})_w,\mathbf{m}^{(t)}_w\right)=(\pmb{\ell}_{M}^{(t)})_w,
$$
as desired.
%
% This was shown in \cite{xhlj19} and \cite{grohewl} for  graph neural networks which, in round $t>1$, compute for each vertex $v$ a label $\pmb{\ell}^{(t)}_{v}$, as follows:
% \begin{equation}
% \pmb{\ell}^{(t)}_{v}:=
% f_{\textsl{comb}}^{(t)}\Bigl(
% \pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)
% \Bigr), \label{eq:combaggr}
% \end{equation}
% where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{agg}}^{(t)}$ are general (computable) combination and aggregation functions. Furthermore, $\pmb{\ell}^{(0)}:=\pmb{\nu}$, just as before.
% Clearly, any aMPNN can be written in the form ~(\ref{eq:combaggr}). Conversely, every graph neural network of the form~(\ref{eq:combaggr}) is readily cast as an aMPNN.
%
% Indeed,
% it suffices to observe, just as we did in Example~\ref{ex:WL}, that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{u\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{u})\bigr)$. This was already observed in Lemma 5 in~\cite{xhlj19}, based on Theorem 2 in~\cite{ZaheerKRPSS17}.
%
% Suppose that
% $\pmb{\nu}:V\to\mathbb{A}^s$. It now suffices to define for every $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{A}^s$, every $v\in V$ and $u\in N_G(u)$, and every $t\geq 1$:
% $$
% \textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},v,u):=h^{(t)}(\mathbf{y}) \text{ and } \textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=f_{\textsl{comb}}^{(t)}\left(\mathbf{x},g^{(t)}\left(\mathbf{y}\right)\right).
% $$
% This is clearly an aMPNN which computes the same labelling as~(\ref{eq:combaggr}). We remark that
% Lemma 5 in~\cite{xhlj19} crucially relies on the assumption that labels come from a countable domain
% and that the size of multisets is bounded. These conditions are satisfied because $\mathbb{A}^s$ is a countable set and there are at most $|V|$ neighbours (elements in the multiset) for every vertex in $V$.

It remains to show that $\architectureWL$ is weaker than $\architectureano$. For this, it suffices to recall that $M_{\textsl{WL}}$ is an element of $\architectureano$.
\end{proof}

We remark that the proofs in \cite{xhlj19} and \cite{grohewl} relate to  graph neural networks which, in round $t>1$, compute for each vertex $v$ a label $\pmb{\ell}^{(t)}_{v}$, as follows:
\begin{equation}
\pmb{\ell}^{(t)}_{v}:=
f_{\textsl{comb}}^{(t)}\Bigl(
\pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)
\Bigr), \label{eq:combaggr}
\end{equation}
where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{aggr}}^{(t)}$ are general (computable) combination and aggregation functions which we assume to assign labels in $\mathbb{A}^s$. Furthermore, $\pmb{\ell}^{(0)}:=\pmb{\nu}$, just as before. Every graph neural network of the form~(\ref{eq:combaggr}) is readily cast as an aMPNN. Indeed,
it suffices to observe, just as we did in Example~\ref{ex:WL}, that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \pmb{\ell}^{(t-1)}_{u} \mid u \in N_G(v) \rdbl\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{u\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{u})\bigr)$. This was already observed in Lemma 5 in~\cite{xhlj19}, based on Theorem 2 in~\cite{ZaheerKRPSS17}.

Suppose that
$\pmb{\nu}:V\to\mathbb{A}^s$. It now suffices to define for every $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{A}^s$, every $v\in V$ and $u\in N_G(u)$, and every $t\geq 1$:
\begin{equation}
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},v,u):=h^{(t)}(\mathbf{y}) \text{ and } \textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=f_{\textsl{comb}}^{(t)}\left(\mathbf{x},g^{(t)}\left(\mathbf{y}\right)\right).\label{eq:combaggrtoaMPNN}
\end{equation}
This is clearly an aMPNN which computes the same labelling as~(\ref{eq:combaggr}). We remark that
Lemma 5 in~\cite{xhlj19} crucially relies on the assumption that labels come from a countable domain
and that the size of multisets is bounded. These conditions are satisfied because $\mathbb{A}^s$ is a countable set and there are at most $|V|$ neighbours (elements in the multiset) for every vertex in $V$.

The aMPNNs that we consider in this paper are slightly more general than those defined by
(\ref{eq:combaggrtoaMPNN}). Indeed, we consider message functions that can also depend on the previous labelling $\pmb{\ell}_v^{(t-1)}$. We remark that the message functions in~(\ref{eq:combaggrtoaMPNN}) only depend on $\mathbf{y}$, which corresponds to the previous labelling of neighbours $u\in N_G(v)$. Let $\architecture{}^{-}_{\textsl{ano}}$ be the class of aMPNNs whose message functions only
depend on the previous labels of neighbours. It now suffices to observe that
 $M_{\textsl{WL}}\in \architecture{}^{-}_{\textsl{ano}}$ to infer,  combined with Theorem~\ref{thm:eqstrongWL}, that:
 \begin{corollary}
	 The classes $\architecture{}^{-}_{\textsl{ano}}$, $\architectureano$ and $\architectureWL$ are all equally strong.
 \end{corollary}
We observe, however, that this does not imply that for every aMPNN $M$ in $\architectureano$ there exists an aMPNN $M'$ in $\architecture{}^{-}_{\textsl{ano}}$
such that $\pmb{\ell}_{M}^{(t)}\equiv \pmb{\ell}_{M'}^{(t)}$ for all $t\geq 0$.
Indeed, the corollary implies that for every $M$ in $\architectureano$ there exists an aMPNN $M'$ in $\architecture{}^{-}_{\textsl{ano}}$ such that $M'\preceq M$, and there exists an $M''$ in $\architectureano$, possibly different from $M$, such that $M''\preceq M'$. In fact, the aMPNN $M''$ in this case is $M_{\textsl{WL}}$.
% What it does imply, however, is that there exists an aMPNN $M$
% in $\architecture{}^{-}_{\textsl{ano}}$ such that $M\equiv M_{\textsl{WL}}$.
\openprob{I actually don't know what is the precise relationship between the aMPNNs coming from combine/aggregate and our aMPNNs.}
\floris{Optional: say something about universal version of equally strong...}

\subsection{GNN-based aMPNNs}
More practical examples of aMPNNs, related to the graph neural networks, were considered in~\cite{grohewl}.
In that paper, the authors consider graph neural networks of the form
$$
\mathbf{L}^{(t)}:=\sigma\left(\mathbf{L}^{(t-1)}\mathbf{W}_1^{(t)}+\mathbf{A}_G\mathbf{L}^{(t-1)}\mathbf{W}_2^{(t)}+\mathbf{B}^{(t)}\right), $$
which we already described in Example~\ref{ex:GNN}. We know from that example that such graph neural networks correspond to aMPNNs.
Let us denote by $\architecture_{\textsl{GNN}}$ the class of aMPNNs with message and update functions of the form
\begin{equation}\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}_2^{(t)}
\text{ and } 
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\sigma\left(\mathbf{x}\mathbf{W}_1^{(t)}+\mathbf{y} + \mathbf{b}^{(t)}\right) \label{eq:MPNN-GNN}
\end{equation}
for any $\mathbf{W}_1^{(t)}\in\mathbb{A}^{s\times s}$,$\mathbf{W}_2^{(t)}\in\mathbb{A}^{s\times s}$, and $\mathbf{B}^{(t)}\in\mathbb{A}^{n\times s}$ consisting of $n$ copies of a row $\mathbf{b}^{(t)}\in\mathbb{A}^{s}$. 

We start by stating a direct consequence of Theorem~\ref{thm:eqstrongWL}. It follows by observing
that $\architecture_{\textsl{GNN}}$ is sub-class of 
$\architectureano$.
\begin{corollary}
$\architecture_{\textsl{GNN}}$ is weaker than $\architectureano$ and is thus also weaker than $\architectureWL$.
\end{corollary}

More challenging is to show that $\architecture_{\textsl{GNN}}$ and $\architectureWL$, and thus also $\architecture_{\textsl{GNN}}$ and $\architectureano$, are equally strong. The following results are known. We denote
by $\architecture_{\textsl{GNN}}^{\textsl{sign}}$
and $\architecture_{\textsl{GNN}}^{\textsl{ReLU}}$ the classes of  aMPNNs in $\architecture_{\textsl{GNN}}$ whose update functions, as defined in~(\ref{eq:MPNN-GNN}), use the sign and ReLU activation function, respectively.

\begin{theorem}[\cite{grohewl}] \label{thm:grohe_lower}
(i)~The classes $\architecture_{\textsl{GNN}}^{\textsl{sgn}}$ and  $\architectureWL$ are equally strong. (ii)~The class 
$\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$ is weaker than $\architectureWL$, and
$\architectureWL$ is weaker than $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$, with a factor of two.
\end{theorem}

% In the proof of this Theorem~\cite{grohewl}, an explicit construction is given of an aMPNN $M_1$ in $\architecture_{\textsl{GNN}}^{\textsl{sgn}}$
% and an aMPNN $M_2$ in $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$
% such that $M_1\preceq M_{\textsl{WL}}$
% and $M_2\preceq_g M_{\textsl{WL}}$ with $g:\mathbb{N}\to \mathbb{N}:t\mapsto 2t$.
The reason for the factor of two  is due to a simulation of the sign activation function by means of a two-fold application of the ReLu function. We next show that this factor of two can be avoided. As a side effect, we obtain a simpler aMPNN $M$ in $\architecture_{\textsl{GNN}}$ such that $M\preceq M_{\textsl{WL}}$ than the one constructed in \cite{grohewl}. The proof strategy is similar to that of~\cite{grohewl}.
Crucial is the notion of \textit{row independent modulo equality}.

\begin{definition}[row independent modulo equality]\label{def:label2}\normalfont
	A labelling $\pmb{\ell}:V\to\mathbb{A}^s$ is \textit{row-independent modulo equality} if the set of unique labels assigned by $\pmb{\ell}$ are linearly independent. \qed
\end{definition}
In what follows, we always assume that the labelling $\pmb{\nu}$ of $G$ is row-independent modulo equality. One can always ensure this by extending the labels.
%
% \begin{definition}[a good matrix]\label{def:label3}\normalfont
% A matrix $\mathbf{F}$ is \textit{good relative to an other matrix} $\mathbf{F}'$ if $\mathbf{F}\equiv \mathbf{F}'$ and $\mathbf{F}$ is row-independent modulo equality.\qed
% \end{definition}
\begin{proposition}
The classes $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$ and  $\architectureWL$ are equally strong.
\end{proposition}
\begin{proof}
We already know that $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$ is weaker than $\architectureWL$. It remains to show that $\architectureWL$ is weaker than $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$. That is, given $M_{\textsl{WL}}$, we need to construct an aMPNN $M$ in $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$ such that 
$\pmb{\ell}_{M}^{(t)}\sqsubseteq \pmb{\ell}_{M_{\textsl{WL}}}$. We will show a stronger claim, i.e., that $\pmb{\ell}_{M}^{(t)}\equiv \pmb{\ell}_{M_{\textsl{WL}}}$. The proof is by induction on the number of computation rounds.
\floris{From here on: ongoing (mix of old and new)}

We assume that $\mathbf{F}^{(0)}\equiv\pmb{\ell}$ and that  $\mathbf{F}^{(0)}$ is row-independent modulo equality. We define for $t>0$:
	\begin{align*}
	\mathbf{G}^{(t)}&:=(\mathbf{A}+p\mathbf{I})\mathbf{F}^{(t-1)}\\
	\mathbf{F}^{(t)}&:=\sigma(\mathbf{G}^{(t)}\mathbf{W}^{(t-1)}+q\mathbf{J}).
	\end{align*}
We assume next that $\mathbf{F}^{(t-1)}$ is good for $\pmb{\ell}{}^{(t-1)}$. We first show that
$\mathbf{G}^{(t)}\equiv\pmb{\ell}^{(t)}$, and then apply
	Lemmas~\ref{lem:signlemma9} and
	~\ref{lem:relulemma9} to find $\mathbf{W}^{(t-1)}$ and $q$ such that $\mathbf{F}^{(t)}$ is good for  $\pmb{\ell}{}^{(t)}$. We observe that these lemmas create a rank one matrix $\mathbf{W}^{(t-1)}$.


We observe that the upper bound proof of Theorem~\ref{thm:generalbound}  implies that  $\pmb{\ell}{}^{(t)}\sqsubseteq \mathbf{G}^{(t)}$. It thus suffices to show that $ \mathbf{G}^{(t)}\sqsubseteq \pmb{\ell}{}^{(t)}$. 
$ \mathbf{G}^{(t)}\sqsubseteq \pmb{\ell}{}^{(t)}$

Suppose, for the sake of contradiction, that there exists two vertices $v,w\in V$ such that 
	\begin{equation}
	\pmb{\ell}{}^{(t)}_v\neq\pmb{\ell}{}^{(t)}_w \text{ and }
	\mathbf{G}^{(t)}_{v\bullet}=\mathbf{G}^{(t)}_{w\bullet}. \label{eq:contra}
	\end{equation} 
We show that this is impossible for $p$ such that $0<p<1$. We distinguish between the following two cases. If $\pmb{\ell}{}^{(t)}_v\neq\pmb{\ell}{}^{(t)}_w$ then either
(i)~$\pmb{\ell}{}^{(t-1)}_v\neq\pmb{\ell}{}^{(t-1)}_w$; or 
(ii)~$\pmb{\ell}{}^{(t-1)}_v=\pmb{\ell}{}^{(t-1)}_w$ and
	$$
	\ldbl \pmb{\ell}{}^{(t-1)}_{u} \st u \in N_G(v) \rdbl\neq
	\ldbl \pmb{\ell}{}^{(t-1)}_{u} \st u \in N_G(w) \rdbl.
	$$
By assumption $\mathbf{F}^{(t-1)}$ is good for $\pmb{\ell}{}^{(t-1)}$.
In particular, if we consider the unique row vectors in  $\mathbf{F}^{(t-1)}$, then these are linearly independent. Let us denote the unique row vectors in $\mathbf{F}^{(t-1)}$ by $\mathbf{F}_1,\ldots,\mathbf{F}_s$ for some $s$.


Suppose that we are in case (i). Then,  $\pmb{\ell}{}^{(t-1)}_v\neq\pmb{\ell}{}^{(t-1)}_w$ implies that
	$\mathbf{F}^{(t-1)}_{v\bullet}\neq \mathbf{F}^{(t-1)}_{w\bullet}$. We can assume, wlog, that $\mathbf{F}^{(t-1)}_{v\bullet}=\mathbf{F_1}$ and
	$\mathbf{F}^{(t-1)}_{w\bullet}=\mathbf{F_2}$. It should be clear from the definition of $\mathbf{G}^{(t)}$ that every of its rows is a linear combination of 
	the unique row vectors $\mathbf{F}_1,\ldots,\mathbf{F}_s$. More specifically:
	\begin{align*}
	\mathbf{G}^{(t)}_{v\bullet}&:=(\alpha_1+p)\mathbf{F}_1+ \alpha_2\mathbf{F}_2+ \sum_{i=3}^s \alpha_i\mathbf{F}_i\\
	%\intertext{and similarly,} 
	\mathbf{G}^{(t)}_{w\bullet}&=\beta_1\mathbf{F}_1+ (\beta_2+p)\mathbf{F}_2+ \sum_{i=3}^s \beta_i\mathbf{F}_i,
	\end{align*}
	for some non-negative natural numbers $\alpha_i$ and $\beta_i$, for $i\in[1,s]$.
We recall, however, that the vertices $v$ and $w$ are such that $\mathbf{G}^{(t)}_{v\bullet}=\mathbf{G}^{(t)}_{w\bullet}$. This in turn implies that
	$$
	(\alpha_1+p-\beta_1)\mathbf{F}_1 + (\alpha_2-\beta_2-p)\mathbf{F}_2 +\sum_{i=3}^s (\alpha_i-\beta_i)\mathbf{F}_s=0.
	$$
	So, unless all these coefficients are zero, we have that $\mathbf{F}_1,\ldots,\mathbf{F}_s$
	are not linearly independent. It is now clear that $\alpha_1+p=\beta_1$ and
	$\alpha_2=\beta_2+p$ cannot hold for any $p$, $0<p<1$. Indeed, $\alpha_i-\beta_j$
	is always an integer number. Hence, case (i) cannot occur.
	

Suppose next that we are in case (ii). Recall that for case (ii), we have $\pmb{\ell}{}^{(t-1)}_v=\pmb{\ell}{}^{(t-1)}_w$.
	Using the same notation as above, we may assume that $\mathbf{F}^{(t-1)}_{v\bullet}=\mathbf{F_1}=\mathbf{F}^{(t-1)}_{w\bullet}$. In case (ii), however, we have that
	$
	\ldbl \pmb{\ell}{}^{(t-1)}_{u} \st u \in N_G(v) \rdbl\neq
	\ldbl \pmb{\ell}{}^{(t-1)}_{u} \st u \in N_G(w) \rdbl
	$.
	That is, there must exist a label assigned by $\pmb{\ell}{}^{(t-1)}$ that does not occurs the same number of times in the neighborhoods of $v$ and $w$, respectively. Recall that $\mathbf{F}^{(t-1)}\equiv \pmb{\ell}{}^{(t-1)}$. We assume that $\mathbf{F}_2$ corresponds to the label witnessing the difference in the neighborhood labels. (As a special case, this label may also correspond to $\mathbf{F}_1$ but the same argument as below applies.)

We develop $\mathbf{G}^{(t)}_{v\bullet}$ and $\mathbf{G}^{(t)}_{w\bullet}$ again as linear combinations of $\mathbf{F}_1,\ldots,\mathbf{F}_s$. That is,
	\begin{align*}
	\mathbf{G}^{(t)}_{v\bullet}&:=(\alpha_1+p)\mathbf{F}_1+ \alpha_2\mathbf{F}_2+ \sum_{i=3}^s \alpha_i\mathbf{F}_i\\
	%\intertext{and similarly,} 
	\mathbf{G}^{(t)}_{w\bullet}&=(\beta_1+p)\mathbf{F}_1+ \beta_2\mathbf{F}_2+ \sum_{i=3}^s \beta_i\mathbf{F}_i.
	\end{align*}
	Our assumption (also in case (ii)) is still that $\mathbf{G}^{(t)}_{v\bullet}=\mathbf{G}^{(t)}_{w\bullet}$. Using the same argument as before, based on linear independence of $\mathbf{F}_1,\ldots,\mathbf{F}_s$, we must have that 
	$\alpha_i=\beta_i$ for all $i\in [1,s]$.
This cannot be true, however. Indeed,
 $\alpha_2$ and $\beta_2$, corresponding to $\mathbf{F}_2$, correspond to the number of time the label corresponding to $\mathbf{F}_2$ occurs in $N_G(u)$ and $N_G(w)$, respectively. By assumption, these numbers are different. So again, case (ii) cannot occur either. Hence, $\mathbf{G}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$, as desired.



	
\end{proof}
It suffices to show that  $\architectureWL$
is weaker than $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$.
% Instead of simulating the sign function by means of ReLU, one can directly
% use ReLU by means of a minor modification of the proof given in~\cite{grohewl}. 
% As a consequence, we avoid the factor $2$ in the correspondence between the 1-WL
% vertex labelling and the labelling induced by the feature vectors.
An inspection of the proof given in~\cite{grohewl} shows that it suffices to re-establish Lemma 9 from~\cite{grohewl} for the ReLU function. \begin{lemma}\label{lem:relulemma9}
  Let
  $\mathbf{B}\in \Nb^{p\times q}$ be a matrix in which all
  rows are pairwise disjoint and such that no row consists entirely
  out of zeroes\footnote{Compared to Lemma 9 in from~\cite{grohewl},
 we additionally require non-zero rows. This can be guaranteed provided that there are no isolated vertices}.
%  \footnote{I believe that this can be
%  guaranteed in 1-WL}).\todo{G: with our extended features we actually guarantee this for free by adding the 1 column; also, t as dimension is a bad choice\ldots}
  Then there exists a matrix $\mathbf{X}$ and a constant $m$
  such that $\text{\normalfont ReLU}(\mathbf{BX}-m\mathbf{J})$ is
  non-singular.
\end{lemma}
\begin{proof}
Let $M$ be the maximal entry in $\mathbf{B}$ and consider the column vector $\mathbf{z}=(1,M,M^2,\ldots,M^{q-1})^{\textsc{t}}$.
Then each entry in $\mathbf{b}=\mathbf{B}\mathbf{z}$ is positive and they are all pairwise distinct. 
Let $\mathbf{P}$ be a permutation matrix in $\Rb^{p\times p}$ such that $\mathbf{b}'=\mathbf{P}\mathbf{b}$ is such that  $\mathbf{b}'=(b_1',b_2',\ldots,b_p')^{\textsc{	t}}\in\Rb^{p\times 1}$ with $ b_1'> b_2'>\cdots > b_p'>0$. 
Consider the $\mathbf{x}=\left(\frac{1}{b_1'},\ldots,\frac{1}{b_p'}\right)\in \Rb^{1\times p}$. Then, for $\mathbf{C}=\mathbf{b}'\mathbf{x}$
$$
\mathbf{C}_{ij}=\frac{b_i'}{b_j'}  \text{ and } \mathbf{C}_{ij}=\begin{cases}  1 & \text{if $i=j$}\\
>1 & \text{if $i<j$}\\
< 1 & \text{if $i>j$}.
\end{cases}
$$
Let $m$ be the greatest value  in $\mathbf{C}$ smaller than $1$.
% G: I think the m instantiated here is not correct
%, i.e., $m=\frac{b_s}{b_1}$.
Consider $\mathbf{E}=\mathbf{C}- m\mathbf{J}$.
Then,
$$
\mathbf{E}_{ij}=\frac{b_i'}{b_j'}- m \text{ and } \mathbf{E}_{ij}=\begin{cases}  1-m & \text{if $i=j$} \\
> 0 & \text{if $i<j$}\\
\leq 0  & \text{if $i>j$}.
\end{cases}
$$
As a consequence,
$$
\text{ReLU}(\mathbf{E})_{ij}=\begin{cases}  1-m & \text{if $i=j$}\\
>0 & \text{if $i<j$}\\
0  & \text{if $i>j$}.
\end{cases}
$$
This is an upper triangular matrix with (nonzero) value $1-m$ on its diagonal. It is therefore non-singular. 
We observe that $\mathbf{Q}\text{ReLU}(\mathbf{E})=\text{ReLU}(\mathbf{Q}\mathbf{E})$ for any row permutation $Q$. Furthermore, non-singularity is preserved under row permutations and $\mathbf{Q}\mathbf{J}=\mathbf{J}$. Hence, if we define $\mathbf{X}=\mathbf{z}\mathbf{x}$ and use the permutation matrix $\mathbf{P}$:
\begin{align*}
\mathbf{P}\text{ReLU}(\mathbf{B}\mathbf{X}-m\mathbf{J})&=
\text{ReLU}(\mathbf{P}\mathbf{B}\mathbf{z}\mathbf{x}-m\mathbf{P}\mathbf{J})=\text{ReLU}(\mathbf{E}-m\mathbf{J}),
\end{align*}
we have that $\text{ReLU}(\mathbf{B}\mathbf{X}-m\mathbf{J})$ is non-singular, as desired.
%So, the lemma is satisfied by taking $m$ as above and
%%$m=b_s/b_1$ and % G: this still looks wrong
%$\mathbf{X}=\mathbf{z}\mathbf{x}$.
\end{proof}

% \begin{proof}
% We only need to show that $\architectureWL$ is weaker than $\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$. More specifically, we need to show that $M_{\textsl{WL}}\preceq M$ for some $M\in\architecture_{\textsl{GNN}}^{\textsl{ReLu}}$. 

% The proof of Theorem~\ref{thm:grohe_lower}~(i) consists of an explicit construction of $M \in \architecture_{\textsl{GNN}}^{\textsl{sgn}}$. The use of the sgn function
% \end{proof}

% \subsection{Simple classes of aMPNNs which are equally strong as $\architectureWL$}
% $\sigma(\mathbf{F}^{(0)})=\mathbf{F}^{(0)}$.}
% $$
% [\mathbf{F}^{(0)},\mathbf{F}^{(t)}]:=\sigma\left([\mathbf{F}^{(0)},\mathbf{F}^{(t-1)}]\begin{pmatrix}
% \mathbf{I}_{n\times n} & \mathbf{O}_{n\times n}\\
% \mathbf{O}_{n\times n} & \mathbf{O}_{n\times n}\end{pmatrix}
% +\mathbf{A}[\mathbf{F}^{(0)},\mathbf{F}^{(t-1)}]
% \begin{pmatrix}
% \mathbf{O}_{n\times n} & \mathbf{O}_{n\times n}\\
% \mathbf{O}_{n\times n} & \mathbf{W}_{n\times n}^{(t-1)}\end{pmatrix}-
% q\begin{pmatrix}
% \mathbf{O}_{n\times n} & \mathbf{J}_{n\times n}\\
% \mathbf{O}_{n\times n} & \mathbf{J}_{n\times n}\end{pmatrix}
% \right),
% $$
% where $\mathbf{W}^{(t-1)}$ is the matrix constructed in Grohe's lower bound proof.





\section{The distinguishing power of degree-aware MPNNs}





We have seen two restricted classes of MPNNs in the previous section: Anonymous MPNNs and degree-aware MPNNs. Together, these classes suffice to capture many common graph neural network architectures. In this section we study their distinguishing power. The take-away message from this section is that degree-aware MPNNs (such as those originating from the GCN architecture by~\cite{kipf-loose}, see Example~\ref{ex:KipfasMPNN}) have a slightly stronger distinguishing power than anonymous MPNNs. This may explain the experimental success of graph neural networks such as the GCNs by~\cite{kipf-loose}. Despite being based on a simple observation, the difference between anonymous and degree-aware MPNNs has, to our knowledge, not been reported in the literature before. In fact, one often finds statements indicating that existing results for anonymous MPNNs \cite{xhlj19,grohewl} carry over verbatim for, say the GCN architecture
by~\cite{kipf-loose}. As we will see, this is not precisely true.
\looseness=-1

This section is organised as follows. We first define how two classes of MPNNs can be compared relative to  their distinguishing power (Section~\ref{subsec:compare}). We then recall known results for anonymous MPNNs (Section~\ref{subsec:aMPNNs}). We conclude by establishing the distinguishing power of degree-aware MPNNs (Section~\ref{subsec:dMPNNs}).



% These notions can be generalised to classes of MPNNs, just as we did in Definition~\ref{def:classesweak}. 
% \begin{definition}\normalfont
% Consider two classes $\architecture_1$ and $\architecture_2$ of MPNNs.
% Then, $\architecture_1$ is said to be weaker than $\architecture_2$, but may be $c$ steps ahead, if for every $M_1\in \architecture_1$
% there exists an $M_2\in\architecture_2$ such that $M_1$ is weaker than $M_2$, but may be $c$ steps ahead. Similarly,
%  $\architecture_1$ is said to be weaker than $\architecture_2$, possibly up to a linear factor $c'$ and $c$ steps ahead, if for every $M_1\in \architecture_1$
% there exists an $M_2\in\architecture_2$ such that $M_1$ is weaker than $M_2$, possibly up to a linear factor $c'$ and  $c$ steps ahead. 
% \qed
% \end{definition}

% 
% The following observation follows immediately from the definitions. Consider two classes $\architecture_1$ and $\architecture_2$ of MPNNs. Let $c$ and $c'$ be arbitrary
% constants.
% If $\architecture_1$ is weaker than $\architecture_2$, then $\architecture_1$ is weaker than $\architecture_2$, but may be $c$ steps ahead. If $\architecture_1$ is weaker than $\architecture_2$, but may be $c$ steps ahead, then $\architecture_1$ is weaker than $\architecture_2$, possibly up to a linear factor $c'$  and $c$ steps ahead. We will see that the reverse implication do not necessarily hold later in the paper.

% We finally observe that when 

% The following definition states when one class of MPNNs is weaker (in terms of distinguishing power) than another class of MPNNs. Intuitively, one class $\architecture$ will be weaker than other class $\architecture'$ if any MPNN $M\in\architecture$ cannot distinguish more vertices than some MPNN $M'\in\architecture'$.

% \begin{definition}\label{def:comparing}\normalfont
% Consider two classes $\architecture$ and $\architecture'$ of MPNNs. We say that $\architecture$ is weaker than $\architecture'$ (or equivalently that $\architecture'$ is stronger than $\architecture$) if there exists a function $g:\mathbb{N}\to \mathbb{N}$ such that for every $M \in \architecture$ there exists an $M'\in \architecture'$ satisfying $\pmb{\ell}_{M'}^{(g(t))}\sqsubseteq \pmb{\ell}_{M}^{(t)}$, for all $t\geq 0$ and for any labeled graph $\langle G,\pmb{\nu},\pmb{\eta})$. This is denoted by $\architecture \sqsubseteq \architecture'$.
% % with $d$ layers 
% % and every labelling $\labl$ compatible with $g$ there exists $g' \in \architecture'$ with $m'$ layers such that $g'(\labl) \sqsubseteq g(\labl)$.
% % This is denoted by $\architecture' \sqsubseteq \architecture$.
% We write $\architecture \not \sqsubseteq \architecture'$ if the above property does not hold. 
% If we additionally require that the function $g:\mathbb{N}\to\mathbb{N}$ satisfies 
% $g(t)\le t +c$ for some constant $c$, then we write that $\architecture \sqsubseteq \architecture'$ up to a constant factor $c$. In the particular case when this holds for $c = 0$ we write that $\architecture \sqsubseteq \architecture'$ with no factor. Similarly, if there exist constants $c, c'$ such that $g(t) \le ct + c'$ we write that $\architecture \sqsubseteq \architecture'$ up to a linear factor $c$.
% \end{definition}




% Of particular interest will be the class of MPNNs corresponding to the WL algorithm. 
% \floris{Ok, add aMPNN description of WL. Note that I added the description of the WL algorithm on undirected graphs but with each labels to the prelims. }
% We will denote this class by $\architectureWL$ and it is defined as MPNNs in which the message 
% We say that an architecture $\architecture$ is \emph{bounded by WL} if $\architecture\sqsubseteq \architectureWL$. We also say that an architecture $\architecture$ is \emph{WL-strong} if $\architectureWL \sqsubseteq \architecture$. The definitions of bounded by WL and WL-strong up to linear and constant factors carry on in the obvious way.

\subsection{Anonymous MPNNs}\label{subsec:aMPNNs}


% Following the work by~\cite{xhlj19} and~\cite{grohewl}, which relates to anonymous MPNNs, we study the distinguishing power of degree-aware MPNNs. We will see that degree-aware MPNNs (such as those originating from the GCN architecture by~\cite{kipf-loose}, see Example~\ref{ex:KipfasMPNN}) have a slightly stronger distinguishing power than anonymous MPNNs.
% More precisely, any labeling computed by $d$ rounds of a degree-aware MPNNs can be computed by $d+1$ rounds of an anonymous MPNN. This may explain the experimental success of graph neural networks such as the GCNs by~\cite{kipf-loose}. We note that in the literature one often does not distinguish between anonymous and degree-aware MPNNs
% and 
% Before showing this, we first define how two classes of MPNNs can be compared relative to

\subsection{Degree-aware MPNNs}\label{subsec:dMPNNs}


It has been recently shown by~\cite{Loukas2019} that MPNNs are Turing complete, i.e., they can compute any (computable) graph function. The proof uses an equivalence between MPNNs and a well-known distributed computation model, LOCAL, which is known to be Turing complete~\cite{Angluin}. We refer to~\cite{Loukas2019} for more details. The completeness result crucially relies on the dependence of the message functions on $v$ and $w$, i.e., on knowing which vertices are being considered. 
% Indeed, after $\delta=\text{diam}(G)$ rounds each vertex has complete information about the graph $\langle G,\pmb{\nu},\pmb{\eta})$
% and once can use $\textsc{Upd}^{(\delta+1)}$ to compute the desired graph function.

\paragraph{Anonymous MPNNs.} In contrast, when the message functions 
$\textsc{Msg}^{(t)}$ in MPNNs only depend on $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_w^{(t-1)}$ and $\pmb{\eta}_{\{v,w\})}$, then the distinguishing power is limited. MPNNs with this restrictions are referred to as \textit{anonymous} MPNNs (or aMPNNs) in~\cite{Loukas2019}. We given an example of aMPNNs next.


% %
% %
% % As just mentioned, there is an ambiguity in the \citet{GilmerSRVD17}
% %
% %
% % It appears that this ambiguity raises some confusion and leads to imprecise statements about the distinguishing power of MPNNs. Indeed, the distinguishing power of MPNNs is often claimed to be bounded by the Weisfeiler-Lehman vertex colouring algorithm. This is verified, however, only when it concerns so-called \textit{anonymous} MPNNs.
% %
% % \smallskip
% % \noindent
% % {\bf Remarks.}
% %
% % First of all, directed graphs are considered.
% % Clearly, undirected graphs, as used in \citet{GilmerSRVD17}, can be regarded as directed graphs.
% % Second, the dependency of the message functions $\textsc{Upd}^{(t)}$ on the vertices $v$ and $w$
% % is made explicit. In~\citet{GilmerSRVD17} this dependency is not specified but the vertices
% %
% %
% % \leftpointright This formalisation of MPNNs given in \citep{Loukas2019} differs from that of in the following: (i)~$E^*$ and $N_G*(v)$ instead of $E$ and $N_G(v)$ are used; (ii) aggregation happens during the update phase and not the messaging phase; and (iii) the update function does not take $\mathbf{m}_v^{(t)}$ as a separate input. I guess that the two models are equivalent, but we may want to check this.
% %
% % \smallskip
% % \noindent
% % \leftpointright The formalisation of MPNNs by \citet{GilmerSRVD17} is a bit underspecified. The message and update functions do not explicitly depend on the vertices themselves, but in the examples given in \citep{GilmerSRVD17}, these functions seem to be allowed to use extra information related to the vertices and input graph. For example, to model the GCN model by~\citep{KipfW16}, the message functions need the degrees of the vertices.
% %%
% % \paragraph{Anonymous MPNNs.}
% % %
% %  The Turing completeness comes a bit as surprise. Indeed, many papers declare MPNNs as being bounded, in terms of their distinguishing power of vertices and graphs, by the Weisfeiler-Lehman algorithm. This is always backed up by referring to the papers~\citet{XuHLJ19,grohewl}. This connection, however, only holds when MPNNs do not have access to the identifiers of vertices $v$ and $w$.  Such MPNNs are referredin~\citet{Loukas2019}  as \textit{anonymous MPNNs.} Anonymous MPNNs (aMPNNs) are thus MPNNs such that the functions 
% % An anonymous MPNN (aMPNN) is an MPNN in which the message functions
% % $\textsc{Msg}^{(t)}$ only depend on $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_w^{(t-1)}$~\citep{Loukas2019}.

% More precisely, we show that aMPNNs are bounded by WL in terms of their distinguishing power. In other words, for every $t\geq 0$, $\mathbf{w}\pmb{\ell}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$. This was already shown  by \cite{XuHLJ19} and~\cite{grohewl} for undirected graphs and in which the edge labeling $\pmb{\eta}$ is absent. The generalisation to labeled directed graphs $\langle G,\pmb{\nu},\pmb{\eta}\rangle$ simply requires the use of the WL-algorithm on such graphs~\citep{}.
% % the following proposition is
% % \citet{Jaume2019} recently showed the following:
% % \smallskip
% % \noindent
% % \leftpointright I am ignoring the edge labeling $\pmb{\eta}$ for now (more later).
% %
% \begin{proposition}\label{prop:WL}
% 	The distinguishing power of anonymous MPNNs is bounded by WL.
% \end{proposition}
% \begin{proof}
% We need to show that $\mathbf{w}\pmb{\ell}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$ for every $t$.
% By definition of $\mathbf{w}\pmb{\ell}^{(0)}=$, $\mathbf{w}\pmb{\ell}^{(0)}=\pmb{\ell}^{(0)}$.
% Assume by induction that $\mathbf{w}\pmb{\ell}^{(t-1)}\sqsubseteq \pmb{\ell}^{(t-1)}$. Consider
% two vertices $v,w\in V$ such that $\mathbf{w}\pmb{\ell}^{(t)}_v=\mathbf{w}\pmb{\ell}^{(t)}_w$.
% By definition of $\mathbf{w}\pmb{\ell}^{(t)}$, this implies that 
% $\mathbf{w}\pmb{\ell}^{(t-1)}_v=\mathbf{w}\pmb{\ell}^{(t-1)}_w$ and furthermore, for every
% edge label $\eta$:
% \begin{align*}
% 	\lmset \mathbf{w}\pmb{\ell}^{(t-1)}_u\mid u\in N_G^*(v), \pmb{\eta}_{(u,v)}=\eta\rmset&=	\lmset \mathbf{w}\pmb{\ell}^{(t-1)}_u\mid u\in N_G^*(w), \pmb{\eta}_{(u,v)}=\eta\rmset.
% 	% \lmset \pmb{\eta}_{(u,v)} \mid (u,v)\in E\rmset&=	\lmset \pmb{\eta}_{(u,w)} \mid (u,w)\in E\rmset\\
% 	% \lmset \pmb{\eta}_{(v,u)} \mid (v,u)\in E\rmset&=	\lmset \pmb{\eta}_{(w,u)} \mid (w,u)\in E\rmset.
% \end{align*}
% This implies that, by induction, $\pmb{\ell}^{(t-1)}_v=\pmb{\ell}^{(t-1)}_w$, and for every $u\in N_G^*(v)$ such that
% $\pmb{\eta}_{(u,v)}=\eta$  there exists a unique corresponding $u'\in N_G^*(w)$ such that $\pmb{\eta}_{u'w}=\eta$. In other words,
% $$
% \mathbf{m}^{(t)}_{v\gets u}=\textsc{Msg}^{(t)}(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_u,\eta)=
% \textsc{Msg}^{(t)}(\pmb{\ell}^{(t-1)}_w,\pmb{\ell}^{(t-1)}_u,\eta)=\mathbf{m}^{(t)}_{w\gets u'}.$$
% As a consequence,
% $$
% \sum_{\substack{u\in N_G^*(v)\\\pmb{\eta}_{(u,v)}=\eta}} \mathbf{m}^{(t)}_{v\gets u}=\sum_{\substack{u'\in N_G^*(w)\\\pmb{\eta}_{(u',w)}=\eta}} \mathbf{m}^{(t)}_{w\gets u'}
% $$
% for every $\eta$ and hence, $\sum_{u\in N_G^*(v)}\mathbf{m}^{(t)}_{v\gets u}=
% \sum_{u\in N_G^*(w)}\mathbf{m}^{(t)}_{w\gets u'}
% $. From this we can infer that
% $$
% \pmb{\ell}^{(t)}:=\textsc{Upd}^{(t)}\left(\sum_{u\in N_G^*(v)}\mathbf{m}^{(t)}_{v\gets u}\right)=\textsc{Upd}^{(t)}\left(\sum_{u'\in N_G^*(w)}\mathbf{m}^{(t)}_{w\gets u'}\right)=\pmb{\ell}^{(t)},
% $$
% as desired.
% % We remark that this proposition encompasses the result by~\citep{XuHLJ19} for learning formalisms
% % which compute a vertex labeling $\pmb{\ell}^{(t)}$ in round $t$, specified as follows: For every vertex $v\in V$,
% %
% \end{proof}

% We remark that a similar result was recently established by~\citet{Jaume2019}. In that work, however, a version of WL on labeled directed graph is used which decouples vertex labels and edge labels. We here consider a version of WL such that correspondences known for undirected graphs carry over. More specifically,
% fractional isomorphisms, $C^2$. Check!
% %
% %
% % \smallskip
% % \noindent
% % \leftpointright I believe it is possible to link aMPNNs to the general model we used earlier.
% % For each layer $t$ we consider two functions:
% % combination function $f_{\textsl{comb}}^{(t)}$; and aggregation function
% % $f_{\textsl{agg}}^{(t)}$. The layers are defined as follows. For each vertex $v\in V$ and layer $t$:

% We further remark that the works \citep{XuHLJ19,grohewl}  considered graph neural models of the form
% $$
% \pmb{\ell}^{(t)}_{v}:=
% f_{\textsl{comb}}^{(t)}\Bigl(
% \pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid w \in N_G(v) \rmset\bigr)
% \Bigr),
% $$
% where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{agg}}^{(t)}$ are general (computable) combination and aggregation functions. It is easily verified that such models are equivalent to aMPNNs. To see this, it suffices to observe that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid u \in N_G(v) \rmset\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{w\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{w})\bigr)$ (this was observed in Lemma 5 in~\cite{XuHLJ19}, based on Theorem 2 in~\cite{ZaheerKRPSS17}). Crucial in all this is that feature values belong to a countable domain and that the size of multisets is bounded. These conditions are satisfied because our domain is $\mathbb{Q}$ and there are at most $|V|$ elements in a multiset. For completeness, we provide additional details in Section~\ref{subsec:aggr} in the appendix.
% % % $f_{\textsl{agg}}^{(t)}$.
% % In~\citet{Loukas2019} there is an argument, based on Lemma 5 in~\citet{XuHLJ19}, which allows to convert any aggregation function to a summation followed by a function. It may need a bit more thought how to squeeze this into aMPNNs.

% % \smallskip
% % \noindent
% % \leftpointright To bring edge information into the picture, we may look into~\citep{Jaume2019}, where WL for edge labeled graphs is considered.
