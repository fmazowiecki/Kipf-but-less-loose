%!TEX root =main.tex

\section{Message Passing Neural Networks}\label{sec:MPNNs}
We start by describing message passing neural networks (MPNNs) for  deep
learning on graphs, introduced by \cite{GilmerSRVD17}. Roughly speaking, in
MPNNs, vertex features are propagated through a graph according to its
connectivity structure. MPNNs are known to model a variety of graph neural
network architectures commonly used in practice.

\subsection{Definition}
Given a labelled graph $( G,\pmb{\nu})$, an MPNN computes a vertex
labelling $\pmb{\ell}:V\to \mathbb{A}^{s}$, for some $s\in\mathbb{N}^+$, by
means of a number of rounds of computation, starting from the input labelling
$\pmb{\nu}:V\to\mathbb{A}^s$.

The vertex labelling computed by an MPNN after round $t$ is denoted by $\pmb{\ell}^{(t)}$. We next detail how $\pmb{\ell}^{(t)}$ is computed.
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Initialisation.]  We let $\pmb{\ell}^{(0)}:=\pmb{\nu}$.
\end{description}
Then, for round $t=1,2,\ldots,d$, we define $\pmb{\ell}^{(t)}:V\to\mathbb{A}^{s}$, as follows\footnote{We can assume, without loss of generality, that every round assigns labels in $\mathbb{A}^s$ for the same $s$. If not, one can include ``padding with zeroes'' in the message and update functions.}:
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Message Passing.] Each vertex $v\in V$ receives messages from its neighbours which are subsequently aggregated. Formally, the function $\textsc{Msg}^{(t)}$ receives as input two vertices $v$ and $u$ with their corresponding labels from the previous iteration $\pmb{\ell}^{(t-1)}_v$ and $\pmb{\ell}^{(t-1)}_u$ and outputs a vector in $\mathbb{A}^s$. Then, for every vertex $v$, we aggregate by summing all such vectors for every neighbour $u$.
$$
\mathbf{m}^{(t)}_{v}:=\sum_{u\in N_G(v)}\textsc{Msg}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_u,v,u\right)\in\mathbb{A}^{s}.
$$
\item [Updating.] Each vertex $v\in V$ further updates $\mathbf{m}^{(t)}_{v}$ possibly based on its current label $\pmb{\ell}^{(t-1)}_v$:
$$
\pmb{\ell}^{(t)}_v:=\textsc{Upd}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\mathbf{m}^{(t)}_{v}\right)\in\mathbb{A}^{s}.
$$
\end{description}
Here, the message functions $\textsc{Msg}^{(t)}$ and update functions
$\textsc{Upd}^{(t)}$ are general (computable) functions. After round $d$, we
define the final labelling $\pmb{\ell}:V\to\mathbb{A}^{s}$ as
$\pmb{\ell}_v:=\pmb{\ell}^{(d)}_v$ for every $v\in V$. If further aggregation
over the entire graph is needed, e.g., for graph classification, an additional
readout function $\textsc{ReadOut}(\ldbl\pmb{\ell}_v\mid v\in V\rdbl)$ can be
applied. We omit the readout function here since most of the computation
happens during the rounds of an MPNN.

\subsection{Examples}
We illustrate MPNNs by a number of examples in which the message functions leverage an increasing amount of information of the vertices involved. 

\paragraph{Anonymous MPNNs.}
We start with two examples of so-called \textit{anonymous}  MPNNs. These are MPNNs whose message functions do not depend on the vertices $v$ and $u$. Phrased otherwise, anonymous MPNNs have message functions only depending on $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_u^{(t-1)}$.


\begin{example}[GNN architectures]\label{ex:GNN}
We first consider
the graph neural network
architectures~\cite{hyl17,grohewl} defined by:
\begin{equation}
\mathbf{L}^{(t)}:=\sigma\left(\mathbf{L}^{(t-1)}\mathbf{W}_1^{(t)}+\mathbf{A}\mathbf{L}^{(t-1)}\mathbf{W}_2^{(t)}+\mathbf{B}^{(t)}\right), \label{gnn:grohe}
\end{equation}
where $\mathbf{L}^{(t)}$ is the $n\times s$-matrix in $\mathbb{A}^{n\times s}$ consisting of the $n$ rows $\pmb{\ell}^{(t)}_v$, for $v\in V$, $\mathbf{A}\in\mathbb{A}^{n\times n}$ is the adjacency matrix of $G$, $\mathbf{W}_1^{(t)}$ and $\mathbf{W}_2^{(t)}$ are (learnable) weight matrices in $\mathbb{A}^{s\times s}$,
$\mathbf{B}^{(t)}$ is a bias matrix in $\mathbb{A}^{n\times s}$ consisting of $n$ copies of the same row $\mathbf{b}^{(t)}\in \mathbb{A}^s$, and $\sigma$ is a non-linear activation function. We can regard this architecture as an MPNN. Indeed,~(\ref{gnn:grohe}) can be equivalently phrased as the architecture which computes, in round $t$, for each vertex $v\in V$ the label defined by:
\[
\pmb{\ell}^{(t)}_v:=\sigma\Bigl(\pmb{\ell}^{(t-1)}_v\mathbf{W}_1^{(t)}+ \sum_{u\in N_G(v)}\pmb{\ell}^{(t-1)}_u\mathbf{W}_2^{(t)}+\mathbf{b}^{(t)} \Bigr),
\]
where we identified the labellings with their images, i.e., a row vector in $\mathbb{A}^s$. 
To phrase this as an MPNN, it suffices to define for each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{A}^s$, each $v\in V$ and $u\in N_G(v)$, and each $t\geq 1$:
\begin{equation*}
	\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}_2^{(t)}
\text{ and } 
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\sigma\left(\mathbf{x}\mathbf{W}_1^{(t)}+\mathbf{y} + \mathbf{b}^{(t)}\right).
\end{equation*} 

We observe that  message functions indeed do not depend on $v$ and $u$. \qed
\end{example}
Another example of an anonymous MPNN originates from the Weisfeiler-Lehman procedure described in the preliminaries.
\begin{example}[Weisfeiler-Lehman]\label{ex:WL}
We recall that WL computes, in round $t \geq 1$, for each vertex $v\in V$ the label:
$$
\pmb{\ell}^{(t)}_v:=\textsc{Hash}\left(\pmb{\ell}^{(t-1)}_v,\ldbl \pmb{\ell}_u^{(t-1)} \st u \in N_G(v) \rdbl\right).
$$
In fact, any injective function $f$ can replace $\textsc{Hash}(\cdot,\cdot)$
above We thus cast the WL procedure as an anonymous MPNN by using an injection
$h^{(t)} : \mathbb{A}^s \to \mathbb{Q}$.  What follows is in fact an
adaptation of Lemma 5 from~\cite{xhlj19} itself based on~\cite[Theorem
2]{ZaheerKRPSS17}.

Let $\tau : \mathbb{A}^s \to \mathbb{N}^+$ be a computable injective function
such as, for instance, the Cantor tuple function.
We now define $h^{(t)}$, for all $t \in \mathbb{N}$, as the mapping
$\mathbf{x} \mapsto (n+1)^{-\tau(\mathbf{x})}$. Note that $h^{(t)}$ is indeed
injective and that it can be seen as a one-hot $(n+1)$-ary representation of
vectors from $\mathbb{A}^s$ --- that is, a single $(n+1)$-ary digit is set to
the nonzero value $n$.
Crucially, for all $t$, it satisfies that $0 < n \cdot h^{(t)}(\mathbf{x})
< 1$. It follows that the function mapping a multiset $S$ of elements of
$\mathbb{A}^s$ to $\sum_{\mathbf{x} \in S} h^{(t)}(\mathbf{x})$ is a
bijection, for any $t$. To conclude, we set 
\[
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},v,u) :=
(h^{(t)}(\mathbf{y}),\mathbf{0}) \text{ and }
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}) := \left(t +
\frac{h^{(t)}(\mathbf{x})}{2} + \frac{h^{(t)}(\mathbf{y})}{2},
\mathbf{0}\right).
\]
We observe that the message functions functions indeed do not depend on $v$
and $u$ and that, together with the update functions, precisely implement the
specified $\textsc{Hash}$ function.
\floris{I don't understand it yet ;-)}
\qed
 \end{example}

\paragraph{Degree-aware MPNNs.} In our next example, the message functions use a bit more information. More specifically they use  degree information of the vertices.
MPNNs whose message functions depend on 
 $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_u^{(t-1)}$, $d_v$ and $d_u$ will be referred to as \textit{degree-aware} MPNNs. 

\begin{example}[GCNs by Kipf and Welling]\label{ex:KipfasMPNN}
We consider the GCN architecture by~\cite{kipf-loose}, which in round $t$ computes
\begin{equation*}
\mathbf{L}^{(t)} = \sigma\left(
   \tilde{\mathbf{D}}^{-1/2}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-1/2}\mathbf{L}^{(t-1)}\mathbf{W}_1^{(t-1)}
  \right),
\end{equation*}
where $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$ and $\tilde{\mathbf{D}}$ is the diagonal matrix defined by $\tilde{\mathbf{D}}_{vv} = d_{v} + 1$. 
This means that, in round $t$,
for each vertex $v\in V$ it computes the label:
\begin{equation}
\pmb{\ell}^{(t)}_v:=\sigma\left(\left(\frac{1}{1+d_v}\right)\pmb{\ell}_v^{(t-1)}\mathbf{W}^{(t)} + \sum_{u\in N_G(v)} \left(\frac{1}{\sqrt{1+d_v}}\right)\left(\frac{1}{\sqrt{1+d_u}}\right)\pmb{\ell}^{(t-1)}_u\mathbf{W}^{(t)}\right), \label{GNN:Kipf}
\end{equation}
where $\mathbf{W}$ is a learnable weight matrix in $\mathbb{A}^{s\times s}$ and $\sigma$ is a non-linear activation function.
We can regard this architecture again as an MPNN. Indeed, it suffices to define for each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{A}^s$, each $v\in V$ and $u\in N_G(v)$, and each $t\geq 1$:
\begin{align*}
\textsc{Msg}^{(t)}\left(\mathbf{x},\mathbf{y},v,u\right)&:=
\frac{1}{d_v}\left(\frac{1}{1+d_v}\right)\mathbf{x}\mathbf{W}^{(t)}+
\left(\frac{1}{\sqrt{1+d_v}}\right)\left(\frac{1}{\sqrt{1+d_u}}\right)\mathbf{y}\mathbf{W}^{(t)}
\intertext{and} \textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y})&:=\sigma(\mathbf{y}).
\end{align*}
We remark that the initial factor $1/d_v$ in the message functions is introduced for renormalisation purposes.
We indeed observe that the message functions depend only on $\pmb{\ell}^{(t-1)}_v$, 
$\pmb{\ell}^{(t-1)}_u$, and the degrees $d_v$ and $d_u$ of the vertices $v$ and $u$, respectively.\qed
\end{example}


\paragraph{Turing complete MPNNs.}
 As a final example, we show how an MPNN can compute any computable function when the message functions may use knowledge of which vertices are under consideration. 
 \todo{G: what do we mean by Turing completeness? It seems not to be that we
   encode the tape of a TM into a labelled graph and then compute the original
 function on that\dots}
\begin{example}[Turing complete MPNN]
	Let $( G,\pmb{\nu})$ be a connected labelled graph with $\pmb{\nu}:V\to\mathbb{A}^r$ and let $\mathbf{f}_G:V\to\mathbb{A}$ be a labelling 
	obtained from a computable function on input 	$( G,\pmb{\nu})$. To compute $\mathbf{f}_G$ by means of an MPNN we first use a number $d$ of rounds to ensure that $\pmb{\ell}^{(d)}_v$ represents $( G,\pmb{\nu})$. In other words, after $d$ rounds, each vertex has the entire input labelled graph as label. An update function simulating $\mathbf{f}_G$ then suffices to compute $\mathbf{f}_G$.
	%We assume that $V=[n]$ and use $i\in[n]$ to denoted the $i$th vertex in $V$.
	
To encode $( G,\pmb{\nu})$ in the labels of vertices we use labels in $\mathbb{A}^s$ with 
$s=r+1+n(r+2)+n^2$. The aim it to ensure that for each vertex $i$, the final label $\pmb{\ell}_i^{(d)}$ is of the form
\begin{equation}
\left(\underbrace{\pmb{\nu}_i,1,}_{\text{initial label}}\underbrace{1,\pmb{\nu}_1,1,}_{\text{vertex $i$}}\ldots,\underbrace{n,\pmb{\nu}_n,1,}_{\text{vertex $n$}}\textsc{Vect}(\mathbf{A})\right), \label{eq:graphinlabel}
\end{equation}

where the first $r$ positions hold the initial label $\pmb{\nu}_i$,
the $(r+1)$-th position holds a counter (in order to remember how many times
this information has been passed on), for each $i\in[n]$, position
$i(r+2)$ holds the vertex id `$i$', positions $i(r+2)+1$ up to $(i+1)(r+2)-2$
hold the label $\pmb{\nu}_i$, and position $(i+1)(r+2)-1$ holds again a counter.
The remaining $n^2$ positions hold a vectorised representation of the adjacency matrix $\mathbf{A}$ obtained by concatenating its rows denoted by $\textsc{Vect}(\mathbf{A})$. Clearly, one can extract 
$(G,\pmb{\nu})$ from the label~(\ref{eq:graphinlabel}). We now show how this label can be obtained.

For convenience, we initially extend $\pmb{\nu}$
to a labelling $V\to\mathbb{A}^s$ by padding each $\pmb{\nu}_i$ with $s-r$ zeroes. We abuse notation and also refer to this initial labelling by $\pmb{\nu}$.

For each round $t$, vertices $i$ and $j\in N_G(i)$, and $\mathbf{x},\mathbf{y}\in\mathbb{A}^s$, we define
$
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},i,j)
$
as the function which attaches\footnote{Technically, we add to the label
$\textbf{x}$ a new vector encoding some new information.} the initial labels of
vertices $i$ and $j$, and adds edges $(i,j)$ and $(j,i)$ to the vectorised
encoding of the adjacency matrix.  More specifically, $
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},i,j)
$ returns:
\[
\mathbf{x}+(\mathbf{0}_{1\times r},1,\ldots,\underbrace{i,\mathbf{x}_{1:r},1}_{\text{vertex $i$}},\ldots, \underbrace{j,\mathbf{y}_{1:r},1}_{\text{vertex $j$}},\ldots,\underbrace{1}_{\text{entry $(i,j)$}},\ldots,\underbrace{1}_{\text{entry $(j,i)$}},\ldots),
\]
where by `entry $(i,j)$' we mean the corresponding entry of $\mathbf{A}$ which in turn concerns position $r+1+n(r+2)+(i-1)n+j$ in our vectorised encoding;
and the entry $(j,i)$
concerns position $r+1+n(r+2)+(j-1)n+i$. We then define
$\textsc{Upd}^{(t)}(\cdot,\cdot)$ as the
function which returns on input 
\[
\mathbf{x}, \left(\mathbf{y}_0,c_0,i_1,\mathbf{y}_1,c_1,\ldots,i_n,\mathbf{y}_n,c_n, \underbrace{a_{11},\ldots,a_{nn}}_{\text{last $n^2$ entries}}\right)
\]
the vector
\[
\left(\frac{1}{c_0}\mathbf{y}_0,\delta(c_0),i_1,\frac{1}{c_1}\mathbf{y}_1,\delta(c_1),\ldots,
i_n,\frac{1}{c_n}\mathbf{y}_n,\delta(c_n),\delta(a_{11}),\ldots,\delta(a_{nn})\right),
\]
where, for convenience, we let $\frac{1}{0}=0$ and
set $\delta(x)=1$ if $x\neq 0$ and $\delta(x)=0$ otherwise. It is easily verified that after $d$ rounds, where $d$ is the diameter of $G$, each vertex will carry
the desired label~(\ref{eq:graphinlabel}). It now suffices to incorporate $\mathbf{f}_G$ in the last update function to ensure that $\pmb{\ell}_i^{(t)}:=(\mathbf{f}_G(i),\mathbf{0})\in\mathbb{A}^s$.\qed
\end{example}
We remark that the Turing-completeness of MPNNs, whose message functions can access  the vertices themselves, was recently shown by~\cite{Loukas2019}  using close connections with the LOCAL model for distributed graph computations of~\cite{Angluin}, which is known to be complete. The previous example provides a direct proof.

In the next section we recall results concerning the distinguishing power of anonymous MPNNs and establish an upper bound on the distinguishing power of degree-aware MPNNs.

\subsection{On the choice of formalism}
The expert reader may have noticed that we use a different formalisation of MPNNs than the one given in~\cite{GilmerSRVD17}. More specifically, we explicitly allow a dependency of the message functions on $v$ and $u\in N_G(v)$. The reason is that there is a certain ambiguity in the formalisation in ~\cite{GilmerSRVD17} on what precisely the message functions can depend on. More specifically, only a dependence on  $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_u^{(t-1)}$
is specified. In contrast, the examples given in~\cite{GilmerSRVD17} use more information, such as the degree of vertices. Another difference is that the MPNNs in ~\cite{GilmerSRVD17} work on graphs that carry both vertex and edge labels. We ignore edge labellings in this paper but all our upper-bound results carry over to that more general setting. Indeed, it suffices to use the extension of the Weisfeiler-Lehman algorithm for edge-labelled graphs~\cite{Jaume2019}. Our formalisation also differs from the one given by~\cite{Loukas2019} in that we only exchange messages from $u\in N_G(v)$
to $v$. In~\cite{Loukas2019}, every vertex can also send itself a message. We provide this functionality by parametrizing the update functions with the current label of the vertex itself, just as in~\cite{GilmerSRVD17}. One can verify that both formalisations are equivalent.
