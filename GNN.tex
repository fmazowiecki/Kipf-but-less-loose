%!TEX root =main.tex

\section{Message Passing Neural Networks}
We start by describing message passing neural networks (MPNNs) for  deep learning on graphs, introduced by \cite{GilmerSRVD17}. Roughly speaking, in MPNNs, vertex features are propagated through a graph according to its connectivity structure. MPNNs are known to model a variety of graph neural network architectures commonly used in practice.

\subsection{Definition}
Given a labeled graph $\langle G,\pmb{\nu}\rangle$, an MPNN computes a vertex labeling $\pmb{\ell}:V\to \mathbb{Q}^{s}$, for some $s\in\mathbb{N}^+$, by means of a number of rounds of computation, starting from the input labeling $\pmb{\nu}:V\to\mathbb{Q}^s$.

The vertex labeling computed by an MPNN after round $t$ is denoted by $\pmb{\ell}^{(t)}$. We next detail how $\pmb{\ell}^{(t)}$ is computed.
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Initialisation.]  We let $\pmb{\ell}^{(0)}:=\pmb{\nu}$.
\end{description}
Then, for round $t=1,2,\ldots,d$, we define $\pmb{\ell}^{(t)}:V\to\mathbb{Q}^{s}$, as follows\footnote{We can assume, without loss of generality, that every round assigns labels in $\mathbb{Q}^s$ for the same $s$. If not, one can include ``padding with zeroes'' in the message and update functions.}:
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Message Passing.] Each vertex $v\in V$ receives messages from its neighbours which are subsequently aggregated:
$$
\mathbf{m}^{(t)}_{v}:=\sum_{u\in N_G(v)}\textsc{Msg}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_u,v,u\right)\in\mathbb{Q}^{s}.
$$
\item [Updating.] Each vertex $v\in V$ further updates $\mathbf{m}^{(t)}_{v}$ possibly based on its current label $\pmb{\ell}^{(t-1)}_v$:
$$
\pmb{\ell}^{(t)}_v:=\textsc{Upd}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\mathbf{m}^{(t)}_{v}\right)\in\mathbb{Q}^{s}.
$$
\end{description}
Here, the message functions $\textsc{Msg}^{(t)}$ and update functions $\textsc{Upd}^{(t)}$ are general (computable) functions.
After round $d$, we define the final labeling $\pmb{\ell}:V\to\mathbb{Q}^{s}$ as  $\pmb{\ell}_v:=\pmb{\ell}^{(d)}_v$ for every $v\in V$. If further aggregation over the entire graph is needed, e.g., for graph classification, an additional readout function 
$\textsc{ReadOut}(\ldbl\pmb{\ell}_v\mid v\in V\rdbl)$ can be applied. We omit the readout function here since most of the computation happens during the rounds of an MPNN.

\subsection{Examples}
We illustrate MPNNs by a number of examples in which the message functions leverage an increasing amount of information of the vertices involved. 

\paragraph{Anonymous MPNNs.}
We start with two examples of so-called \textit{anonymous}  MPNNs. These are MPNNs whose message functions do not depend on the vertices $v$ and $u$. Phrased otherwise, anonymous MPNNs have message functions only depending on $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_u^{(t-1)}$.
 % will be referred to as \textit{anonymous MPNNs}, since the message functions do not know which vertices are being considered.
\begin{example}[GNN architectures]\normalfont
We first consider
the graph neural network
architectures~\cite{Hamilton2017a,grohewl} defined by:
\begin{equation}
\mathbf{L}^{(t)}:=\sigma\left(\mathbf{L}^{(t-1)}\mathbf{W}_1^{(t)}+\mathbf{A}_G\mathbf{L}^{(t-1)}\mathbf{W}_2^{(t)}+\mathbf{B}^{(t)}\right), \label{gnn:grohe}
\end{equation}
where $\mathbf{L}^{(t)}$ is the $n\times s$-matrix in $\mathbb{Q}^{n\times s}$, consisting of the $n$ rows $\pmb{\ell}^{(t)}_v$, for $v\in V$, $\mathbf{A}_G\in\mathbb{Q}^{n\times n}$ is the adjacency matrix of $G$, $\mathbf{W}_1^{(t)}$ and $\mathbf{W}_2^{(t)}$ are (learnable) weight matrices in $\mathbb{Q}^{s\times s}$,
$\mathbf{B}^{(t)}$ is a bias matrix in $\mathbb{Q}^{n\times s}$ consisting of $n$ copies of the same row $\mathbf{b}^{(t)}\in \mathbb{Q}^s$, and $\sigma$ is a non-linear activation function. We can regard this architecture as an MPNN. Indeed,~(\ref{gnn:grohe}) can be equivalently phrased as the architecture which computes, in round $t$, for each each vertex $v\in V$ the label defined by:
$$
\pmb{\ell}^{(t)}_v:=\sigma\Bigl(\pmb{\ell}^{(t-1)}_v\mathbf{W}_1^{(t)}+ \sum_{u\in N_G(v)}\pmb{\ell}^{(t-1)}_u\mathbf{W}_2^{(t)}+\mathbf{b}^{(t)} \Bigr),
$$
where we identified the labelings with their images, i.e., a row vector in $\mathbb{Q}^s$. 
To phrase this as an MPNN, it suffices to define for each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{Q}^s$, each $v\in V$ and $u\in N_G(v)$, and each $t\geq 1$:
\begin{equation*}
	\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}_2^{(t)}
\text{ and } 
\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\sigma\left(\mathbf{x}\mathbf{W}_1^{(t)}+\mathbf{y} + \mathbf{b}^{(t)}\right).
\end{equation*} 
We observe that  message functions indeed do not depend on $v$ and $u$. \qed
\end{example}
Another example of an anonymous MPNN originates from the Weisfeiler-Lehman procedure described in the preliminaries.
\begin{example}[Weisfeiler-Lehman]\normalfont
We recall that WL computes, in round $t$, for each vertex $v\in V$ the label:
$$
\pmb{\ell}^{(t)}_v:=\textsc{Hash}\left(\pmb{\ell}^{(t-1)}_v,\ldbl \pmb{\ell}_u^{(t-1)} \st u \in N_G(v) \rdbl\right).
$$
We can cast this as an anonymous MPNN, as follows. 
\floris{Please complete.}
\qed
 \end{example}

% MPNNs with update functions only depending on $\pmb{\ell}_v^{(t)}$ and $\pmb{\ell}_u^{(t)}$ will be referred to as \textit{anonymous MPNNs}, since the message functions do not know which vertices are being considered.



\paragraph{Degree-aware MPNNs.} In our next example, the message functions use a bit more information. More specifically they use  degree information of the vertices.
MPNNs whose message functions depend on 
 $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_u^{(t-1)}$, $d_v$ and $d_u$ will be referred to as \textit{degree-aware} MPNNs. 

\begin{example}[GCN by Kipf and Welling]\label{ex:KipfasMPNN}\normalfont
We consider the GCN architecture by~\cite{kipf-loose}, which in round $t$ computes for each vertex $v\in V$ the label:
\begin{equation}
\pmb{\ell}^{(t)}_v:=\sigma\Bigl(\bigl(\frac{1}{1+d_v}\bigr)\pmb{\ell}_v^{(t-1)}\mathbf{W}^{(t)} + \sum_{u\in N_G(v)} \bigl(\frac{1}{\sqrt{1+d_v}}\bigr)\bigl(\frac{1}{\sqrt{1+d_u}}\bigr)\pmb{\ell}^{(t-1)}_u\mathbf{W}^{(t)}\Bigr), \label{GNN:Kipf}
\end{equation}
where $\mathbf{W}$ is a learnable weight matrix in $\mathbb{Q}^{s\times s}$ and $\sigma$ is a non-linear activation function.
We can  regarded this architecture again as an MPNN. Indeed, it suffices to define for each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{Q}^s$, each $v\in V$ and $u\in N_G(v)$, and each $t\geq 1$:
\begin{align*}
\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u)&:=
\frac{1}{d_v}\bigl(\frac{1}{1+d_v}\bigr)\mathbf{x}\mathbf{W}^{(t)}+
\bigl(\frac{1}{\sqrt{1+d_v}}\bigr)\bigl(\frac{1}{\sqrt{1+d_u}}\bigr)\mathbf{y}\mathbf{W}_2^{(t)}
\intertext{and} \textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y})&:=\sigma(\mathbf{y}).
\end{align*}
We remark that the initial factor $1/d_v$ in the message functions is introduced for renormalisation purposes.
We indeed observe that the message functions depend on $\pmb{\ell}^{(t-1)}_v$, 
$\pmb{\ell}^{(t-1)}_u$ and the degrees $d_v$ and $d_u$ of the vertices $v$ and $u$, respectively.\qed
\end{example}


\paragraph{Turing complete MPNNs.}
 As a final example, we show how an MPNN can compute any computable function when the message functions can use knowledge of which vertices are under consideration. 

\begin{example}[Turing complete MPNN]\normalfont
	Let $\langle G,\pmb{\nu}\rangle$ be a connected labeled graph with $\pmb{\nu}:V\to\mathbb{Q}^t$ and let $\mathbf{f}_G:V\to\mathbb{Q}$ be a labeling 
	computed by a computable function on input 	$\langle G,\pmb{\nu}\rangle$. To compute $\mathbf{f}_G$ by means of an MPNN we first use a number  of $d$ rounds  to ensure that $\pmb{\ell}^{(d)}_v$ represents $\langle G,\pmb{\nu}\rangle$. In other words, after $d$ rounds,  each vertex has the entire input labeled graph as label. An update function simulating $\mathbf{f}_G$ then suffices to computer $\mathbf{f}_G$. We assume that $V=[n]$ and use $i\in[n]$ to denoted the $i$th vertex in $V$.
	
To encode $\langle G,\pmb{\nu}\rangle$ in the labels of vertices we use labels in $\mathbf{Q}^s$ with 
$s=t+1+n(t+2)+n^2$. The aim it to ensure that for each vertex $i$, the final label $\pmb{\ell}_i^{(d)}$ is of the form
\begin{equation}
(\underbrace{\vphantom{f}\pmb{\nu}_i,1}_{\text{initial label}},\underbrace{\vphantom{f}1,\pmb{\nu}_1,1}_{\text{vertex $i$}},\ldots,\underbrace{\vphantom{f}n,\pmb{\nu}_n,1}_{\text{vertex $n$}},\textsc{Vect}(\mathbf{A}_G)), \label{eq:graphinlabel}
\end{equation}
where the first $t$ positions hold the initial label $\pmb{\nu}_i$,
the $(t+1)$st position holds a counter (in order to remember how many times
this information has been passed on), for each $i\in[n]$, position
$i(t+2)$ holds the vertex id ($i$), positions $i(t+2)+1$ up $(i+1)(t+2)-2$
hold label $\pmb{\nu}_i$, and position $(i+1)(t+2)-1$ holds again a counter.
The remaining $n^2$ positions hold a vectorised representation of the adjacency matrix $\mathbf{A}_G$ obtained by concatenating its rows denoted by $\textsc{Vect}(\mathbf{A}_G)$. Clearly, one can extract 
$\langle G,\pmb{\nu}\rangle$ from the label~(\ref{eq:graphinlabel}). We now show how this label can be obtained.

For convenience, we initially extend $\pmb{\nu}$
to a labeling $V:\to\mathbb{Q}^s$ by padding each $\pmb{\nu}_i$ with $s-t$ zeroes. We abuse notation and also refer to this initial labeling by $\pmb{\nu}$.

For each round $t$, vertices $i$ and $j\in N_G(i)$, and $\mathbf{x},\mathbf{y}\in\mathbb{Q}^s$, we define
$
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},i,j)
$
as the function which attaches the initial labels to vertices $i$ and $j$ and adds edges $(i,j)$ and $(j,i)$ to the vectorised encoding of the adjacency matrix.
More specifically, $
\textsc{Msg}^{(t)}(\mathbf{x},\mathbf{y},i,j)
$ returns:
$$
\mathbf{x}+(\mathbf{0}_{1\times t},0,\ldots,\underbrace{i,\mathbf{x}_{1:t},1}_{\text{vertex $i$}},\ldots, \underbrace{j,\mathbf{y}_{1:t},1}_{\text{vertex $j$}},\ldots,\underbrace{1}_{\text{entry $(i,j)$}},\ldots,\underbrace{1}_{\text{entry $(j,i)$}},\ldots),
$$
where entry $(i,j)$ concerns position $t+1+n(t+2)+(i-1)n+j$ and the entry $(j,i)$
concerns position $t+1+n(t+2)+(j-1)n+i$. We then define
$\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y})$ as the
function which returns on input 
$$
(\mathbf{y}_0,c_0,i_1,\mathbf{y}_1,c_1,\ldots,i_n,\mathbf{y}_n,c_n, \underbrace{c_{11},\ldots,c_{nn}}_{\text{last $n^2$ slots}})
$$
the vector
$$
(\frac{1}{c_0}\mathbf{y}_0,\delta(c_0),i_1,\frac{1}{c_1}\mathbf{y}_1,\delta(c_1),\ldots,
i_n,\frac{1}{c_n}\mathbf{y}_n,\delta(c_n),\delta(c_{11}),\ldots,\delta(c_{nn})),
$$
where we assume that $\frac{1}{c}=0$ when $c=0$ and $\delta(x)=1$ if $x\neq 0$ and $\delta(x)=0$ otherwise. It is easily verified that after $d$ rounds, where $d$ is the diameter of $G$, each vertex will carry
the desired label~(\ref{eq:graphinlabel}). It now suffices incorporate $\mathbf{f}_G$ in the last update function to ensure that $\pmb{\ell}_i^{(t)}:=(\mathbf{f}_G(i),\mathbf{0})\in\mathbb{Q}^s$.\qed
\end{example}
We remark that the Turing-completeness of MPNNs, whose message functions can access  the vertices themselves, was recently shown by~\cite{Loukas2019}  using close connections with the LOCAL model for distributed graph computations of~\cite{Angluin}, which is known to be complete. The previous example provides a direct proof.

In the next section we recall results concerning the distinguishing power of anonymous MPNNs and establish an upper bound on the distinguishing power of degree-aware MPNNs.

\subsection{On the choice of formalism}
We use a different formalisation of MPNNs than given in~\cite{GilmerSRVD17}. More specifically, we explicitly allow a dependency of the message functions on $v$ and $u\in N_G(v)$. The reason is that there is a certain ambiguity in the formalisation in ~\cite{GilmerSRVD17} on what precisely the message functions can depend on. More specifically, only a dependence on  $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_u^{(t-1)}$
is specified. In contrast, the examples given in~\cite{GilmerSRVD17} use more information, such as the degree of vertices. Another difference is that the MPNNs in ~\cite{GilmerSRVD17} work on graphs that carry both vertex and edge labels. We ignore edge labelings in this paper but all our upper bound results carry over to this more general setting. Indeed, it suffices to use the extension of the Weisfeiler-Lehman algorithm for this more general class of graphs~\cite{Jaume2019}. Our formalisation also differs from the one given by~\cite{Loukas2019} in that we only exchange messages from $u\in N_G(v)$
to $v$. In~\cite{Loukas2019}, every vertex can also send itself a message. We provide this functionality by parametrizing the update functions with the current label of the vertex itself, just as in~\cite{GilmerSRVD17}. One can verify that both formalisations are equivalent.
