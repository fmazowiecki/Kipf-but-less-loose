%!TEX root =main.tex

\section{Message Passing Neural Networks}
We start by describing message passing neural networks (MPNNs) for  deep learning on graphs, introduced by \cite{GilmerSRVD17}. Roughly speaking, in MPNNs, vertex features are propagated through the graph according to its connectivity structure. 
An MPNN computes a vertex labeling $\pmb{\ell}:V\to \mathbb{Q}^{s}$, for some $s\in\mathbb{N}^+$, by means of a number of rounds of computation, starting from the input labeled graph $\langle G,\pmb{\nu}\rangle$.

The vertex labeling computed after round $t$ is denoted by $\pmb{\ell}^{(t)}$. We next detail how $\pmb{\ell}^{(t)}$ is computed.
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Initialisation.]  We let $\pmb{\ell}^{(0)}:=\pmb{\nu}$.
\end{description}
Then, for round $t=1,2,\ldots,d$, we define $\pmb{\ell}^{(t)}:V\to\mathbb{Q}^{s}$, as follows\footnote{We can assume, without loss of generality, that every round assigns labels in $\mathbb{Q}^s$ for the same $s$. If not, one can include ``padding with zeroes'' in the message and update functions.}:
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Message Passing.] Each vertex $v\in V$ receives messages from its neighbours which are subsequently aggregated:
$$
\mathbf{m}^{(t)}_{v}:=\sum_{u\in N_G(v)}\textsc{Msg}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_u,v,u\right)\in\mathbb{Q}^{s}.
$$
\item [Updating.] Each vertex $v\in V$ further updates $\mathbf{m}^{(t)}_{v}$ possibly based on its current label $\pmb{\ell}^{(t-1)}_v$:
$$
\pmb{\ell}^{(t)}_v:=\textsc{Upd}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\mathbf{m}^{(t)}_{v}\right)\in\mathbb{Q}^{s}.
$$
\end{description}
Here, $\textsc{Msg}^{(t)}$ and $\textsc{Upd}^{(t)}$ are general (computable) functions.
After round $d$, we define the final labeling $\pmb{\ell}:V\to\mathbb{Q}^{s}$ as  $\pmb{\ell}_v:=\pmb{\ell}^{(d)}_v$ for every $v\in V$. If further aggregation over the entire graph is needed, e.g., for graph classification, an additional readout function 
$\textsc{ReadOut}(\ldbl\pmb{\ell}_v\mid v\in V\rdbl)$ can be applied. We omit the readout function here since most of the computation happens during the rounds of an MPNN.

We illustrate MPNNs by a number of examples, in which the message functions leverage an increasing amount of information from the vertices involved.
\begin{example}\normalfont
We first consider
the graph neural network
architecture~\cite{Hamilton2017a,grohewl} defined by:
\begin{equation}
\mathbf{L}^{(t)}:=\sigma\left(\mathbf{L}^{(t-1)}\mathbf{W}_1^{(t)}+\mathbf{A}_G\mathbf{L}^{(t-1)}\mathbf{W}_2^{(t)}+\mathbf{B}^{(t)}\right), \label{gnn:grohe}
\end{equation}
where $\mathbf{L}^{(t)}$ is the $n\times s$-matrix consisting of the rows $\pmb{\ell}^{(t)}_v$, for $v\in V$, $\mathbf{A}_G$ is the adjacency matrix of $G$, $\mathbf{W}_1^{(t)}$ and $\mathbf{W}_2^{(t)}$ are (learnable) weight matrices,
$\mathbf{B}^{(t)}$ is a bias matrix consisting of $n$ copies of the same row $\mathbf{b}^{(t)}$, and $\sigma$ is a non-linear activation function. We can regard this architecture as an MPNN. Indeed,~(\ref{gnn:grohe}) can be equivalently phrased as the architecture which computes, in round $t$, for each each vertex $v\in V$ the label defined by:
$$
\pmb{\ell}^{(t)}_v:=\sigma\Bigl(\pmb{\ell}^{(t-1)}_v\mathbf{W}_1^{(t)}+ \sum_{u\in N_G(v)}\pmb{\ell}^{(t-1)}_u\mathbf{W}_2^{(t)}+\mathbf{b}^{(t)} \Bigr).
$$
To phrase this as an MPNN, it suffices to define for each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{Q}^s$, each $v\in V$ and $u\in N_G(v)$, and each $t\geq 1$:
$$\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{y}\mathbf{W}_2^{(t)}
$$
and 
$$\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\mathbf{x}\mathbf{W}_1^{(t)}+\mathbf{y} + \mathbf{b}^{(t)}.
$$
\qed
\end{example}
%
We observe that in the previous example the message functions do not depend on $v$ and $u$. MPNNs with update functions only depending on $\pmb{\ell}_v^{(t)}$ and $\pmb{\ell}_u^{(t)}$ will be referred to as \textit{anonymous MPNNs}, since they do not know which vertices are being considered.
In our next example, the message functions used a bit more information. More specifically the can use the degrees of the vertices.

\begin{example}\normalfont
Similarly, the GCN architecture by~\cite{kipf-loose}, which in round $t$ computes for each vertex $v\in V$ the label:
\begin{equation}
\pmb{\ell}^{(t)}_v:=\sigma\Bigl(\bigl(\frac{1}{1+d_v}\bigr)\pmb{\ell}_v^{(t-1)}\mathbf{W}^{(t)} + \sum_{u\in N_G(v)} \bigl(\frac{1}{\sqrt{1+d_v}}\bigr)\bigl(\frac{1}{\sqrt{1+d_u}}\bigr)\pmb{\ell}^{(t-1)}_u\mathbf{W}^{(t)}\Bigr), \label{GNN:Kipf}
\end{equation}
where $\mathbf{W}$ is a learnable weight matrix and $\sigma$ is a non-linear activation function, can be regarded as an MPNN. Indeed, it suffices to define for each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{Q}^s$, each $v\in V$ and $u\in N_G(v)$, and each $t\geq 1$:
$$\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=
\frac{1}{d_v}\bigl(\frac{1}{1+d_v}\bigr)\mathbf{x}\mathbf{W}^{(t)}+
\bigl(\frac{1}{\sqrt{1+d_v}}\bigr)\bigl(\frac{1}{\sqrt{1+d_u}}\bigr)\mathbf{y}\mathbf{W}_2^{(t)}
$$
and 
$$\textsc{Upd}^{(t)}(\mathbf{x},\mathbf{y}):=\mathbf{y}.
$$
\qed
\end{example}
We indeed observe that the message functions depend on the degrees of the vertices $v$ and $u$. MPNNs whose message functions depend on 
 $\pmb{\ell}_v^{(t)}$, $\pmb{\ell}_u^{(t)}$, $d_v$ and $d_u$ will be referred to as \textit{degree-aware} MPNNs. As a final example, we show how an MPNN can compute any computable function. 

\begin{example}\normalfont
We will construct	
	
Assume that $\nu:V\to \mathbf{Q}^{s}$ with $s=n^2$. We can always assume this by padding with zeroes.
We  will construct an MPNN that computes a labeling which represents the adjacency matrix $\mathbf{A}_G$ of $G$. We here use a vectorised representation of the adjacency matrix obtained by concatenating its rows.
We define for 
each $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{Q}^s$ and each $v\in V$ and $u\in N_G(v)$: 
$$
\textsc{Msg}^{(t)}\bigl(\mathbf{x},\mathbf{y},v,u):=\mathbf{e}_{v,w},
$$
where $\mathbf{e}_{v,w}$ is a binary row vector in $\mathbf{Q}^{s}$ with entries equal to $1$ on positions $vn+w$ and $wn+v$ and all other entries equal to $0$. Then,
clearly
\end{example}

For a given labeled graph $\langle G,\pmb{\nu},\pmb{\eta})$ and MPNN $M$ we denote by 
$\pmb{\ell}_M^{(t)}$ the vertex labeling computed by $M$ after $t$ rounds. In other words, we always assume that it is clear which labeled graph is considered.

We remark that formalisation of MPNNs given here differs slightly from the formalisation presented by \cite{GilmerSRVD17}. We show in the appendix that the two formalisms are equivalent, provided that the message functions in \cite{GilmerSRVD17} are allowed to take vertices $v$ and $w$ as arguments. In fact, there is a certain ambiguity on what the message functions in the MPNNs of \cite{GilmerSRVD17} can depend. More precisely, in~\cite{GilmerSRVD17}, only $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_w^{(t-1)}$ and $\pmb{\eta}_{\{v,w\}}$ are specified as arguments. Nevertheless, the examples given in  \cite{GilmerSRVD17} illustrating which graph neural network architectures can be captured by MPNNs use message functions that may depend on, e.g., the degree  of vertices. As we will see, this has an impact on the distinguishing power of MPNNs.

We will compare different classes of MPNNs.  Classes of MPPNs can be defined, e.g., by restricting the allowed message and update functions in MPNNs. We will see  examples of MPNN classes later in the paper. 

The following definition states when one class of MPNNs is weaker (in terms of distinguishing power) than another class of MPNNs. Intuitively, one class $\architecture$ will be weaker than other class $\architecture'$ if any MPNN $M\in\architecture$ cannot distinguish more vertices than some MPNN $M'\in\architecture'$.

\begin{definition}\label{def:comparing}\normalfont
Consider two classes $\architecture$ and $\architecture'$ of MPNNs. We say that $\architecture$ is weaker than $\architecture'$ (or equivalently that $\architecture'$ is stronger than $\architecture$) if there exists a function $g:\mathbb{N}\to \mathbb{N}$ such that for every $M \in \architecture$ there exists an $M'\in \architecture'$ satisfying $\pmb{\ell}_{M'}^{(g(t))}\sqsubseteq \pmb{\ell}_{M}^{(t)}$, for all $t\geq 0$ and for any labeled graph $\langle G,\pmb{\nu},\pmb{\eta})$. This is denoted by $\architecture \sqsubseteq \architecture'$.
% with $d$ layers 
% and every labelling $\labl$ compatible with $g$ there exists $g' \in \architecture'$ with $m'$ layers such that $g'(\labl) \sqsubseteq g(\labl)$.
% This is denoted by $\architecture' \sqsubseteq \architecture$.
We write $\architecture \not \sqsubseteq \architecture'$ if the above property does not hold. 
If we additionally require that the function $g:\mathbb{N}\to\mathbb{N}$ satisfies 
$g(t)\le t +c$ for some constant $c$, then we write that $\architecture \sqsubseteq \architecture'$ up to a constant factor $c$. In the particular case when this holds for $c = 0$ we write that $\architecture \sqsubseteq \architecture'$ with no factor. Similarly, if there exist constants $c, c'$ such that $g(t) \le ct + c'$ we write that $\architecture \sqsubseteq \architecture'$ up to a linear factor $c$.
\end{definition}

Of particular interest will be the class of MPNNs corresponding to the WL algorithm. 
\floris{Ok, add aMPNN description of WL. Note that I added the description of the WL algorithm on undirected graphs but with each labels to the prelims. }
We will denote this class by $\architectureWL$ and it is defined as MPNNs in which the message 
We say that an architecture $\architecture$ is \emph{bounded by WL} if $\architecture\sqsubseteq \architectureWL$. We also say that an architecture $\architecture$ is \emph{WL-strong} if $\architectureWL \sqsubseteq \architecture$. The definitions of bounded by WL and WL-strong up to linear and constant factors carry on in the obvious way.


\section{PREVIOUS VERSION: Graph neural networks}
Neural networks are a model to compute functions $f : \Rb^s \to \Rb^k$ obtained as a composition of layers $l^1,\ldots, l^m$.  Slightly abusing the notation we will identify a GNN with the function $g$ it computes. Every layer $l^t$ computes a function $f^{(t)} : \Rb^{s} \to \Rb^{k_t}$ based on the previous layer, where $k_0 = s$ and $k_m =k$. To ease the presentation we assume that $f^{(0)} : \Rb^s \to \Rb^s$ is the identity function. Formally, a neural network is $g = l^m \circ \ldots \circ l^1 $ and every layer defines $f^{(t)} = l^t(f^{(t-1)})$ as follows
\begin{equation}\label{eq:NN}
 f^{(t)} (\vec{x}) = \sigma \left(\bW^{(t)} f^{(t-1)}(\vec{x}) + b^{(t)}  \right),
\end{equation}
where: $\bW^{(t)} \in \Rb^{k_{t} \times k_{t-1}}$ is called the \emph{weight}; $b^{(t)} \in \Rb^{k_t}$ is called the \emph{bias}; and $\sigma$ is an \emph{activation function}. Activation functions are nonlinear transformations that are applied pointwise. We will discuss them in more detail later. We will study several architectures of neural networks that will be variants of~\eqref{eq:NN}, which is a standard presentation of a neural network~\cite{?}.

In this paper we are interested in architectures of \emph{graph neural networks} (GNN in short). 
There are many different architectures for GNNs in the literature and the goal of this paper is to compare these architectures in terms of expressiveness. In this paper we will present GNNs as a model computing labellings of a graph. However, a labelling of a graph is essentially a function assigning a real vector to every node and it could be also interpreted as a node classifier (as it often is in the literature). We will start by giving a very general definition of a GNN and afterwards, we will discuss how particular architectures are captured by this definition.

A GNN is defined with respect to a labelling $\labl$ such that there is a labelled graph $(G,\labl)$, where $G = (V,E)$ and $\labl : V \to \Rb^{k}$. A GNN computes a function $g : L_s \to L_k$, where $L_j$ is the set of all labellings $f : V \to \Rb^j$.
We will identify every labelling $f \in L_j$ with a matrix $\bF \in \Rb^{n \times j}$ by putting the label of every node in a separate row.
A GNN is obtained as composition of layers $l^1, \ldots, l^m$, where every layer computes $l^t(f^{(t-1)}) = f^{(t)} \in L_{k_t}$ and $f^{(0)} \equiv \labl$. For every $f^{(t)}$ let $\bF^{(t)} \in \Rb^{n \times k_{t}}$ be its corresponding matrix. We consider architectures, where each layer defines $\bF^{(t)} = l^t(\bF^{(t-1)})$ as follows
\begin{equation}\label{eq:GNNs}
\bF^{(t)} = \sigma \left( \bF^{(t)}\bW_1^{(t)} + \bN\bF^{(t-1)}\bW_2^{(t)} + \bB^{(t)} \right),
\end{equation}
where $\bN \in \Rb^{n \times n}$ is called the \emph{neighbourhood}, $\bW_1^{(t)}, \bW_2^{(t)} \in \Rb^{k_{t-1} \times k_t}$ are called \emph{weights}, and $\bB^{(t)} \in \Rb^{n \times k_t}$ is called the \emph{bias}. The bias is restricted to matrices such that all rows of the matrix are the same. Compared to~\eqref{eq:NN} there are four important differences.
\begin{enumerate}
\item The initial $f^{(0)}$ is equivalent to the labelling $\labl$ instead of being identity. This is because we consider labelled graphs. Unlabelled graphs are represented with $\labl$ that is identical on all vertices.
 \item There is a new neighbourhood component. Intuitively, this allows us to gather the information about the neighbourhood of each node. Typically $\bN$ will be the adjacency matrix $\bA$.
 \item There are two weight matrices and they appear on the right side of the previous labelling $\bF^{(t)}$. Here it is important weight matrices appear on the right because we want to restrict combining information about different rows to the neighbourhood component. The reason that there are two matrices is to have separately the information about the neighbourhood and about the previous labelling.
 \item The bias is restricted. Like before, the reason is to forbid distinguishing rows with measures other than the neighbourhood information.
\end{enumerate}
The final labelling defined by a GNN $g$ on the graph $(G,\labl)$ is $g(\labl)$. Notice that $g(\labl')$ is well-defined for the same graph with a different labelling $(G,\labl')$. For convenience we call labellings $\labl$ \emph{compatible with $g$} if $g(\labl)$ is well-defined.

In this paper we will consider GNN architectures that are restrictions of~\eqref{eq:GNNs}. For example we call \emph{standard architecture} the architecture, where $\bN = \bA$ and the bias $\mathbf{B}^{(t)}$ is always of the form $q \bJ$, where $q \in \Rb$ and $\bJ$ is a matrix with all entries equal to~$1$. This is denoted by
\begin{equation}
\mathbf{F}^{(t)}:=\sigma\Bigl(\mathbf{F}^{(t-1)}\mathbf{W}_1^{(t)}+ \mathbf{A}\mathbf{F}^{(t-1)}\mathbf{W}_2^{(t)} + q^{(t)} \bJ \Bigr), \label{eq:groheGNN}
\end{equation}
where we write $q^{(t)}$ to emphasise that it can be different for each layer.

We will identify architectures with the set of functions $g$ that are definable by GNNs that the architecture allows. 
We will denote architectures using the font $\architecture$ and we will write $g \in \architecture$ meaning that $g$ is computable by a GNN in the architecture $\architecture$.

% For every GNN $f$ with $m$ layers we define the \emph{layer sequence} of labellings obtained between each layers $f^{(0)}, \ldots f^{(m)}$. We will analyse the properties of architectures by analysing the layer sequences. We say that a GNN $f$ is \emph{monotonic} if $f^{(m)} \sqsubseteq f^{(m-1)} \sqsubseteq \ldots \sqsubseteq f^{(0)}$. We say that an architecture $\architecture$ is monotonic if for every $f\in \architecture$ and its layer sequence $f^{(0)}, \ldots f^{(m)}$ there exists $g \in \architecture$ with the layer sequence $g^{(0)}, \ldots g^{(m)}$ such that $g$ is monotonic and $g^{(i)} \sqsubseteq f^{(i)}$ for all $i = 0,\ldots, m$.

\begin{definition}\label{def:comparing}
Consider two architectures $\architecture$ and $\architecture'$. We say that $\architecture$ is weaker than $\architecture'$ (or equivalently that $\architecture'$ is stronger than $\architecture$) if: for every $g \in \architecture$ with $m$ layers and every labelling $\labl$ compatible with $g$ there exists $g' \in \architecture'$ with $m'$ layers such that $g'(\labl) \sqsubseteq g(\labl)$.
This is denoted by $\architecture' \sqsubseteq \architecture$. We write $\architecture' \not \sqsubseteq \architecture$ if this property does not hold.
If we require that there exists a uniform constant $c$ such that $m' \le m + c$ then we write that $\architecture' \sqsubseteq \architecture$ up to a constant factor $c$. In the particular case when this holds for $c = 0$ we write that $\architecture' \sqsubseteq \architecture$ with no factor. Similarly, if there exist uniform constants $c, c'$ such that $m' \le cm + c'$ we write that $\architecture' \sqsubseteq \architecture$ up to a linear factor $c$.
\end{definition}

\begin{example}
Consider two architectures:
\begin{enumerate}
 \item $\architecture_1$, where we fix $\bN = \bA$;
 \item $\architecture_2$, where we fix $\bN = \bA^2$.
\end{enumerate}
Then $\architecture_1 \sqsubseteq \architecture_2$ up to a linear factor~$2$ and $\architecture_2 \not \sqsubseteq \architecture_1$.
\end{example}
\todo{F: I don't know a simple proof of the first statement (I think one can prove it through WL). The second statement requires some simple example.}

Notice that the WL algorithm can be viewed as an architecture. Indeed, for every labelled graph the WL algorithm outputs a labelling. We will denote this by $\architectureWL$. In view of this definition we say that an architecture $\architecture$ is \emph{bounded by WL} if $\architectureWL \sqsubseteq \architecture$. We also say that an architecture $\architecture$ is \emph{WL-strong} if $\architecture \sqsubseteq \architectureWL$. The definitions of bounded by WL and WL-strong up to linear and constant factors carry on in the obvious way.

% More specifically, a GNN computes for each vertex $v\in V$ a feature vector $\mathbf{F}^{(t)}_{v\bullet}$  in layer $t$ by combining:
% \begin{itemize}
% 	\item[] (i)~the feature vector $\mathbf{F}^{(t-1)}_{v\bullet}$ of $v$ from the previous layer; and
% 	\item[] (ii)~the multi-set of features from the previous layer  of $v$'s neighbors, i.e., the multi-set $\ldbl \mathbf{F}^{(t-1)}_{u\bullet} \st u \in N_G(v) \rdbl$.
% \end{itemize}
% One typically parametrises GNNs by how this combination is achieved.

\subparagraph*{Activation functions.}
Before we present how architectures compare with the WL algorithm we discuss the activation functions $\sigma$ from~\eqref{eq:GNNs}. We will assume that an architecture has a fixed activation function. Moreover, when we compare two architectures we assume that they use the same activation function. If the activation function is not specified it means that it can be any function. This is particularly important when comparing architectures with WL. When we verify whether $\architectureWL \sqsubseteq \architecture$ we do not specify the activation function $\sigma$ and the intended meaning is that the property of being bounded by WL should work for any $\sigma$. On the contrary, when we verify $\architecture \sqsubseteq \architectureWL$ we write for which activation function the architecture is WL-strong. In this paper we will consider two example activation functions:
\begin{itemize}
 \item ReLU defined pointwise as $x \to \max(x, 0)$ for all $x \in \Rb$;
 \item sign, defined as $sign(x) = 1$ if $x > 0$, $sign(x) = -1$ if $x < 0$ and $sign(0) = 0$ for all $x \in \Rb$.
\end{itemize}

We will discuss a more abstract definition of a GNN than~\eqref{eq:GNNs}.
For each layer $t$ we consider two functions:
combination function $f_{\textsl{comb}}^{(t)}$; and aggregation function
$f_{\textsl{agg}}^{(t)}$. The layers are defined as follows. For each vertex $v\in V$ and layer $t$:
\begin{equation}
\mathbf{F}^{(t)}_{v\bullet}:=
f_{\textsl{comb}}^{(t)}\Bigl(
\mathbf{F}_{v\bullet}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\ldbl \mathbf{F}^{(t-1)}_{u\bullet} \st u \in N_G(v) \rdbl\bigr)
\Bigr), \label{lab:generalGNN}
\end{equation}
where $\mathbf{F}^{(t)} = l^{t}(\mathbf{F}^{(t-1)})$. Notice that if in~\eqref{eq:GNNs} we restrict to $\bN = \bA$ then~\eqref{lab:generalGNN} captures that definition.

% Finally, if a GNN consists of $L$ layers then
% $\mathbf{F}^{(L)}$ is used to classify the vertices in $G$ different classes. More precisely, vertices $v$ and $w$ will belong to the same class if and only if 
% $\mathbf{F}^{(L)}_{v\bullet}=\mathbf{F}^{(L)}_{w\bullet}$, i.e., when their feature vectors are equal.

% The way that GNNs update feature vectors is clearly reminiscent of how the WL procedure works. The relationship between vertex classification by GNNs of the form~(\ref{lab:generalGNN}) and vertex classification by WL can be made precise~\cite{grohewl,DBLP:conf/iclr/XuHLJ19}. To state this relation formally, we define the notions of being \textit{bounded by WL}, for upper bounding the classification power of GNNs, and being \textit{WL-strong}, for lower bounding the classification power of GNNs~\cite{grohewl}. 

% \begin{definition}[Bounded by WL]\normalfont\label{def:wlupper}
% Let $(G,\pmb{\ell})$ be a labeled graph and let $\pmb{\ell}^{(0)}:=\pmb{\ell}$. Furthermore, denote by 
% $\pmb{\ell}^{(t)}$ the vertex labeling obtained after $t$ iterations of the WL procedure, starting from $\pmb{\ell}^{(0)}$. Let $\mathbf{F}^{(t)}$ be feature matrices computed using a GNN of the form ~(\ref{lab:generalGNN}), for $t\geq 0$. Then, given $\mathbf{F}^{(0)}$ such that $\pmb{\ell}^{(0)}\sqsubseteq \mathbf{F}^{(0)}$ holds, one says that the GNN  is \textit{bounded by WL} if for all $t\geq 0$, 
% $\pmb{\ell}^{(t)}\sqsubseteq \mathbf{F}^{(t)}$.
% % That is, the vertex labeling induced by $\mathbf{F}^{(t)}$ is always coarser than the vertex labeling $\pmb{\ell}^{(t)}$  obtained by WL after $t$ iterations.
% \end{definition}
% In terms of vertex classification this implies that when a GNN is bounded by WL, then 
% for every $t$, when WL classifies two vertices as the same in step $t$, then so does the GNN. 

% The boundedness property holds for \textit{any} GNN of the form~(\ref{lab:generalGNN}):
\begin{proposition}[\cite{grohewl,DBLP:conf/iclr/XuHLJ19}]\label{prop:upperboundgeneral}
% Let $(G,\pmb{\ell})$ be a labeled graph and let $\mathbf{F}^{(0)}$ consist of feature vectors such that $\pmb{\ell}^{(0)}\sqsubseteq \mathbf{F}^{(0)}$. Then, $\pmb{\ell}^{(t)}\sqsubseteq \mathbf{F}^{(t)}$ for every $t>0$, where $\mathbf{F}^{(t)}$ is computed by a
Any architecture of the form~(\ref{lab:generalGNN}), in particular the standard architecture~\eqref{eq:groheGNN}, is bounded by WL with no factor.
\end{proposition}

% When the combination and aggregation functions are assumed to be all injective one can further show that GNNs of the form~(\ref{lab:generalGNN}) are also as powerful as WL~\cite{DBLP:conf/iclr/XuHLJ19}. That is, when two vertices are classified as the same by the GNN then they are also classified the same by WL. 
% We can phrase this more generally in terms of \textit{classes} of GNNs. Here, a class of GNNs is simply a collection of GNNs of the form~(\ref{lab:generalGNN}) in which the allowed aggregation and combination functions are restricted to belong to some class. For example, one could consider the class of injective functions for the combination and aggregation functions.
% 
% \begin{definition}[WL-strong]\normalfont\label{def:wlstrong}
% A \textit{class} of GNNs of the form~(\ref{lab:generalGNN}) is \textit{WL-strong} if for any given labeled graph
% 	$(G,\pmb{\ell})$, there exist instantiations of the combination and aggregation functions in that class such that for all $t\geq 0$, $\mathbf{F}^{(t)}\sqsubseteq \pmb{\ell}{}^{(t)}$.
% \end{definition}

% Clearly, for classes of GNNs of the form~(\ref{lab:generalGNN}) that are WL-strong, 
% Proposition~\ref{prop:upperboundgeneral} implies that for every labeled graph $(G,\pmb{\ell})$ there
% exist instantiations of the combination and aggregation functions in that class such that
% for all $t\geq 0$, $\mathbf{F}^{(t)}\equiv \pmb{\ell}{}^{(t)}$. That is, there is at least one particular instantiation of the GNNs in the class that is equivalent to WL in terms of vertex classification.

This result is complemented by the following.
\begin{proposition}[\cite{grohewl}]\label{prop:lowerboundgeneral}
The standard architecture defined by~\eqref{eq:groheGNN} is WL-strong with no factor.
\end{proposition}

% where $\mathbf{W}_1^{(t)}$, $\mathbf{W}_2^{(t)}$  are weight matrices, $\mathbf{B}^{(t)}$ are 
% constant\footnote{A constant matrix is matrix which is a multiple of the all-ones matrix $\mathbf{J}$.} bias matrices, and $\sigma$ is the sign function. It is readily verified that GNNs of the form~(\ref{eq:groheGNN}) can be seen as  GNNs of the form~(\ref{lab:generalGNN}). Hence, GNNs of the form~(\ref{eq:groheGNN}) are also bounded by WL by Proposition~\ref{prop:upperboundgeneral}.

More specifically, it was shown in~\cite{grohewl} that the standard architecture is WL-strong wit no factor even if we assume that the bias is uniformly $- \mathbf{J}$ for all layers.

To conclude the section we discuss architectures from the literature that are restrictions of~\eqref{eq:GNNs}. We divide them into three groups.

\begin{enumerate}
 \item \emph{Neighbourhood only.} When $\bW_1^{(t)}$ is always the $0$ matrix. The examples we consider are
 \begin{description}
% \item[\textit{Adjacency}:]
% % $\mathbf{L}=\mathbf{R}:=\mathbf{I}$, $p=q:=0$. Hence,
% $
% \mathbf{F}^{(t+1)}:=\sigma\left(\mathbf{A}\mathbf{F}^{(t)}\mathbf{W}^{(t)}\right)
% $
\item[\textit{Random walk (RW-GNN)}:] 
% $\mathbf{L}:=\mathbf{D}^{-1}$ with $\mathbf{D}$ the degree matrix of $\mathbf{A}$, $\mathbf{R}:=\mathbf{I}$, $p=q:=0$. Hence,
$$
\mathbf{F}^{(t)}:=\sigma\left(\mathbf{D}^{-1}\mathbf{A}\mathbf{F}^{(t-1)}\mathbf{W}^{(t-1)}\right);$$
\item[\textit{Augmented random walk (RW-GNN+)}:] 
% $\mathbf{L}:=\tilde{\mathbf{D}}^{-1}$ with $\tilde{\mathbf{D}}$ the degree matrix of $\mathbf{A}+\mathbf{I}$, $\mathbf{R}:=\mathbf{I}$, $p:=1$, $q:=0$. Hence,
$$
\mathbf{F}^{(t)}:=\sigma\left(\tilde{\mathbf{D}}^{-1}(\mathbf{A}+\mathbf{I})\mathbf{F}^{(t-1)}\mathbf{W}^{(t-1)}\right),$$
with $\tilde{\mathbf{D}}$ the degree matrix of $\mathbf{A}+\mathbf{I}$;
\item[\textit{Normalized adjacency (NA-GNN)}:] 
% $\mathbf{L}=\mathbf{R}:=\mathbf{D}^{-1/2}$, $p=q:=0$. Hence,
$$
\mathbf{F}^{(t)}:=\sigma\left(\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\mathbf{F}^{(t-1)}\mathbf{W}^{(t-1)}\right)
;$$
\end{description}
\item \emph{Degree normalised.} Without restrictions on $\bW_1^{(t)}$ but fixing $\bN = \mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$. For example
\begin{description}
\item[\textit{$1$st Order GCN (1-GCN)}:]
$$
\mathbf{F}^{(t)}:=\sigma\left(\mathbf{F}^{(t-1)}\mathbf{W}_1^{(t-1)}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\mathbf{F}^{(t-1)}\mathbf{W}_2^{(t-1)}\right);$$

\item[\textit{Simplified $1$st Order GCN (1-GCNs)}:]
$$
\mathbf{F}^{(t)}:=\sigma\left((\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}+\mathbf{I})\mathbf{F}^{(t-1)}\mathbf{W}_2^{(t-1)}\right);$$
\end{description}

\item \emph{Normalised and optimised.} These are the remaining architectures.
\begin{description}
\item[\textit{Augmented adjacency (NA-GNN+)}~\cite{kipf-loose}:] % $\mathbf{L}=\mathbf{R}:=\tilde{\mathbf{D}}^{-1/2}$, $p:=1$, $q:=0$. Hence,
$$
\mathbf{F}^{(t)}:=\sigma\left(\tilde{\mathbf{D}}^{-1/2}(\mathbf{A}+\mathbf{I})\tilde{\mathbf{D}}^{-1/2}\mathbf{F}^{(t-1)}\mathbf{W}^{(t-1)}\right);
$$
\item[\textit{Weighted augmented adjacency (NA-GNN++)}~\cite{DBLP:journals/corr/abs-1905-03046}:]
$$\mathbf{F}^{(t)}:=\sigma\left((r\mathbf{I}+(1-r)\tilde{\mathbf{D}})^{-1/2}(\mathbf{A}+p\mathbf{I})(r\mathbf{I}+(1-r)\mathbf{D})^{-1/2}\mathbf{F}^{(t-1)}\mathbf{W}^{(t-1)}\right),
$$
which underlies the PiNet architecture.
\end{description}
\end{enumerate}

