\section{The distinguishing power of anonymous and degree-aware MPNNs}
We have seen two restricted classes of MPNNs in the previous section: Anonymous MPNNs and degree-aware MPNNs. Together, these classes suffice to capture many common graph neural network architectures. In this section we study their distinguishing power. The take-away message from this section is that degree-aware MPNNs (such as those originating from the GCN architecture by~\cite{kipf-loose}, see Example~\ref{ex:KipfasMPNN}) have a slightly stronger distinguishing power than anonymous MPNNs. This may explain the experimental success of graph neural networks such as the GCNs by~\cite{kipf-loose}. Despite being based on a simple observation, the difference between anonymous and degree-aware MPNNs has, to our knowledge, not been reported in the literature before. In fact, one often finds statements indicating that existing results for anonymous MPNNs \cite{xhlj19,grohewl} carry over verbatim for, say the GCN architecture
by~\cite{kipf-loose}. As we will see, this is not precisely true.
\looseness=-1

This section is organised as follows. We first define how two classes of MPNNs can be compared relative to  their distinguishing power (Section~\ref{subsec:compare}). We then recall known results for anonymous MPNNs (Section~\ref{subsec:aMPNNs}). We conclude by establishing the distinguishing power of degree-aware MPNNs (Section~\ref{subsec:dMPNNs}).

\subsection{Comparing classes of MPNNs}\label{subsec:compare}
For a given labeled graph $\langle G,\pmb{\nu}\rangle$ and MPNN $M$, we denote by 
$\pmb{\ell}_M^{(t)}$ the vertex labeling computed by $M$ after $t$ rounds. We will fix the input graph in what follows, so we do not need to include the dependence on the graph in the notation of labelings.
The \textit{distinguishing power} of an MPNN $M$ relates to its ability to distinguish vertices by means of the labelings $\pmb{\ell}_M^{(t)}$, for $t\geq 0$. 

\begin{definition}\label{def:mpnnweak}\normalfont
Consider two MPNNs $M_1$ and $M_2$ and let $\pmb{\ell}_{M_1}^{(t)}$ and $\pmb{\ell}_{M_2}^{(t)}$  be their corresponding labelings on an input graph $\langle G,\pmb{\nu}\rangle$ after $t$ rounds of computation. Then,
$M_1$ is said to be \textit{weaker} than $M_2$, denoted by $M_1\preceq M_2$, if $M_1$ cannot distinguish more vertices of $G$ than $M_2$ in every round of computation. We also say that $M_2$ is \textit{stronger} than $M_1$. More formally, $M_1\preceq M_2$ if $\pmb{\ell}_{M_2}^{(t)}\sqsubseteq
\pmb{\ell}_{M_1}^{(t)}$ for every $t\geq 0$.\qed
\end{definition}
We can lift this notion to classes  $\architecture_1$ and $\architecture_2$ of MPNNs in a natural way. 

\begin{definition}\label{def:classesweak}\normalfont
Consider two classes $\architecture_1$ and $\architecture_2$ of MPNNs.
Then, $\architecture_1$ is said to be \textit{weaker} than $\architecture_2$, denoted by 
$\architecture_1\sqsubseteq \architecture_2$, if for every $M_1\in \architecture_1$
there exists an $M_2\in\architecture_2$ which is stronger than $M_1$. \qed
\end{definition}

We will also need a generalisation of the previous definitions in which the MPNNs involved may run a different number of rounds. 

\begin{definition}\normalfont
Consider two MPNNs $M_1$ and $M_2$ and let $\pmb{\ell}_{M_1}^{(t)}$ and $\pmb{\ell}_{M_2}^{(t)}$  be their corresponding labelings on an input graph $\langle G,\pmb{\nu}\rangle$ after $t$ rounds of computation. Let $g:\mathbb{N}\to \mathbb{N}$ be a function. We say that $M_1$ is \textit{$g$-weaker} than $M_2$, denoted by $M_1\preceq_{g} M_2$ if 
$\pmb{\ell}_{M_2}^{g(t)}\sqsubseteq
\pmb{\ell}_{M_1}^{(t)}$ for every $t\geq 0$.\qed
\end{definition}
In other words, the distinguishing power of $M_1$ is bounded to that of $M_2$, provided that $M_2$ can run for possibly different number of rounds. The number of rounds that $M_2$ can run, however, is controlled by the function $g$.

The following special cases of this definition will be relevant to this paper:
\begin{itemize}
    \item $g(t)=t$, for all $t\geq 0$. This case corresponds to Definition~\ref{def:mpnnweak}. If $M_1\preceq_{g} M_2$ then we simply say that $M_1$ is weaker than $M_2$, as before.
    \item $g(t)\leq t+c$, for all $t\geq 0$ and some constant $c$. If $M_1\preceq_{g} M_2$ then we say that $M_1$ is weaker than $M_2$ \textit{but may be $c$ steps ahead};
    \item $g(t)\leq c't+c$, for all $t\geq 0$ and some constants $c'$ and $c$. If $M_1\preceq_{g} M_2$ then we say that $M_1$ is weaker than $M_2$ \textit{possibly up to a linear factor of $c'$ and $c$ steps ahead}.
\end{itemize}
These notions can be generalised to classes of MPNNs, just as we did in Definition~\ref{def:classesweak}. 
\begin{definition}\normalfont
Consider two classes $\architecture_1$ and $\architecture_2$ of MPNNs.
Then, $\architecture_1$ is said to be weaker than $\architecture_2$, but may be $c$ steps ahead, if for every $M_1\in \architecture_1$
there exists an $M_2\in\architecture_2$ such that $M_1$ is weaker than $M_2$, but may be $c$ steps ahead. Similarly,
 $\architecture_1$ is said to be weaker than $\architecture_2$, possibly up to a linear factor $c'$ and $c$ steps ahead, if for every $M_1\in \architecture_1$
there exists an $M_2\in\architecture_2$ such that $M_1$ is weaker than $M_2$, possibly up to a linear factor $c'$ and  $c$ steps ahead. 
\qed
\end{definition}

The following observation follows immediately from the definitions. Consider two classes $\architecture_1$ and $\architecture_2$ of MPNNs. Let $c$ and $c'$ be arbitrary
constants.
If $\architecture_1$ is weaker than $\architecture_2$, then $\architecture_1$ is weaker than $\architecture_2$, but may be $c$ steps ahead. If $\architecture_1$ is weaker than $\architecture_2$, but may be $c$ steps ahead, then $\architecture_1$ is weaker than $\architecture_2$, possibly up to a linear factor $c'$  and $c$ steps ahead. We will see that the reverse implication do not necessarily hold later in the paper.

To conclude this section, we 

% The following definition states when one class of MPNNs is weaker (in terms of distinguishing power) than another class of MPNNs. Intuitively, one class $\architecture$ will be weaker than other class $\architecture'$ if any MPNN $M\in\architecture$ cannot distinguish more vertices than some MPNN $M'\in\architecture'$.

% \begin{definition}\label{def:comparing}\normalfont
% Consider two classes $\architecture$ and $\architecture'$ of MPNNs. We say that $\architecture$ is weaker than $\architecture'$ (or equivalently that $\architecture'$ is stronger than $\architecture$) if there exists a function $g:\mathbb{N}\to \mathbb{N}$ such that for every $M \in \architecture$ there exists an $M'\in \architecture'$ satisfying $\pmb{\ell}_{M'}^{(g(t))}\sqsubseteq \pmb{\ell}_{M}^{(t)}$, for all $t\geq 0$ and for any labeled graph $\langle G,\pmb{\nu},\pmb{\eta})$. This is denoted by $\architecture \sqsubseteq \architecture'$.
% % with $d$ layers 
% % and every labelling $\labl$ compatible with $g$ there exists $g' \in \architecture'$ with $m'$ layers such that $g'(\labl) \sqsubseteq g(\labl)$.
% % This is denoted by $\architecture' \sqsubseteq \architecture$.
% We write $\architecture \not \sqsubseteq \architecture'$ if the above property does not hold. 
% If we additionally require that the function $g:\mathbb{N}\to\mathbb{N}$ satisfies 
% $g(t)\le t +c$ for some constant $c$, then we write that $\architecture \sqsubseteq \architecture'$ up to a constant factor $c$. In the particular case when this holds for $c = 0$ we write that $\architecture \sqsubseteq \architecture'$ with no factor. Similarly, if there exist constants $c, c'$ such that $g(t) \le ct + c'$ we write that $\architecture \sqsubseteq \architecture'$ up to a linear factor $c$.
% \end{definition}




% Of particular interest will be the class of MPNNs corresponding to the WL algorithm. 
% \floris{Ok, add aMPNN description of WL. Note that I added the description of the WL algorithm on undirected graphs but with each labels to the prelims. }
% We will denote this class by $\architectureWL$ and it is defined as MPNNs in which the message 
% We say that an architecture $\architecture$ is \emph{bounded by WL} if $\architecture\sqsubseteq \architectureWL$. We also say that an architecture $\architecture$ is \emph{WL-strong} if $\architectureWL \sqsubseteq \architecture$. The definitions of bounded by WL and WL-strong up to linear and constant factors carry on in the obvious way.

\subsection{Anonymous MPNNs}\label{subsec:aMPNNs}
Following the work by~\cite{xhlj19} and~\cite{grohewl}, which relates to anonymous MPNNs, we study the distinguishing power of degree-aware MPNNs. We will see that degree-aware MPNNs (such as those originating from the GCN architecture by~\cite{kipf-loose}, see Example~\ref{ex:KipfasMPNN}) have a slightly stronger distinguishing power than anonymous MPNNs.
More precisely, any labeling computed by $d$ rounds of a degree-aware MPNNs can be computed by $d+1$ rounds of an anonymous MPNN. This may explain the experimental success of graph neural networks such as the GCNs by~\cite{kipf-loose}. We note that in the literature one often does not distinguish between anonymous and degree-aware MPNNs
and 
Before showing this, we first define how two classes of MPNNs can be compared relative to

\subsection{Degree-aware MPNNs}\label{subsec:dMPNNs}


It has been recently shown by~\cite{Loukas2019} that MPNNs are Turing complete, i.e., they can compute any (computable) graph function. The proof uses an equivalence between MPNNs and a well-known distributed computation model, LOCAL, which is known to be Turing complete~\cite{Angluin}. We refer to~\cite{Loukas2019} for more details. The completeness result crucially relies on the dependence of the message functions on $v$ and $w$, i.e., on knowing which vertices are being considered. 
% Indeed, after $\delta=\text{diam}(G)$ rounds each vertex has complete information about the graph $\langle G,\pmb{\nu},\pmb{\eta})$
% and once can use $\textsc{Upd}^{(\delta+1)}$ to compute the desired graph function.

\paragraph{Anonymous MPNNs.} In contrast, when the message functions 
$\textsc{Msg}^{(t)}$ in MPNNs only depend on $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_w^{(t-1)}$ and $\pmb{\eta}_{\{v,w\})}$, then the distinguishing power is limited. MPNNs with this restrictions are referred to as \textit{anonymous} MPNNs (or aMPNNs) in~\cite{Loukas2019}. We given an example of aMPNNs next.


% %
% %
% % As just mentioned, there is an ambiguity in the \citet{GilmerSRVD17}
% %
% %
% % It appears that this ambiguity raises some confusion and leads to imprecise statements about the distinguishing power of MPNNs. Indeed, the distinguishing power of MPNNs is often claimed to be bounded by the Weisfeiler-Lehman vertex colouring algorithm. This is verified, however, only when it concerns so-called \textit{anonymous} MPNNs.
% %
% % \smallskip
% % \noindent
% % {\bf Remarks.}
% %
% % First of all, directed graphs are considered.
% % Clearly, undirected graphs, as used in \citet{GilmerSRVD17}, can be regarded as directed graphs.
% % Second, the dependency of the message functions $\textsc{Upd}^{(t)}$ on the vertices $v$ and $w$
% % is made explicit. In~\citet{GilmerSRVD17} this dependency is not specified but the vertices
% %
% %
% % \leftpointright This formalisation of MPNNs given in \citep{Loukas2019} differs from that of in the following: (i)~$E^*$ and $N_G*(v)$ instead of $E$ and $N_G(v)$ are used; (ii) aggregation happens during the update phase and not the messaging phase; and (iii) the update function does not take $\mathbf{m}_v^{(t)}$ as a separate input. I guess that the two models are equivalent, but we may want to check this.
% %
% % \smallskip
% % \noindent
% % \leftpointright The formalisation of MPNNs by \citet{GilmerSRVD17} is a bit underspecified. The message and update functions do not explicitly depend on the vertices themselves, but in the examples given in \citep{GilmerSRVD17}, these functions seem to be allowed to use extra information related to the vertices and input graph. For example, to model the GCN model by~\citep{KipfW16}, the message functions need the degrees of the vertices.
% %%
% % \paragraph{Anonymous MPNNs.}
% % %
% %  The Turing completeness comes a bit as surprise. Indeed, many papers declare MPNNs as being bounded, in terms of their distinguishing power of vertices and graphs, by the Weisfeiler-Lehman algorithm. This is always backed up by referring to the papers~\citet{XuHLJ19,grohewl}. This connection, however, only holds when MPNNs do not have access to the identifiers of vertices $v$ and $w$.  Such MPNNs are referredin~\citet{Loukas2019}  as \textit{anonymous MPNNs.} Anonymous MPNNs (aMPNNs) are thus MPNNs such that the functions 
% % An anonymous MPNN (aMPNN) is an MPNN in which the message functions
% % $\textsc{Msg}^{(t)}$ only depend on $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_w^{(t-1)}$~\citep{Loukas2019}.

% More precisely, we show that aMPNNs are bounded by WL in terms of their distinguishing power. In other words, for every $t\geq 0$, $\mathbf{w}\pmb{\ell}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$. This was already shown  by \cite{XuHLJ19} and~\cite{grohewl} for undirected graphs and in which the edge labeling $\pmb{\eta}$ is absent. The generalisation to labeled directed graphs $\langle G,\pmb{\nu},\pmb{\eta}\rangle$ simply requires the use of the WL-algorithm on such graphs~\citep{}.
% % the following proposition is
% % \citet{Jaume2019} recently showed the following:
% % \smallskip
% % \noindent
% % \leftpointright I am ignoring the edge labeling $\pmb{\eta}$ for now (more later).
% %
% \begin{proposition}\label{prop:WL}
% 	The distinguishing power of anonymous MPNNs is bounded by WL.
% \end{proposition}
% \begin{proof}
% We need to show that $\mathbf{w}\pmb{\ell}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$ for every $t$.
% By definition of $\mathbf{w}\pmb{\ell}^{(0)}=$, $\mathbf{w}\pmb{\ell}^{(0)}=\pmb{\ell}^{(0)}$.
% Assume by induction that $\mathbf{w}\pmb{\ell}^{(t-1)}\sqsubseteq \pmb{\ell}^{(t-1)}$. Consider
% two vertices $v,w\in V$ such that $\mathbf{w}\pmb{\ell}^{(t)}_v=\mathbf{w}\pmb{\ell}^{(t)}_w$.
% By definition of $\mathbf{w}\pmb{\ell}^{(t)}$, this implies that 
% $\mathbf{w}\pmb{\ell}^{(t-1)}_v=\mathbf{w}\pmb{\ell}^{(t-1)}_w$ and furthermore, for every
% edge label $\eta$:
% \begin{align*}
% 	\lmset \mathbf{w}\pmb{\ell}^{(t-1)}_u\mid u\in N_G^*(v), \pmb{\eta}_{(u,v)}=\eta\rmset&=	\lmset \mathbf{w}\pmb{\ell}^{(t-1)}_u\mid u\in N_G^*(w), \pmb{\eta}_{(u,v)}=\eta\rmset.
% 	% \lmset \pmb{\eta}_{(u,v)} \mid (u,v)\in E\rmset&=	\lmset \pmb{\eta}_{(u,w)} \mid (u,w)\in E\rmset\\
% 	% \lmset \pmb{\eta}_{(v,u)} \mid (v,u)\in E\rmset&=	\lmset \pmb{\eta}_{(w,u)} \mid (w,u)\in E\rmset.
% \end{align*}
% This implies that, by induction, $\pmb{\ell}^{(t-1)}_v=\pmb{\ell}^{(t-1)}_w$, and for every $u\in N_G^*(v)$ such that
% $\pmb{\eta}_{(u,v)}=\eta$  there exists a unique corresponding $u'\in N_G^*(w)$ such that $\pmb{\eta}_{u'w}=\eta$. In other words,
% $$
% \mathbf{m}^{(t)}_{v\gets u}=\textsc{Msg}^{(t)}(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_u,\eta)=
% \textsc{Msg}^{(t)}(\pmb{\ell}^{(t-1)}_w,\pmb{\ell}^{(t-1)}_u,\eta)=\mathbf{m}^{(t)}_{w\gets u'}.$$
% As a consequence,
% $$
% \sum_{\substack{u\in N_G^*(v)\\\pmb{\eta}_{(u,v)}=\eta}} \mathbf{m}^{(t)}_{v\gets u}=\sum_{\substack{u'\in N_G^*(w)\\\pmb{\eta}_{(u',w)}=\eta}} \mathbf{m}^{(t)}_{w\gets u'}
% $$
% for every $\eta$ and hence, $\sum_{u\in N_G^*(v)}\mathbf{m}^{(t)}_{v\gets u}=
% \sum_{u\in N_G^*(w)}\mathbf{m}^{(t)}_{w\gets u'}
% $. From this we can infer that
% $$
% \pmb{\ell}^{(t)}:=\textsc{Upd}^{(t)}\left(\sum_{u\in N_G^*(v)}\mathbf{m}^{(t)}_{v\gets u}\right)=\textsc{Upd}^{(t)}\left(\sum_{u'\in N_G^*(w)}\mathbf{m}^{(t)}_{w\gets u'}\right)=\pmb{\ell}^{(t)},
% $$
% as desired.
% % We remark that this proposition encompasses the result by~\citep{XuHLJ19} for learning formalisms
% % which compute a vertex labeling $\pmb{\ell}^{(t)}$ in round $t$, specified as follows: For every vertex $v\in V$,
% %
% \end{proof}

% We remark that a similar result was recently established by~\citet{Jaume2019}. In that work, however, a version of WL on labeled directed graph is used which decouples vertex labels and edge labels. We here consider a version of WL such that correspondences known for undirected graphs carry over. More specifically,
% fractional isomorphisms, $C^2$. Check!
% %
% %
% % \smallskip
% % \noindent
% % \leftpointright I believe it is possible to link aMPNNs to the general model we used earlier.
% % For each layer $t$ we consider two functions:
% % combination function $f_{\textsl{comb}}^{(t)}$; and aggregation function
% % $f_{\textsl{agg}}^{(t)}$. The layers are defined as follows. For each vertex $v\in V$ and layer $t$:

% We further remark that the works \citep{XuHLJ19,grohewl}  considered graph neural models of the form
% $$
% \pmb{\ell}^{(t)}_{v}:=
% f_{\textsl{comb}}^{(t)}\Bigl(
% \pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid w \in N_G(v) \rmset\bigr)
% \Bigr),
% $$
% where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{agg}}^{(t)}$ are general (computable) combination and aggregation functions. It is easily verified that such models are equivalent to aMPNNs. To see this, it suffices to observe that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid u \in N_G(v) \rmset\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{w\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{w})\bigr)$ (this was observed in Lemma 5 in~\cite{XuHLJ19}, based on Theorem 2 in~\cite{ZaheerKRPSS17}). Crucial in all this is that feature values belong to a countable domain and that the size of multisets is bounded. These conditions are satisfied because our domain is $\mathbb{Q}$ and there are at most $|V|$ elements in a multiset. For completeness, we provide additional details in Section~\ref{subsec:aggr} in the appendix.
% % % $f_{\textsl{agg}}^{(t)}$.
% % In~\citet{Loukas2019} there is an argument, based on Lemma 5 in~\citet{XuHLJ19}, which allows to convert any aggregation function to a summation followed by a function. It may need a bit more thought how to squeeze this into aMPNNs.

% % \smallskip
% % \noindent
% % \leftpointright To bring edge information into the picture, we may look into~\citep{Jaume2019}, where WL for edge labeled graphs is considered.
