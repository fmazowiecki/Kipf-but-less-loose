\documentclass[10pt,a4paper]{article}

\usepackage{dingbat}
\usepackage[margin=1in]{geometry}
\usepackage{palatino}
\usepackage[mathlines]{lineno}
\usepackage{mathpazo}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[bookmarksdepth=3]{hyperref}
\hypersetup{%
    colorlinks=true,
    linkcolor=darkgray,
    citecolor=darkgray,
   urlcolor  = gray}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\mathtoolsset{centercolon}

\usepackage[affil-it]{authblk}
\usepackage[titlenumbered, linesnumbered, ruled]{algorithm2e}
\usepackage{bbold,adjustbox,multicol,multirow}
\usepackage{natbib}
\bibliographystyle{plainnat}

\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{prop}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\newcommand{\ML}[1]{\mathsf{ML}(#1)}
\newcommand{\MLANG}{\mathsf{MATLANG}}
\newcommand{\detsf}{\mathsf{det}}
\newcommand{\ones}{\mathbb{1}}
\newcommand{\zerovec}{\mathbb{0}}
\newcommand{\inv}{\mathsf{inv}}
\newcommand{\dime}{\mathsf{dim}_\nu}
\newcommand{\tr}{\mathsf{tr}}
\newcommand{\Id}{\mathsf{Id}}
\newcommand{\adj}[1]{\mathsf{adj}\left(#1\right)}
\newcommand{\deter}[1]{\mathsf{det}\left(#1\right)}
\newcommand{\tp}[1]{#1^{\mathsf{t}}}
\newcommand{\transp}[1]{#1^*}
\newcommand{\rowdom}{\mathbb{1}}
\DeclareMathOperator{\diag}{\mathsf{diag}}
\DeclareMathOperator{\Apply}{\mathsf{apply}}
\DeclareMathOperator{\Inv}{\inv}
\newcommand{\one}{\rowdom}
\newcommand{\eigen}{\mathsf{eigen}}
\newcommand{\degr}{\mathsf{deg}}
\newcommand{\lbag}{\{\!\!\{}
\newcommand{\rbag}{\}\!\!\}}
\newcommand*{\lmset}{\{\mskip-5mu\{} 
\newcommand*{\rmset}{\}\mskip-5mu\}} 
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\CLK}[1]{\mathsf{C}^{#1}}
\newcommand{\FOK}[1]{\mathsf{FO}^{#1}}
\newcommand{\CL}{\mathsf{C}}
\newcommand{\FO}{\mathsf{FO}}
\newcommand{\ICL}{\mathsf{C}^3_{\infty\omega}}
\let\originalcdot\cdot
\renewcommand*{\cdot}{\mskip2mu {\originalcdot}\mskip 2mu\relax}
\newcommand{\mdot}{\cdot}
\let\transp\relax
\let\lbag\relax
\let\rbag\relax



\newcommand{\change}[1]{#1}

\bibliographystyle{plainurl}

\title{Message Passing Neural Networks}

\author{Floris Geerts}
% \affil{University of Antwerp, Antwerp, Belgium}

% \date{}

\newcommand{\Kb}{\mathbb{K}}
\begin{document}

\maketitle
\begin{abstract}
	A short note explaining how the GNNs that we currently consider
	can be all put under the umbrella of so-called Message Passing Neural
	Networks.
\end{abstract}


\section{Rough sketch}
\paragraph{Preliminaries.}
We use the following notation: $[n]:=\{1,2,\ldots,n\}$, $\mathbb{N}^+$ denotes the positive natural numbers, and $\mathbb{R}$ denotes the real numbers. We consider (directed) graphs $G=(V,E)$ with vertex set $V$ and edge set $E\subseteq V\times V$. 
We let $E^*:=E\cup \{(v,v)\mid v\in V\}$. The neighbour set of a vertex $v$ in $G$ is denoted by $N_G(v)$ and is defined as $N_G(v):=\{u\in V\mid (u,v)\in E\}$. We let $N_G^*(v):=N_G(v)\cup \{(v,v)\}$. Furthermore, the degree of a vertex $v$ in $G$ is denoted by 
$d_v$ and defined as $d_v:=|N_G(v)|$. We similarly define $d_v^*=d_v+1=|N_G^*(v)|$.

A \textit{labeled} graph is a triple  $\langle G,\pmb{\nu},\pmb{\eta}\rangle$ consisting of a graph $G$ and two labeling functions 
$\pmb{\nu}:V\to\mathbb{R}^{f}$ and
$\pmb{\eta}:E\to \mathbb{R}^{g}$, for some $f,g\in\mathbb{N}^+$.  We denote the label
$\pmb{\nu}(v)$ of vertex $v$ by $\pmb{\ell}_v$, and similarly, $\pmb{\eta}_{(v,w)}$ denotes
$\pmb{\eta}((v,w))$. For convenience we identify $V$ with $[n]$ so that each $i\in [n]$ uniquely identifies a vertex in $V$. We assume, without of loss of generality, that when $\pmb{\nu}_v=\pmb{\nu}_w$ then also $\pmb{\eta}_{(v,v)}=\pmb{\eta}_{(w,w)}$. 


\section{Message Passing Neural Networks}
We start by describing message passing neural networks (MPNNs), introduced by \citet{GilmerSRVD17} for deep learning on graphs. Roughly speaking, in MPNNs, vertex features are propagated through the graph according to its connectivity structure. We closely follow the exposition of MPNNs as given in~\citep{Loukas2019}. 

% We next define \textit{message passing neural networks} (MPNNs). We here follow the exposition given in~\citet{Loukas2019}. MPNNs were originally proposed by \citet{GilmerSRVD17}. Intuitively,
An MPNN computes a vertex labeling $\pmb{\ell}:V\to \mathbb{Q}^{s}$ for some $s\in\mathbb{N}^+$, by means of a number of rounds of computation, starting from the input labeled graph $\langle G,\pmb{\nu},\pmb{\eta}\rangle$.
Each such round of computation consists of a message passing phase along the edges in $G$, followed by an aggregation phase in which each vertex aggregates its received messages. The vertex labeling computed after round $t$ is denoted by $\pmb{\ell}^{(t)}$. We next detail how $\pmb{\ell}^{(t)}$ is computed.

Initially, $\pmb{\ell}^{(0)}:=\pmb{v}$. Then, for round $t=1,2,\ldots,d$, we define $\pmb{\ell}^{(t)}:V\to\mathbb{Q}^{s}$, as follows\footnote{We can assume, without loss of generality, that every round assigns labels in $\mathbb{Q}^s$ for the same $s$. If not, one can include ``padding with zeroes'' in the message and update functions.}:
\begin{description}\setlength{\itemsep}{-0.4ex}
\item  [Message passing.] For each edge $(w,v)\in E^*$, $v$ receives a message $\mathbf{m}_{v\gets w}^{(t)}$ from $w$:
$$
\mathbf{m}^{(t)}_{v\gets w}:=\textsc{Msg}^{(t)}\left(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_w,v,w,\pmb{\eta}_{(w,v)}\right)\in\mathbb{Q}^{s}.
$$
\item [Aggregation.] For each vertex $v\in V$, all of its received messages are aggregated:
$$
\pmb{\ell}^{(t)}_v:=\textsc{Upd}^{(t)}\left(\sum_{w\in N_G^*(v)} \mathbf{m}_{v\gets w}^{(t)}\right)\in\mathbb{Q}^{s}.
$$
\end{description}
Here, $\textsc{Msg}^{(t)}$ and $\textsc{Upd}^{(t)}$ are general (computable) functions.
After round $d$, we define the final labeling $\pmb{\ell}:V\to\mathbb{Q}^{s}$ as  $\pmb{\ell}_v:=\pmb{\ell}^{(d)}_v$ for every $v\in V$. If further aggregation over the entire graph is needed, e.g., for graph classification, an additional readout function 
$\textsc{Read}(\lmset \pmb{\ell}_v\mid v\in V\rmset)$ can be applied. We omit the readout function here since most of the computation happens during the rounds of an MPNN.

We remark that formalisation of MPNNs given here differs slightly from the formalisation presented by \citet{GilmerSRVD17}. We show in the appendix (Section~\ref{subsec:relationship}) that the two formalisms are equivalent, provided that the message functions in \citet{GilmerSRVD17} are allowed to take vertices $v$ and $w$ as arguments. In fact, there is a certain ambiguity on what the message functions in the MPNNs of \citet{GilmerSRVD17} can depend. More precisely, in~\citep{GilmerSRVD17}, only $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_w^{(t-1)}$ and $\pmb{\eta}_{(w,v)}$ are specified as arguments. Nevertheless, the examples illustrating which graph neural network architectures can be captured by MPNNs use message functions that may depend on, e.g., the degree  of vertices. As we will see, this has an impact on the distinguishing power of MPNNs.

\section{The expressive power of MPNNs}
It has been recently shown by~\citet{Loukas2019} that MPNNs are Turing complete, i.e., they can compute any (computable) graph function. The proof uses an equivalence between MPNNs and a well-known distributed computation model, LOCAL, which is known to be Turing complete~\citep{Angluin} (See~\citep{Loukas2019} for more details). The completeness result crucially relies on the dependence of the message functions on $v$ and $w$, i.e., on knowing which vertices are being considered.

\paragraph{Anonymous MPNNs.} In contrast, when the message functions 
$\textsc{Msg}^{(t)}$ only depend on $\pmb{\ell}_v^{(t-1)}$, $\pmb{\ell}_w^{(t-1)}$ and $\pmb{\eta}_{(w,v)}$, then the distinguishing power is limited. MPNNs with this restrictions are referred to as \textit{anonymous} MPNNs in~\citep{Loukas2019}.
%
%
%
% As just mentioned, there is an ambiguity in the \citet{GilmerSRVD17}
%
%
% It appears that this ambiguity raises some confusion and leads to imprecise statements about the distinguishing power of MPNNs. Indeed, the distinguishing power of MPNNs is often claimed to be bounded by the Weisfeiler-Lehman vertex colouring algorithm. This is verified, however, only when it concerns so-called \textit{anonymous} MPNNs.
%
% \smallskip
% \noindent
% {\bf Remarks.}
%
% First of all, directed graphs are considered.
% Clearly, undirected graphs, as used in \citet{GilmerSRVD17}, can be regarded as directed graphs.
% Second, the dependency of the message functions $\textsc{Upd}^{(t)}$ on the vertices $v$ and $w$
% is made explicit. In~\citet{GilmerSRVD17} this dependency is not specified but the vertices
%
%
% \leftpointright This formalisation of MPNNs given in \citep{Loukas2019} differs from that of in the following: (i)~$E^*$ and $N_G*(v)$ instead of $E$ and $N_G(v)$ are used; (ii) aggregation happens during the update phase and not the messaging phase; and (iii) the update function does not take $\mathbf{m}_v^{(t)}$ as a separate input. I guess that the two models are equivalent, but we may want to check this.
%
% \smallskip
% \noindent
% \leftpointright The formalisation of MPNNs by \citet{GilmerSRVD17} is a bit underspecified. The message and update functions do not explicitly depend on the vertices themselves, but in the examples given in \citep{GilmerSRVD17}, these functions seem to be allowed to use extra information related to the vertices and input graph. For example, to model the GCN model by~\citep{KipfW16}, the message functions need the degrees of the vertices.
%%
% \paragraph{Anonymous MPNNs.}
% %
%  The Turing completeness comes a bit as surprise. Indeed, many papers declare MPNNs as being bounded, in terms of their distinguishing power of vertices and graphs, by the Weisfeiler-Lehman algorithm. This is always backed up by referring to the papers~\citet{XuHLJ19,grohewl}. This connection, however, only holds when MPNNs do not have access to the identifiers of vertices $v$ and $w$.  Such MPNNs are referredin~\citet{Loukas2019}  as \textit{anonymous MPNNs.} Anonymous MPNNs (aMPNNs) are thus MPNNs such that the functions 
% An anonymous MPNN (aMPNN) is an MPNN in which the message functions
% $\textsc{Msg}^{(t)}$ only depend on $\pmb{\ell}_v^{(t-1)}$ and $\pmb{\ell}_w^{(t-1)}$~\citep{Loukas2019}.

More precisely, we show that aMPNNs are bounded by WL in terms of their distinguishing power. In other words, for every $t\geq 0$, $\mathbf{w}\pmb{\ell}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$. This was already shown  by \citet{XuHLJ19} and~\citet{grohewl} for undirected graphs and in which the edge labeling $\pmb{\eta}$ is absent. The generalisation to labeled directed graphs $\langle G,\pmb{\nu},\pmb{\eta}\rangle$ simply requires the use of the WL-algorithm on such graphs~\citep{}.
% the following proposition is
% \citet{Jaume2019} recently showed the following:
% \smallskip
% \noindent
% \leftpointright I am ignoring the edge labeling $\pmb{\eta}$ for now (more later).
%
\begin{proposition}\label{prop:WL}
	The distinguishing power of anonymous MPNNs is bounded by WL.
\end{proposition}
\begin{proof}
We need to show that $\mathbf{w}\pmb{\ell}^{(t)}\sqsubseteq \pmb{\ell}^{(t)}$ for every $t$.
By definition of $\mathbf{w}\pmb{\ell}^{(0)}=$, $\mathbf{w}\pmb{\ell}^{(0)}=\pmb{\ell}^{(0)}$.
Assume by induction that $\mathbf{w}\pmb{\ell}^{(t-1)}\sqsubseteq \pmb{\ell}^{(t-1)}$. Consider
two vertices $v,w\in V$ such that $\mathbf{w}\pmb{\ell}^{(t)}_v=\mathbf{w}\pmb{\ell}^{(t)}_w$.
By definition of $\mathbf{w}\pmb{\ell}^{(t)}$, this implies that 
$\mathbf{w}\pmb{\ell}^{(t-1)}_v=\mathbf{w}\pmb{\ell}^{(t-1)}_w$ and furthermore, for every
edge label $\eta$:
\begin{align*}
	\lmset \mathbf{w}\pmb{\ell}^{(t-1)}_u\mid u\in N_G^*(v), \pmb{\eta}_{(u,v)}=\eta\rmset&=	\lmset \mathbf{w}\pmb{\ell}^{(t-1)}_u\mid u\in N_G^*(w), \pmb{\eta}_{(u,v)}=\eta\rmset.
	% \lmset \pmb{\eta}_{(u,v)} \mid (u,v)\in E\rmset&=	\lmset \pmb{\eta}_{(u,w)} \mid (u,w)\in E\rmset\\
	% \lmset \pmb{\eta}_{(v,u)} \mid (v,u)\in E\rmset&=	\lmset \pmb{\eta}_{(w,u)} \mid (w,u)\in E\rmset.
\end{align*}
This implies that, by induction, $\pmb{\ell}^{(t-1)}_v=\pmb{\ell}^{(t-1)}_w$, and for every $u\in N_G^*(v)$ such that
$\pmb{\eta}_{(u,v)}=\eta$  there exists a unique corresponding $u'\in N_G^*(w)$ such that $\pmb{\eta}_{u'w}=\eta$. In other words,
$$
\mathbf{m}^{(t)}_{v\gets u}=\textsc{Msg}^{(t)}(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_u,\eta)=
\textsc{Msg}^{(t)}(\pmb{\ell}^{(t-1)}_w,\pmb{\ell}^{(t-1)}_u,\eta)=\mathbf{m}^{(t)}_{w\gets u'}.$$
As a consequence,
$$
\sum_{\substack{u\in N_G^*(v)\\\pmb{\eta}_{(u,v)}=\eta}} \mathbf{m}^{(t)}_{v\gets u}=\sum_{\substack{u'\in N_G^*(w)\\\pmb{\eta}_{(u',w)}=\eta}} \mathbf{m}^{(t)}_{w\gets u'}
$$
for every $\eta$ and hence, $\sum_{u\in N_G^*(v)}\mathbf{m}^{(t)}_{v\gets u}=
\sum_{u\in N_G^*(w)}\mathbf{m}^{(t)}_{w\gets u'}
$. From this we can infer that
$$
\pmb{\ell}^{(t)}:=\textsc{Upd}^{(t)}\left(\sum_{u\in N_G^*(v)}\mathbf{m}^{(t)}_{v\gets u}\right)=\textsc{Upd}^{(t)}\left(\sum_{u'\in N_G^*(w)}\mathbf{m}^{(t)}_{w\gets u'}\right)=\pmb{\ell}^{(t)},
$$
as desired.
% We remark that this proposition encompasses the result by~\citep{XuHLJ19} for learning formalisms
% which compute a vertex labeling $\pmb{\ell}^{(t)}$ in round $t$, specified as follows: For every vertex $v\in V$,
%
\end{proof}

We remark that a similar result was recently established by~\citet{Jaume2019}. In that work, however, a version of WL on labeled directed graph is used which decouples vertex labels and edge labels. We here consider a version of WL such that correspondences known for undirected graphs carry over. More specifically,
fractional isomorphisms, $C^2$. Check!
%
%
% \smallskip
% \noindent
% \leftpointright I believe it is possible to link aMPNNs to the general model we used earlier.
% For each layer $t$ we consider two functions:
% combination function $f_{\textsl{comb}}^{(t)}$; and aggregation function
% $f_{\textsl{agg}}^{(t)}$. The layers are defined as follows. For each vertex $v\in V$ and layer $t$:

We further remark that the works \citep{XuHLJ19,grohewl}  considered graph neural models of the form
$$
\pmb{\ell}^{(t)}_{v}:=
f_{\textsl{comb}}^{(t)}\Bigl(
\pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid w \in N_G(v) \rmset\bigr)
\Bigr),
$$
where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{agg}}^{(t)}$ are general (computable) combination and aggregation functions. It is easily verified that such models are equivalent to aMPNNs. To see this, it suffices to observe that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid u \in N_G(v) \rmset\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{w\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{w})\bigr)$ (this was observed in Lemma 5 in~\citep{XuHLJ19}, based on Theorem 2 in~\citep{ZaheerKRPSS17}). Crucial in all this is that feature values belong to a countable domain and that the size of multisets is bounded. These conditions are satisfied because our domain is $\mathbb{Q}$ and there are at most $|V|$ elements in a multiset. For completeness, we provide additional details in Section~\ref{subsec:aggr} in the appendix.
% % $f_{\textsl{agg}}^{(t)}$.
% In~\citet{Loukas2019} there is an argument, based on Lemma 5 in~\citet{XuHLJ19}, which allows to convert any aggregation function to a summation followed by a function. It may need a bit more thought how to squeeze this into aMPNNs.

% \smallskip
% \noindent
% \leftpointright To bring edge information into the picture, we may look into~\citep{Jaume2019}, where WL for edge labeled graphs is considered.


\paragraph{Degree-aware MPNNs.} 
One of the example graph neural architectures that was cast as an MPNN by \citet{GilmerSRVD17}
was the GCN architecture by~\citet{KipfW16}. In that architecture~\footnote{We note that we here identify the vertex labeling $\pmb{\ell}^{(t)}$ with its vector in $\mathbb{Q}^s$.}, for each round $t=1,2,\ldots,d$,
\begin{equation}
\pmb{\ell}^{(t)}:=\sigma\Bigl(\bigl(\mathbf{D}+\mathbf{I}\bigr)^{-1/2} (\mathbf{A}+\mathbf{I})\bigl(\mathbf{D}+\mathbf{I}\bigr)^{-1/2} \pmb{\ell}^{(t-1)}\mathbf{W}\Bigr), \label{GNN:Kipf}
\end{equation}
where $\mathbf{A}$ denotes the adjacency matrix of the input graph, $\mathbf{D}$ is the diagonal matrix holding the degree of the vertices on its diagonal, and $\mathbf{I}$ is the identity matrix. Finally, $\sigma$ is a non-linear activation function such as sign or ReLU. To capture this architecture we  introduce \textit{degree-aware} MPNNs, of dMPNNs for short. These are MPNNs
whose message functions $\textsc{Msg}^{(t)}$ depend on the intermediate features $\pmb{\ell}^{(t-1)}_v$ and $\pmb{\ell}^{(t-1)}_w$, and furthermore on $d_v$ and $d_w$. They are not allowed to depend on  $v$ and $w$.

It is readily observed that we can see architectures of the form~(\ref{GNN:Kipf}) as a dMPNNs by defining for each $(w,v)\in E^*$
\begin{align*}
\mathbf{m}_{v\gets w}^{(t)}&:=	
\textsc{Msg}(\pmb{\ell}^{(t-1)}_v,\pmb{\ell}^{(t-1)}_w,d_v,d_w)=
	\left(\frac{1}{\sqrt{1+d_v}}\right)	\left(\frac{1}{\sqrt{1+d_w}}\right)((\pmb{\ell}^{(t-1)}_w)^{\textsc{t}}\mathbf{W})^{\textsc{t}}\\
\pmb{\ell}_v^{(t)}&:=\textsc{Upd}\left(\sum_{w\in N_G^*(v)} \mathbf{m}_{v\gets w}^{(t)}\right)=
\sigma\left(\sum_{w\in N_G^*(v)} \mathbf{m}_{v\gets w}^{(t)})\right).
\end{align*}
We remark, however, that the resulting MPNN is not an anonymous MPNN, and thus Proposition~\ref{prop:WL} does not apply. To understand the distinguishing power of dMPNNs we make the following observation. 

\begin{lemma}
	The degrees of all vertices can be obtained in one round of computation of an anonymous MPNN. Furthermore, this degree information can be encoded together with the initial labeling $\pmb{\nu}$.
	\end{lemma}
\begin{proof}
Let $\pmb{\ell}^{(0)}:=\left[\begin{smallmatrix}0\\\pmb{\nu}\end{smallmatrix}\right]\in\mathbf{Q}^{s+1}$. Then, for round $t=1$, we first define for each $(w,v)\in E^*$, 
$$\mathbf{m}^{(1)}_{v\gets w}=\textsc{Msg}(\pmb{\ell}^{(0)}_v,\pmb{\ell}^{(0)}_{w},\pmb{\eta}_{(w,v)}):=\left[\begin{smallmatrix}1\\\pmb{\nu}\end{smallmatrix}\right]\in\mathbf{Q}^{s+1}.$$ Then, for any
$\left[\begin{smallmatrix} a\\\mathbf{b}\end{smallmatrix}\right]\in \mathbb{Q}^{s+1}$ with $a\neq 0$, we 
let 
$$\textsc{Upd}\left(\begin{bmatrix} a\\\mathbf{b}\end{bmatrix}\right):=\begin{bmatrix} a\\\frac{1}{a}\mathbf{b}\end{bmatrix}.
$$
It is now clear that $\textsc{Upd}\left(\sum_{w\in N_G^*(v)}\mathbf{m}_{v\gets w}^{(t)}\right)=\left[\begin{smallmatrix} d_v\\\pmb{\nu}\end{smallmatrix}\right]\in \mathbb{Q}^{s+1}$.
\end{proof}

As an immediate consequence is that every dMPNN corresponds to an aMPNN satisfying the following property.
\begin{proposition}\label{prop:onestep}
Consider a dMPNN which computes labelings $\pmb{\ell}^{(t)}$, for $t=1,2,\ldots,d$. Then there exists a corresponding aMPNN which computes labeling $\hat{\pmb{\ell}}{}^{(t)}$ such that $\pmb{\ell}^{(t)}=\hat{\pmb{\ell}}^{(t+1)}$.
\end{proposition}
\begin{proof}
	The desired aMPNN consists of a one round computation to obtain all degree information, followed by a modified aMPNN simulating the given dMPNN.
\end{proof}

As an immediate corollary of Propositions~\ref{prop:WL} and~\ref{prop:onestep},  we obtain:
\begin{corollary}
	The distinguish power of dMPNNs is bounded by WL, with a factor of one.
\end{corollary}

% As a consequence, we cannot rely on Proposition~\ref{prop:WL} to  upper bound
% the distinguishing power of graph neural networks whose propagation rules rely on the degrees
% (such the GCNs by ~\citep{KipfW16}). In the literature, however, it is often mentioned that
% GCNs by ~\citep{KipfW16} are bounded by WL, hereby referring to \citep{XuHLJ19,grohewl}.
% This is, strictly speaking, not correct.
%
% To provide a correct upper bound, we  We denote dMPNNs the class of degree-aware MPNNs.
% In the next section, we study dMPNNs in more detail.

% \section{The distinguishing power of dMPNNs}
%
%
%
% We show that dMPNNs are still bounded by WL, but are in general on step ahead. This may explain the success of such architectures for graph related classification tasks.
%
% \begin{proposition}
% 	Degree-aware MPNNs are bounded by WL, with a factor of one.
% \end{proposition}
%
% As check, let us model the GCNs by \citep{KipfW16} as dMPNNs. We recall from \citet{KipfW16}
% that features are obtained as:
% $$\mathbf{F}^{(t)}:=\sigma(\tilde{D}{}^{-1/2}\tilde{\mathbf{A}}\tilde{\mathbf{D}}{}^{-1}\mathbf{F}^{(t-1)}\mathbf{W}).$$
% It is readily observed that we can see this as a dMPNNs by defining for each $(w,v)\in E^*$
% \begin{align*}
% \mathbf{m}_v^{(t)}&:=
% \textsc{Msg}(\mathbf{F}_{v\bullet}^{(t-1)},\mathbf{F}_{v\bullet}^{(t-1)},d_v,d_w)=
% 	\left(\frac{1}{\sqrt{1+d_v}}\right)	\left(\frac{1}{\sqrt{1+d_w}}\right)\mathbf{F}_{w\bullet}^{(t-1)}\mathbf{W}\\
% \mathbf{F}_v^{(t)}&:=\textsc{Upd}(\sum_{w\in N_G^*(v)} \mathbf{m}_w^{(t)})=
% \sigma\left(\sum_{w\in N_G^*(v)} \mathbf{m}_w^{(t)})\right).
% \end{align*}

\begin{corollary}
	The GCNs by \citet{KipfW16} are bounded by WL, but with a factor of one.
\end{corollary}
This corollary more correctly states the relationship between these GCNs and WL.




\smallskip
\noindent
\leftpointright
One possible additional insight is that dMPNNS can be defined such that
the message functions  only depend on $d_w$ and the features. That is, we 
can eliminate the dependency on $d_v$.
Indeed, to see this, one just adds one extra entry with value $1$ to $\mathbf{m}_v^{(t)}$ by adjusting the definition of $\textsc{Msg}$. Then, when summing over
$w\in N_G^*(v)$, these values add up to $d_v$, and $\textsc{Upd}$ can be defined such that it takes $d_v$ into account when aggregating the features. 
As a consequence, when GNN architectures use degree information, but only use $d_v$, then the dMPPNs is in fact an aMPNN and the WL bound holds. This basically corresponds to our current understanding between the asymmetry of $\mathbf{L}$ and $\mathbf{R}$. 



\paragraph{Selfless MPNNs} Finally, in our definitions of MPNNs, aMPNNs and dMPNNs, we assumes that edges in $E^*$
and vertices in $N_G^*(v)$ are considered. Quite a number of GNNs architectures do not take into account the features of the vertices themself. We say that an MPNN, aMPNN and dMPNN is \textit{selfless}, if in the definition of we use $E$ and $N_G(v)$. We denote the corresponding MPNNs, by sMPNNs, saMPNNs and sdMPNNs, respectively. We show that saMPNNs are bounded by a weak version of WL (WWL), and sdMPNNs are bounded by WLL, but are again one step ahead.

\begin{proposition}
The selfless versions of aMPNNs are bounded by WWL, the selfless versions of dMPNNs are bounded by WWL, but with a factor of one.
\end{proposition}

\smallskip
\noindent
\leftpointright I do not know the effect of selfless MPNNs or Turing completeness.


\bibliography{paper-MPNN}

\appendix
\section{Appendix}
\subsection{Relationship between MPNNs of~\citet{GilmerSRVD17} and~\citet{Loukas2019}}\label{subsec:relationship}
We first show that every MPPN in~\citep{GilmerSRVD17} corresponds to an MPNN in~\citep{Loukas2019}.
We note that~\citet{GilmerSRVD17} use an undirected labeled graph $\langle G,\pmb{\nu},\pmb{\eta}\rangle$. 

In the following, the computed labelings and message and update functions used by ~\citet{GilmerSRVD17} are 
``hatted'' to distinguish them from those computed and used by~\citet{Loukas2019}. We recall the MPNNs in~\citet{GilmerSRVD17}.
Initially $\hat{\pmb{\ell}}{}^{(0)}:=\pmb{\nu}$ and in each round $t=1,2,\ldots,d$:
\noindent
\begin{description}\setlength{\itemsep}{-0.4ex}
\item [Message passing \& aggregation.] Each vertex $v\in V$ receives and aggregates labelings from its neighbours, i.e.,
$$
\hat{\mathbf{m}}^{(t)}_v:=\sum_{w\in N_G(v)}\widehat{\textsc{Msg}}{}^{(t)}\left(\hat{\pmb{\ell}}{}^{(t-1)}_v,\hat{\pmb{\ell}}{}^{(t-1)}_w,v,w,\pmb{\eta}_{(w,v)}\right)\in\mathbb{Q}^{s}.
$$
We here assume that the message functions $\widehat{\textsc{Msg}}^{(t)}$ are allowed to use $v$ and $w$. There is certain ambiguity in \citep{GilmerSRVD17} on what precisely the arguments are of these functions.

\item [Updating.] In each vertex $v\in V$ messages are further updated:
$$
\hat{\pmb{\ell}}{}^{(t)}_v:=\widehat{\textsc{Upd}}{}^{(t)}\left(\hat{\pmb{\ell}}^{(t-1)}_v, \hat{\mathbf{m}}_w^{(t)}\right)\in\mathbb{Q}^{s}.
$$
\end{description}
Here, $\widehat{\textsc{Msg}}{}^{(t)}$ and $\widehat{\textsc{Upd}}^{(t)}$ are general (computable) functions. This can again be complemented with a readout function.


The corresponding MPNN in the formalism of \citet{Loukas2019} now can be easily defined, as follows. For convenience, we identify labelings with their corresponding vector in $\mathbb{Q}^s$.
Initially, we let $\pmb{\ell}^{(0)}=\left[\begin{smallmatrix}
	\mathbf{0} \\
     \pmb{\nu}\end{smallmatrix}\right]\in\mathbb{Q}^{2s}$. Then,
for each $(w,v)\in E$ we define:
$$
\textsc{Msg}^{(t)}\left(\begin{bmatrix}
	\mathbf{0} \\
	\pmb{\ell}^{(t-1)}_v
\end{bmatrix},\begin{bmatrix}
	\mathbf{0} \\
	\pmb{\ell}^{(t-1)}_w
\end{bmatrix},v,w,\pmb{\eta}_{(w,v)}\right):=
\begin{bmatrix}
	\mathbf{0}\\
\widehat{\textsc{Msg}}{}^{(t)}\left(\hat{\pmb{\ell}}{}^{(t-1)}_v,\hat{\pmb{\ell}}{}^{(t-1)}_w,v,w,\pmb{\eta}_{(w,v)}\right)
\end{bmatrix}\in\mathbb{Q}^{2s},
$$
for $(v,v)$ (recall that  \citet{Loukas2019} uses $N_G^*(v)$)  we define:
$$
\textsc{Msg}^{(t)}\left(\begin{bmatrix}
	\mathbf{0} \\
	\pmb{\ell}^{(t-1)}_v
\end{bmatrix},\begin{bmatrix}
	\mathbf{0} \\
	\pmb{\ell}^{(t-1)}_v
\end{bmatrix},v,w,\pmb{\eta}_{(v,v)}\right):=
\begin{bmatrix}
	\pmb{\ell}^{(t-1)}_v \\
	\pmb{0}
\end{bmatrix}\in\mathbb{Q}^{2s},
$$
Finally, for each vertex $v\in V$ and any $\mathbf{a},\mathbf{b}\in\mathbb{Q}^{s}$, we define:
$$\textsc{Upd}^{(t)}\left(\begin{bmatrix}\mathbf{a}\\
\mathbf{b}\end{bmatrix}\right):=\begin{bmatrix} \mathbf{0}\\
\widehat{\textsc{Upd}}^{(t)}(\mathbf{a},\mathbf{b})\end{bmatrix}\in\mathbb{Q}^{2s}.
$$
It is clear that for every $v\in V$, $\pmb{\ell}^{(t)}_v=\left[\begin{smallmatrix}\mathbf{0}\\
\hat{\pmb{\ell}}{}^{(t)}_v\end{smallmatrix}\right]$.


Conversely, given an MPNN in the form of~\citet{Loukas2019}, it suffices to define 
$\hat{\pmb{\ell}}^{(0)}:=\pmb{\nu}$. Then, for round $t=1,2,\ldots,d$, we define for every vertex $v\in V$ and $w\in N_G(v)$,
$$
\widehat{\textsc{Msg}}{}^{(t-1)}(\hat{\pmb{\ell}}{}_v^{(t-1)},\hat{\pmb{\ell}}{}_w^{(t)},v,w,\pmb{\eta}_{(w,v)}):=\textsc{Msg}{}^{(t-1)}(\hat{\pmb{\ell}}{}_v^{(t)},\hat{\pmb{\ell}}{}_w^{(t)},v,w,\pmb{\eta}_{(w,v)})
$$
and 
$$
\widehat{\textsc{Upd}}^{(t)}(\hat{\pmb{\ell}}_v^{(t-1)},\hat{\mathbf{m}}{}^{(t)}_v):=
\textsc{Upd}^{(t)}(\hat{\pmb{\ell}}_v^{(t-1)}+\hat{\mathbf{m}}{}^{(t)}_v)
$$
Clearly, $\hat{\pmb{\ell}}{}^{(t)}=\pmb{\ell}^{(t)}$.
% Due to some ambiguity in the definition MPNNs in  \citep{GilmerSRVD17}, i.e., it is not entirely clear what kind of information from the graph the function $\widehat{\}$
%  every MPNN in~~\citep{Loukas2019} corresponds to an MPNN in \citep{GilmerSRVD17}

\subsection{Relationship with aggregate/combine formalism in \citet{XuHLJ19}}\label{subsec:aggr}
We show that the formalism of \citep{XuHLJ19} can be case as an aMPNN. As already mentioned in the body of the paper, we can write $f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid u \in N_G(v) \rmset\bigr)$ in the form $g^{(t)}\bigl(\sum_{w\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{w})\bigr)$. It is straightforward to regard this as an aMPNN in the formalism of ~\citep{GilmerSRVD17}, described in the previous section.
Indeed, it suffices to let 
\begin{align*}
\widehat{\textsc{Msg}}^{(t)}(\pmb{\ell}_v^{(t-1)},\pmb{\ell}_w^{(t-1)},v,w,\pmb{\eta}_{(w,v)})&:=h^{(t)}(\pmb{\ell}_w^{(t-1)})\\
\widehat{\textsc{Upd}}^{(t)}(\pmb{\ell}_v^{(t-1)},\mathbf{m}^{(t)}_v)&:=f_{\textsl{comb}}^{(t)}(\pmb{\ell}_v^{(t-1)},g^{(t)}(\mathbf{m}^{(t)}_v)).
\end{align*}
We have shown in the previous section that this can be cast as an MPNN in the form considered in this paper. Furthermore, the message function is independent of $v$ and $w$, hence an aMPNN is obtained.

%
%
% \smallskip
% \noindent
% \leftpointright I believe it is possible to link aMPNNs to the general model we used earlier.
% For each layer $t$ we consider two functions:
% combination function $f_{\textsl{comb}}^{(t)}$; and aggregation function
% $f_{\textsl{agg}}^{(t)}$. The layers are defined as follows. For each vertex $v\in V$ and layer $t$:
% $$
% \pmb{\ell}^{(t)}_{v}:=
% f_{\textsl{comb}}^{(t)}\Bigl(
% \pmb{\ell}_{v}^{(t-1)},f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid w \in N_G(v) \rmset\bigr)
% \Bigr),
% $$
% where $f_{\textsl{comb}}^{(t)}$ and  $f_{\textsl{agg}}^{(t)}$ are general (computable) combination and aggregation functions. To see this, it suffices to observe that the aggregation functions $f_{\textsl{aggr}}^{(t)}\bigl(\lmset \pmb{\ell}^{(t-1)}_{w} \mid u \in N_G(v) \rmset\bigr)$ can be written in the form $g^{(t)}\bigl(\sum_{w\in N_G(v)} h^{(t)}(\pmb{\ell}^{(t-1)}_{w})\bigr)$ (this was observed in Lemma 5 in~\citep{XuHLJ19}, based on Theorem 2 in~\citep{ZaheerKRPSS17}). Crucial in all this is that feature values belong to a countable domain and that the size of multisets is bounded. These conditions are satisfied because our domain is $\mathbb{Q}$ and there are at most $|V|$ elements in a multiset. For completeness, we detail in Section~\ref{subsec:aggr} in the appendix.
% % % $f_{\textsl{agg}}^{(t)}$.
% In~\citet{Loukas2019} there is an argument, based on Lemma 5 in~\citet{XuHLJ19}, which allows to convert any aggregation function to a summation followed by a function. It may need a bit more thought how to squeeze this into aMPNNs.

% \smallskip
% \noindent
% \leftpointright To bring edge information into the picture, we may look into~\citep{Jaume2019}, where WL for edge labeled graphs is considered.


% \paragraph{Degree-aware MPNNs.}
\end{document}